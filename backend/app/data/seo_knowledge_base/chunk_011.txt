# How Google Works

How Google Works




The Google Algorithm Update Correction Hypothesis (Shaun Anderson, Hobo)


Google’s DOJ trial disclosures confirm that search ranking hinges on two top-level
signals - Quality (Q*) and Popularity (P*) - powered by modular systems like
Navboost, Topicality, and PageRank, with user interaction data at the core.

SEO success now depends on trust, authority, user engagement, freshness, and
adapting to potential legal-driven changes in Google’s architecture.

SEO really can be broken down, based on Google V DOJ evidence, into "Mastering
your P*s and Q*s". "Knowing your A, B and Cs", "dotting your I's" and "crossing
your T*s".




                                                             9

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Key Takeaways: DOJ v. Google – SEO Insights (2025)

   ●​ Two Core Signals Drive Ranking: Google’s system reduces to Quality (Q*)
      and Popularity (P*), revealed in DOJ trial documents.
   ●​ Modular Architecture: Underlying systems like Topicality (T*), Navboost, and
      RankBrain feed into those top-level signals.
   ●​ User Data Is Central: Clicks, scrolls, Chrome visit data, dwell time, and
      pogo-sticking are leveraged as critical ranking feedback.
   ●​ Hand-Crafted Signals Dominate: Most ranking factors are engineered
      manually, not black-box ML, for control and stability.
   ●​ Freshness Matters: Google boosts recency for queries where timeliness is
      essential (news, events), balancing against historical clicks.
   ●​ Links Still Core: PageRank (distance from trusted sources), anchor text, and
      backlink quality remain crucial authority signals.
   ●​ Context Layers Refine Results: Location and personalisation heavily shape
      what individual users see beyond the “universal” rank.
   ●​ AI as Final Layer: RankBrain, BERT, and MUM don’t replace hand-crafted
      signals — they refine them with semantic understanding.
   ●​ ⚖️ Legal Impact Ahead: DOJ remedies may force changes to Google’s
      ranking systems, shaping SEO strategy going forward.




                                                         10

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




United States et al. v. Google LLC




The antitrust case, United States et al. v. Google LLC, initiated by the US Department
of Justice (DOJ) in 2020, represents the most significant legal challenge to Google’s
market power in a generation.

While the legal arguments focused on market monopolisation, the proceedings
inadvertently became a crucible for technical disclosure, forcing Google to all but
reveal the long-guarded secrets of its search engine architecture.

The trial's technical revelations were not incidental; they were central to the core legal
conflict.

The DOJ's case rested on the premise that Google unlawfully maintained its monopoly
in general search and search advertising through a web of anticompetitive and
exclusionary agreements with device manufacturers and browser developers,
including Apple, Samsung, and Mozilla.




                                                          11

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




These contracts, often involving payments of billions of dollars annually, ensured
Google was the pre-set, default search engine for the vast majority of users, thereby
foreclosing competition by denying rivals the scale and data necessary to build a
viable alternative.

This legal challenge created a strategic paradox for Google.

To counter the DOJ's accusation that its dominance was the result of illegal
exclusionary contracts, Google's primary defence was to argue that its success is a
product of superior quality and continuous innovation - that users and partners
choose Google because it is simply the best search engine available.

This "superior product" defence, however, could not be asserted in a vacuum.

To substantiate the claim, Google was compelled to present evidence of this
superiority, which necessitated putting its top engineers and executives on the
witness stand. Individuals like Pandu Nayak, Google's Vice President of Search, and
Elizabeth Reid, Google's Head of Search, were tasked with explaining, under oath, the
very systems that produce this acclaimed quality.

Consequently, the act of defending its market position legally forced Google to
compromise its most valuable intellectual property and its long-held strategic secrecy.

The sworn testimonies and internal documents entered as evidence provided an
unprecedented, canonical blueprint of Google's key competitive advantages.

At the heart of these revelations is the central role of user interaction data.

A recurring theme throughout the testimony was that Google's "magic" is not merely
a static algorithm but a dynamic, learning system engaged in a "two-way dialogue"
with its users.

Every click, every scroll, and every subsequent query is a signal that might teach the
system what users find valuable.

This continuous feedback loop, operating at a scale that Google's monopoly ensures
no competitor can replicate, is the foundational resource for the powerful ranking
systems detailed in the trial.
                                                         12

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Architecture of Google Search Ranking

The trial testimony and exhibits dismantle the popular conception of Google’s ranking
system as a single, monolithic algorithm. Instead, they reveal a sophisticated,
multi-stage pipeline composed of distinct, modular systems, each with a specific
function and data source.

This architecture is built upon a foundation of traditional information retrieval
principles and human-engineered logic, which is then powerfully refined by systems
that leverage user behaviour data at an immense scale.

While initial trial exhibits hinted at this modularity, the later unredacted remedial
opinion in the DOJ v. Google case provided the definitive, high-level blueprint. The
court revealed that Google has two "fundamental top-level ranking signals" that
are the primary inputs for a webpage's final score: Quality (Q*) and Popularity (P*).

These two signals also help Google determine how frequently to crawl webpages to
keep its index fresh.

This analysis details the core components of this architecture, showing how the
previously revealed systems of Topicality (T*), Navboost, and Q* are the essential
building blocks for Google's top-level signals.




                                                         13

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Two Pillars of Ranking: An Overview
The systems detailed in the original trial are now best understood as the underlying
components that feed into two fundamental signals. The following table summarises this
confirmed two-pillar architecture.


 System Name            Primary                  Key Data                  Engineering              Key Revelation
                        Function                 Inputs                    Approach                 Source


 Quality (Q*)           Assesses the             PageRank                  Hand-crafted by          Remedial
                        overall                  (distance from            engineers,               Opinion, Trial
                        trustworthiness,         seed sites),              largely static           Exhibits,
                        authoritativenes         content-derived           score.                   Engineer
                        s, and quality of        metrics (like the                                  Deposition
                        a                        'Body' signal),
                        website/domain.          spam scores,
                                                 and human rater
                                                 evaluations.


 Popularity (P*)        Measures how             Chrome visit              Data-driven              Remedial
                        widely visited           data, number of           system, refined          Opinion, Pandu
                        and                      anchors (the 'A'          by engineers.            Nayak
                        "well-linked" a          signal), user                                      Testimony
                        page is to               interaction data
                        promote popular          (from Navboost,
                        documents.               including
                                                 aggregated
                                                 good/bad/lastLo
                                                 ngest clicks).


 RankBrain              Interprets novel,        Historical search         Machine                  Eric
                        ambiguous, and           data (not live            Learning                 Lehman/Pandu
                        long-tail search         user data).               (unsupervised).          Nayak
                        queries.                                                                    Testimony




                                                          14

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Deconstructing the Signals - The Core Systems
The foundational systems revealed during the trial provide the mechanics for the
top-level signals.

Quality Score (Q*) - The Engine of the 'Quality' Signal

The Quality Score (Q*) is the internal system that assesses the overall
trustworthiness and quality of a website or domain. It is a hand-crafted, largely static
score that functions as the core of the top-level Quality signal. Its key data inputs
include PageRank and the link distance from trusted "seed" sites, which align
perfectly with the remedial opinion's description of the Quality signal.

Navboost and Topicality (T*) - The Engines of the 'Popularity' Signal

The top-level Popularity (P*) signal is powered by a combination of systems that
measure user engagement and link structures.

   ●​ Navboost: This is the primary user interaction engine. As revealed in Pandu
      Nayak’s testimony, Navboost is a data-driven system that refines rankings
      based on 13 months of aggregated user click data, including metrics like
      good, bad, and “last longest” clicks. This system draws its information from
      a vast, underlying data warehouse codenamed ‘Glue’, which logs the trillions
      of user interactions that Google processes. The explicit confirmation of
      Chrome visit data as a direct input for the Popularity signal was particularly
      significant, as Google had historically been less direct about the extent to
      which it leverages its browser's data for ranking purposes. It provides the
      “Chrome visit data” and “user interaction” components of the P* signal.




                                                          15

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




   ●​ Topicality (T*): This system computes a document's direct relevance to query
      terms and serves as a foundational score. Testimony from engineer HJ Kim
      revealed it is composed of "ABC" signals:
         ○​ Anchors (A), Body (B), and Clicks (C). Within the new framework:​

                     ■​ The Anchors (A) and Clicks (C) components serve as direct
                        inputs to the Popularity (P*) signal.
                     ■​ The Body (B) component, based on the text of the document
                        itself, is a "content-derived metric" that feeds into the Quality
                        signal.

Information Retrieval and the Primacy of "Hand-Crafted" Signals

Contrary to the prevailing narrative of an all-encompassing artificial intelligence, the
trial revealed that Google's search ranking systems are fundamentally grounded
in signals that are "hand-crafted" by its engineers.

This deliberate engineering philosophy prioritises control, transparency, and the
ability to diagnose and fix problems, a stark contrast to the opaque, "black box"
nature of more complex, end-to-end machine learning models.

The deposition of Google Engineer HJ Kim was particularly illuminating on this point.
He testified that "the vast majority of signals are hand-crafted," explaining that the
primary reason for this approach is so that "if anything breaks, Google knows what
to fix".

This methodology is seen as a significant competitive advantage over rivals like
Microsoft's Bing, which was described as using more complex and harder-to-debug
ML techniques.

The process of "hand-crafting" involves engineers analysing relevant data, such as
webpage content, user clicks, and feedback from human quality raters, and then
applying mathematical functions, like regressions, to define the "curves" and
"thresholds" that determine how a signal should respond to different inputs.

This human-in-the-loop system ensures that engineers can modify a signal's
behaviour to handle edge cases or respond to public challenges, such as the spread
                                                          16

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




of misinformation on a sensitive topic.

This foundational layer of human-engineered logic provides the stability and
predictability upon which more dynamic systems are built.




                                                         17

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




Trustworthiness
“Q* (page quality (i.e., the notion of trustworthiness)) is incredibly important. If
competitors see the logs, then they have a notion of "authority" for a given
site.” February 18, 2025, Call with Google Engineer HJ Kim (DOJ Case)

I agree - if this information were made available, it would be abused.




E-E-A-T


The emergence of these distinct systems - T* for query-specific relevance, Q* for
static site quality, and Navboost for dynamic user-behaviour refinement - paints
a clear picture of a modular, multi-stage ranking pipeline.

The process does not rely on a single, all-powerful algorithm.

Instead, it appears to be a logical sequence: initial document retrieval is followed by
foundational scoring based on relevance (T*) and trust (Q*).


                                                            18

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




This scored list is then subjected to a massive re-ranking and filtering process
by Navboost, which leverages the collective historical behaviour of users.

Only the small, refined set of results that survives this process is passed to the final,
most computationally intensive machine learning models.

This architecture elegantly balances the need for speed, scale, and accuracy, using
less expensive systems to do the initial heavy lifting before applying the most
powerful models.




Disconnected Entity




                                                            19

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Freshness (Timeliness of Content)

Google also considers freshness - how recent or up-to-date the information on a
page is, especially for queries where timeliness matters.

Trial testimony and exhibits detailed how freshness influences rankings:

   ●​ Freshness as a Relevance Signal: “Freshness is another signal that is
      ‘important as a notion of relevance’,” Pandu Nayak testified regmedia.co.uk. In
      queries seeking current information, newer content can be more relevant.
      Nayak gave an example: if you’re searching for the latest sports scores or
      today’s news, “you want the pages that were published maybe this morning or
      yesterday, not the ones that were published a year ago.” regmedia.co.uk Even if
      an older page might have been relevant in general, it won’t satisfy a user
      looking for the latest updates. Thus, Google’s ranking system will favour more
      recently published pages for fresh information queries. Conversely, for topics
      where age isn’t detrimental (say, a timeless recipe or a classic novel), an
      older authoritative page can still rank well. As Nayak put it, “deciding
      whether to use [freshness] or not is a crucial element” of delivering quality
      results regmedia.co.uk - Google must judge when recency should boost a
      result’s ranking and when it’s less important.
   ●​ Real-Time Updates for Breaking Queries: John Giannandrea (former head of
      Google Search) explained that “Freshness is about latency, not quantity.” It’s
      not just showing more new pages, but showing new information fast when it’s
      needed regmedia.co.uk. “Part of the challenge of freshness,” he testified, “is
      making sure that whatever gets surfaced to the top… is consistent with what
      people right now are interested in.” regmedia.co.uk For example, “if somebody
      famous dies, you kind of need to know that within seconds,” Giannandrea said
      regmedia.co.uk. Google built systems to handle such spikes in information
      demand. An internal 2021 Google document (presented in court) described a
      system called “Instant Glue” that feeds very fresh user-interaction data into
      rankings in near real-time. “One important aspect of freshness is ensuring that
      our ranking signals reflect the current state of the world,” the document stated.
      “Instant Glue is a real-time pipeline aggregating the same fractions of
      user-interaction signals as [the main] Glue, but only from the last 24 hours of
      logs, with a latency of ~10 minutes.” justice.gov In practice, this means if there’s
                                                         20

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   a sudden surge of interest in a new topic (e.g. breaking news), Google’s
   algorithms can respond within minutes by elevating fresh results (including
   news articles, recent forum posts, etc.) that match the new intent. Google also
   uses techniques (code-named “Tetris” in one exhibit) to demote stale
   content for queries that deserve fresh results and to promote newsy content
   (e.g. Top Stories) when appropriate justice.gov
●​ Balancing Freshness vs. Click History: One difficulty discussed at trial is that
   older pages naturally accumulate more clicks over time, which could bias
   ranking algorithms that learn from engagement data. Nayak noted that pages
   with a long history tend to have higher raw click counts than brand-new pages
   (simply by having been around longer) regmedia.co.uk. If the system naively
   preferred results with the most clicks, it might favour an outdated page that
   users have clicked on for years, over a fresher page that hasn’t had time to
   garner clicks. “Clicks tend to create staleness,” as one exhibit put it
   regmedia.co.uk. To address this, Google “compensates” by boosting fresh
   content for queries where recency matters, ensuring the top results aren’t just
   the most popular historically, but the most relevant now. In essence, Google’s
   ranking algorithms include special freshness adjustments so that new,
   pertinent information can outrank older but formerly popular pages when
   appropriate regmedia.co.uk. This keeps search results timely for the user’s
   context.




                                                      21

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Linking Behaviour (Link Signals and Page Reputation)

The trial also illuminated how Google uses the web’s linking behaviour - how pages
link to each other - as a core ranking factor. Links serve both as votes of authority
and as contextual relevance clues:

   ●​ Backlink Count & Page Reputation: Google evaluates the number and quality
      of links pointing to a page to gauge its prominence. Dr. Lehman explained
      during testimony that a ranking “signal might be how many links on the web are
      there that point to this web page or what is our estimate of the sort of
      authoritativeness of this page.”regmedia.co.uk In other words, Google’s
      algorithms look at the link graph of the web to estimate a page’s authority: if
      dozens of sites (especially reputable ones) link to Page X, that’s a strong
      indication that Page X is important or trustworthy on its topic. This
      principle underlies PageRank and other authority signals. By assessing “how
      many links… point to the page,” Google infers the page’s popularity and
      credibility within the web ecosystem regmedia.co.uk. (However, it’s not just raw
      counts - the quality of linking sites matters, as captured by PageRank’s
      “distance from a known good source” metric justice.gov.)
   ●​ Anchor Text (Link Context): Links don’t only confer authority; they also carry
      information. The anchor text (the clickable words of a hyperlink) tells Google
      what the linked page is about. As noted earlier, Pandu Nayak highlighted that
      anchor text provides a “valuable clue” to relevance regmedia.co.uk. For
      example, if dozens of sites hyperlink the text “best wireless headphones” to a
      particular review page, Google’s system learns that the page is likely about
      wireless headphones and is considered “best” by those sources, boosting its
      topical relevancy for that query. This context from linking behaviour helps
      Google align pages to queries beyond what the page’s own text says. It’s a way
      of leveraging the collective judgment of website creators: what phrases do
      others use to describe or reference your page? Those phrases become an
      external signal of the page’s content. Google combines this with on-page
      signals (as part of topicality scoring) to better understand a page’s subject
      matter regmedia.co.uk.




                                                         22

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




   ●​ Link Quality over Quantity: Not all links are equal. Through PageRank and
      related “authority” algorithms, Google gives more weight to links from
      reputable or established sites. One trial exhibit described PageRank as
      measuring a page’s proximity to trusted sites (a page linked by high-quality
      sites gains authority; one linked only by dubious sites gains much less)
      justice.gov. This shows that linking behaviour is evaluated qualitatively. A
      single backlink from, say, a respected news outlet or university might boost a
      page’s authority more than 100 backlinks from low-quality blogs. Google also
      works to ignore or devalue spammy linking schemes. (While specific anti-spam
      tactics weren’t detailed in the trial excerpts we saw, the focus on “authoritative,
      reputable sources” implies that links from spam networks or “content farms”
      are discounted - aligning with Google’s long-standing efforts to prevent link
      manipulation.) I go into link building more in my article on Link building for
      beginners.

In summary, the DOJ’s antitrust trial pulled back the curtain on Google’s ranking
system.

Topicality signals (page content and context from anchors) tell Google what a page
is about and how relevant it is to a query.

Authority signals (like PageRank and quality scores) gauge if the page comes from a
trustworthy, reputable source.

Freshness metrics ensure the information is up-to-date when timeliness matters. And
the web’s linking behaviour - both the number of links and the anchor text - feeds
into both relevance and authority calculations.

All these factors, largely handcrafted and fine-tuned by Google’s engineers
justice.gov, work in concert to rank the billions of pages on the web for any given
search.

As Pandu Nayak summed up in court, Google uses “several hundred signals” that
“work together to give [Google Search] the experience that is search today.”
regmedia.co.uk

Each factor - topical relevance, authority, freshness, links, and many more - plays a
                                                         23

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




part in Google’s complex, evolving ranking algorithm, with the aim of delivering the
most relevant and reliable results to users.

The Final Layers: Location and Personalisation

While the core systems produce a universal ranking based on quality and popularity,
the results a user actually sees are heavily tailored by a final, powerful layer of
context. The trial focused on the foundational architecture, but the live search
experience is profoundly shaped by signals specific to the individual user.

   ●​ Location as a Dominant Signal: For a vast number of queries, the user's
      physical location is the single most important ranking factor. For searches like
      "pubs near me" or "solicitors in Greenock," Google's core algorithm is
      secondary to its ability to identify relevant, local results. It determines a user’s
      location with high precision using device GPS, Wi-Fi signals, and IP addresses
      to transform a generic query into a geographically specific and immediately
      useful answer.
   ●​ Personalisation from Search History: Beyond location, Google refines
      rankings based on a user's individual search history. This system learns a user's
      interests and intent over time to resolve ambiguity. For instance, a user who
      frequently searches for software development topics will likely see results
      about the programming language if they search for "python," whereas a user
      whose history is filled with zoology queries will see results about the snake.
      This layer of personalisation ensures that the final search results page is not
      just a list of high-quality, popular documents, but a bespoke answer sheet
      tailored to the user's implicit context and previous behaviour.




                                                         24

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Beyond RankBrain: The Shift to Semantic Understanding

The trial rightly highlighted RankBrain as a pioneering use of machine learning in
search. However, to understand Google's modern capabilities, it is crucial to recognise
that its AI has since evolved from simply interpreting novel queries to fundamentally
understanding the meaning of language itself. This represents a shift towards
semantic search.

Subsequent models like BERT (Bidirectional Encoder Representations from
Transformers) changed the game entirely. Unlike earlier systems that processed
words in a query one by one, BERT analyses the full context of a word by looking at
the words that come before and after it. This is critical for understanding nuance and
intent. For example, in the query "can you get medicine for someone pharmacy," BERT
understands that the preposition "for" is the most important word, fundamentally
changing the query's meaning.

This evolution continued with later models like MUM (Multitask Unified Model),
designed to understand information across different languages and formats (like
images and text) simultaneously. These advanced AI systems do not replace the
foundational signals like Navboost or Q*. Instead, they act as a supremely intelligent
final analysis layer. They take the pool of high-quality, relevant results identified by the
core systems and re-rank them based on a deep, contextual comprehension of what
the user truly means, making the search engine feel less like a database and more like
a dialogue.

A Forward-Looking Note on Legal Remedies

Looking ahead, it is important to note that while this architecture represents Google's
current competitive advantage, the ongoing remedies phase of the DOJ trial could
mandate changes to these very systems. The future of search, therefore, may be
shaped as much by court rulings as by Google's own engineers.




                                                          25

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




How Human Quality Raters Are Used

The court document reveals that scores from human quality raters are a direct and
foundational input for training Google's core ranking models.

Key Takeaways

   ●​ Human quality rater scores are a direct training input for Google’s core
      ranking models (RankEmbed, RankEmbedBERT).
   ●​ These scores are foundational data, not peripheral feedback, combined with
      query and click logs.
   ●​ Court testimony shows rater-trained models improved Google’s performance
      on long-tail queries.
   ●​ This contradicts Google’s long-standing public stance that rater scores only
      serve as indirect benchmarks.

The Role of Human Quality Raters

The DOJ v. Google remedial opinion makes clear that human quality raters are not
just external evaluators - their judgments directly shape the very core of Google’s
ranking systems. The opinion reveals that the RankEmbed and RankEmbedBERT
models, which are central to Google’s AI-based ranking, are trained on two primary
sources of data: search logs and human rater scores. This elevates rater input
from “guidance” to direct training data.

The testimony of Google’s Vice President of Search, Dr. Pandu Nayak, further
highlights their impact: rater-trained RankEmbedBERT models significantly improved
Google’s ability to process complex, long-tail queries, where language
understanding is essential.

The court emphasised that these scores form a foundational dataset in combination
with user interaction logs. The data pipeline for RankEmbed models explicitly relies on
the scoring of web pages by raters, embedding their judgments into machine
learning systems that decide how billions of pages are ranked.

This stands in contrast to Google’s public communications, which have long
maintained that rater scores do not directly affect site rankings. While technically true
                                                          26

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




at the individual page level, the opinion shows that, in aggregate, rater scores are
systemic training inputs that define how the search engine learns to rank. The
models built from this data have “directly contributed to Google’s quality edge
over competitors,” underscoring just how central rater input is to the evolution of
Google Search.

Direct Training Data for Ranking Models

The document explicitly states that human rater scores are one of two primary data
sources used to train the RankEmbed and RankEmbedBERT models. These are
described as sophisticated, AI-based systems critical to Google's search quality:

     “RankEmbed and its later iteration RankEmbedBERT are ranking
     models that rely on two main sources of data: % of 70 days of search
     logs plus scores generated by human raters and used by Google to
     measure the quality of organic search results.”​
     View in PDF

Improving Performance on Difficult Queries

The impact of these rater-trained models is significant, particularly in improving
Google's ability to handle complex and less common searches. Testimony from
Google’s Vice President of Search confirms:

     “RankEmbedBERT was again one of those very strong impact things, and it
     particularly helped with long-tail queries where language understanding is
     that much more important.”​
     View in PDF

Providing Foundational Data for Machine Learning

The document clarifies that rater scores are not just casual feedback but a
fundamental dataset for these AI systems:

     “The data underlying RankEmbed models is a combination of
     click-and-query data and scoring of web pages by human raters.”​
     View in PDF
                                                         27

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




This establishes human judgments as a core component used to teach the models
how to rank search results.


Deviation From Google's Past Statements

The finding that rater scores are a direct training input for a core ranking model like
RankEmbed clarifies and arguably deviates from the spirit of Google’s long-held
public statements.

Google’s Public Stance

For years, Google has consistently stated that quality rater scores do not directly
impact the ranking of any individual website. The company’s official guidance
describes the raters’ role as providing feedback to help “benchmark the quality of our
results” and “evaluate changes.” This has often been interpreted to mean their
influence is indirect - more like feedback that helps engineers tune the system
overall, rather than a direct ranking signal.

The Deviation Revealed in Court

The court document clarifies that this influence is far more direct and systemic than
previously understood. While a single rater’s score may not manually move a page up
or down, the aggregated scores are a foundational dataset used to build and train
an automated ranking system that is a core part of the algorithm:

     “The RankEmbed models trained on that data have directly contributed to
     the company’s quality edge over competitors.”​
     View in PDF

In essence, Google’s statements are technically correct in that no single rater score
directly changes a site’s ranking. But the court’s findings show a direct, systemic link
where the collective judgment of raters is used to train core AI ranking models.

This role is far more influential than the “feedback” or “benchmarking” role Google
has historically emphasised.



                                                          28

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




DOJ v. Google Disclosure: The Popularity Signal (P*)




     “Google’s Popularity signal (P*) ‘uses Chrome data’ to capture the
     popularity of websites. The signal is a measure quantifying the number of
     links between pages, as reflected in anchor text and user interactions, and
     it is used to promote well-linked documents.”

In the DOJ v. Google remedial opinion, the court revealed that Google’s Popularity
signal (P*) is one of the company’s fundamental top-level ranking signals,
alongside quality. While Google has long emphasised PageRank as its primary
authority signal, the opinion shows that Popularity (P*) draws on Chrome browsing
data and the number of anchors (link connections between pages) to measure
how “well-linked” and widely visited a page is. This detail was not available in the
original trial exhibits, which only contained a redacted line noting a “popularity
signal that uses Chrome data.” The fuller explanation - including anchors and the role
of promoting well-linked documents — only emerged later in the unredacted
remedial opinion. This makes P* a core mechanism by which Google evaluates and
elevates content in search results, and demonstrates how user browsing data feeds
directly into ranking.

In the DOJ v. Google remedial opinion, the court identified Quality as one of Google’s
fundamental top-level ranking signals, paired with Popularity. Quality is described
as being derived largely from the webpage itself, supplemented by inputs such as

                                                         29

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




PageRank (distance from known authoritative sources), spam scores, and human
rater evaluations. In the original trial exhibits, much of this section was blacked out
under “Highly Confidential Information” protections, leaving only partial
references to PageRank and generic mentions of a “quality score.”

The later unredacted remedial opinion confirmed that Quality incorporates signals
of authoritativeness, content-derived metrics, and rater scoring, and is directly
used in determining a page’s ranking. This disclosure highlights Quality as a systemic
scoring function — not just an abstract concept — and underscores its central role in
shaping search results.


What This Means

   ●​ Chrome Visit Data → The signal directly taps into browsing behaviour from
      Google Chrome, giving Google visibility into what users are visiting across the
      web.
   ●​ Anchor Link Structures → It measures how pages are connected by links,
      including anchor text relevance.
   ●​ User Interaction Data → Beyond static link graphs, it incorporates behavioural
      evidence of popularity from actual user navigation.
   ●​ Promotion of Well-Linked Documents → Pages with stronger link
      connectivity and demonstrated popularity are pushed higher in ranking.


SEO Relevance

This is a significant confirmation because:

   1.​ It ties Chrome browsing data directly into organic ranking (something
       Google has denied or downplayed publicly).
   2.​ It shows popularity is not just PageRank, but a broader signal combining
       Chrome data + anchor link structures + user behaviour.
   3.​ It acts as a boost mechanism for documents seen as “well-linked” both by
       hyperlink structure and user engagement.




                                                         30

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Topicality (T*) or The ABCs of Relevance
"Topicality" was addressed with surprising specificity in the trial.

Far from being an abstract concept, "Topicality" was revealed to be a formal,
engineered system within Google, designated as T*.

The explicit function of the T* system is to compute a document's fundamental,
query-dependent relevance.

It serves as a "base score" that answers the question: how relevant is this document
to the specific terms used in this search query?

Google uses topicality signals to judge how well a page’s content matches a user’s
query. This is essentially the relevance of the page’s topic and text to the search
terms:

   ●​ On-Page Content: The actual words on a webpage are the foundation of
      topical relevance. “The most basic and in some ways the most important
      signal is the words on the page and where they occur,” testified Pandu Nayak
      (Google’s Vice President of Search) regmedia.co.uk. He emphasized that the
      presence of query terms in the content - whether in the title, headings, meta
      tags, or body text - is “actually kind of crucial” for ranking regmedia.co.uk. In
      short, what the document “says about itself” is central to determining its
      topicality. Nayak noted that signals such as term frequency and position (e.g.
      title vs. body) are “very important” relevance cues regmedia.co.uk.​

   ●​ Anchor Text (Context from Links): Google also evaluates what other websites
      say about a page. Nayak testified that “another very important signal is the
      [hyper]links between pages,” known as anchor text, which provides “a very
      valuable clue in deciding what the target page is relevant to.” regmedia.co.uk In
      other words, if many pages link to a webpage using certain keywords, it signals
      the topic or context of that page. (For example, a page heavily cited with the
      anchor “JavaScript tutorial” is likely about JavaScript tutorials, boosting its
      topical relevance for that query.) Importantly, Google clarified that it does
      not mix user click data into its link analysis. When asked if click data
      influences the anchor signal, Dr. Kenneth “Ken” Lehman (a Google search
                                                          31

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   quality witness) explained: “To generate the anchor signal, that’s just from
   links between web pages, and it doesn’t involve clicks.” regmedia.co.uk
   Anchors are purely derived from the web’s linking behaviour, independent of
   user interactions.​

●​ User Interaction Signals: Internal evidence shows Google also monitors
   aggregate user behaviour to refine relevance. A Google “Three Pillars of
   Ranking” slide (from 2016) listed User-interactions (“what users say about the
   document”) as a third pillar alongside Body and Anchors. These interactions
   can include clicks, attention (hover/scroll), swipes, and whether users quickly
   return to search results. While Google has long maintained publicly that
   clicks are not a direct rank booster, trial documents indicate Google does
   use such data in a feedback loop to evaluate search quality. Indeed, as HJ
   Kim noted, Google historically tracked dwell time (length of time on a result
   before returning) as part of topicality scoring justice.gov. However, Google
   witnesses stressed that user data is used carefully - primarily to learn and
   adjust algorithms, not to blindly promote whatever gets the most clicks. (See
   the discussion under Authority about pitfalls of click metrics.) Q “How does
   that relate to the question of user data or user interaction data? A. So the chart
   is a little bit complex, but what it’s illustrating is one of the problems with using
   click data in connection with ranking search results. It’s a very strong
   observation that people tend to click on lower-quality, less-authoritative
   content than we would like to show on our search engine. Our goal is to show
   -- when someone issues a query, to give them information that’s relevant and
   from authoritative, reputable sources. People tend not to click on those so
   much. So if we’re guided too much by clicks, our results would be of a
   lower quality than we’re targeting.




                                                      32

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The T* score is composed of three core signals, collectively referred to as the
"ABC signals," which are themselves developed and tuned by engineers.

“ABC” Signals - Anchors, Body, Clicks: Hyung-Jin “HJ” Kim (a Google search
engineer) explained in a February 2025 DOJ interview (Trial Exhibit PXR0356) that
Google’s “ABC signals are the key components of topicality (or a base score)”, which
is Google’s determination of a document’s relevance to a query justice.gov.

●​ A - Anchors: This signal is derived from the anchor text of hyperlinks pointing
   from a source page to the target document. This confirms the enduring
   importance of descriptive, relevant anchor text as a powerful signal of what
   another page on the web believes a document is about, a direct legacy of the
   principles that underpinned Google's original PageRank algorithm.
●​ B - Body: This is the most traditional information retrieval signal, based on the
   presence and prominence of the query terms within the text content of the
   document itself.
●​ C - Clicks: This signal was one of the most significant confirmations of the trial. It
   is derived directly from user behaviour, specifically defined in testimony as how
   long a user dwelled on a clicked-upon page before navigating back to the search
   engine results page (SERP). The inclusion of a direct user engagement metric at
   this foundational level of relevance scoring underscores the centrality of user
   feedback to Google's core ranking logic.

Clicks vs. Quality
One revelation was Google’s caution against using click metrics as a proxy for quality.
An internal evaluation found that “a large number of clicks on a link does not
necessarily mean that the page is of high quality.”regmedia.co.uk
Dr. Lehman explained a known issue: “It’s a very strong observation that people tend
to click on lower-quality, less-authoritative content” disproportionately. In other
words, popular clicks can sometimes go to clickbait or less trustworthy pages.
“If we were guided too much by clicks, our results would be of a lower quality
than we’re targeting,”
Lehman warned (discussing an internal slide. Google’s ranking engineers therefore
treat user click data with skepticism when it comes to authority - they use it to refine
                                                         33

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




algorithms but do not simply promote pages because they’re popular.
In fact, Pandu Nayak noted that page quality tends to be “anti-correlated” with
pure click-through rates in some cases - improving the quality of results in tests
sometimes led to fewer clicks, as users might chase clickbait even when
higher-quality info is available. This reinforces why authority signals (like PageRank
and quality scores) are crucial to keep search results genuinely trustworthy.
It is clear - Google uses some types of click signals (like dwell time for Navboost/T*) to
refine relevance but avoids using raw click volume as a direct measure of authority or
quality, as it can be misleading (e.g., clickbait).

These three were “fundamental signals” combined into a topicality score (T★) to
judge relevance justice.gov and justice.gov.

Notably, Kim said even historical user behavior - e.g. “how long a user stayed at a
particular linked page before bouncing back to the SERP” - was used as a
topical relevance signal in the past justice.gov.

Google’s ranking engineers hand-crafted the formulas for these signals (rather than
relying purely on ML) so they could understand and adjust how each factor
contributes to relevance justice.gov.

These three signals are combined in what was described as a "relatively hand-crafted
way" to generate the final T* score.

The user engagement data that powers the Navboost re-ranking system also provides
the foundational 'Clicks' signal for the T* topicality score.

The development of this system was a major engineering undertaking, described as
being in a "constant state of development" from its inception until approximately five
years prior to the testimony, indicating its maturity and foundational status within the
ranking stack.




                                                          34

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Q* Metric - A New Understanding of Site-Level Quality
The trial also brought to light a previously secret internal metric known as Q*
(pronounced "Q-star"), which functions as a measure of a website's overall quality
and trustworthiness.

This revelation is significant because it apparently confirms the existence of a
site-level quality score, something Google representatives have publicly and
repeatedly avoided confirming (in these exact terms) for years.

According to trial exhibits, Q* is "an internal metric that assesses the trustworthiness
of a whole website (most often the domain)".

A crucial characteristic of Q* is that it is largely static and query-independent.

If a website earns a high Q* score, it is considered a high-quality, reliable source
across all related topics for which it might rank.

This explains why certain authoritative domains consistently appear in search results
for a wide range of queries.

Like the T* system, Q* is described as being "deliberately engineered rather than
machine-learned," reinforcing the theme of human oversight in Google's foundational
ranking systems.

   ●​ Quality Score (Q★ - Trustworthiness): Google assigns pages a general
      quality score (often called “Q-star” or Q* internally) that reflects their
      overall credibility and utility, independent of any specific query. HJ Kim
      noted “Q (page quality, i.e., the notion of trustworthiness) is incredibly
      important”* in ranking justice.gov. This quality score is largely static (does not
      fluctuate per query) and “largely related to the site rather than the query”
      justice.gov. Kim testified that “Quality score is hugely important even today.
      Page quality is something people complain about the most.” justice.gov He
      recounted that Google formed a Page Quality team ~17 years ago (which he
      led) when “content farms” flooded search results with low-quality pages
      justice.gov. In response, Google developed methods to identify authoritative
      sources and demote the content-farm pages, improving the overall

                                                          35

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      trustworthiness of top results justice.gov. In short, Google tries to
      consistently reward pages that demonstrate experience, expertise,
      authority, and trust (E-E-A-T), and that reputation persists across
      queries. The slide contains the following text: Quality • Generally static across
      multiple queries and not connected to a specific query. • However, in some
      cases Quality signal incorporates information from the query in addition
      to the static signal. For example, a site may have high quality but general
      information so a query interpreted as seeking very narrow/technical information
      may be used to direct to a quality site that is more technical.

A key input into the Q* score is a modern, evolved version of Google's original
breakthrough algorithm, PageRank.

Testimony revealed that PageRank is still an important signal, but its function is
now framed as measuring the "distance from a known good source".

The system uses a set of trusted "seed" sites for a given topic; pages that are closer
to these authoritative seeds in the web's link graph receive a stronger PageRank
score, which in turn contributes to a higher Q*.

The confirmation of a domain-level authority score like Q* stands in stark
contradiction to years of public communications from Google.

“QRank”, Quality Scores & Authority Signals (2010s)
“‘Even the most fascinating content, if tied to an anonymous profile, simply won’t be
seen because of its excessively low rank.’ Cited to Eric Schmidt ex-Google, 2014.

In response, Google developed internal Page Quality metrics - sometimes referenced
as “QScore” or “QRank” - to judge the overall authority, expertise, and
trustworthiness of a page or site.

Google’s Hyung-Jin Kim (VP of Search) described this as a “page quality (i.e., the
notion of trustworthiness)” score, often denoted internally as Q* (“Q-star”).

He noted in testimony that “Q is incredibly important”* and that Google formed a
dedicated “Page Quality” team ~17 years ago when low-quality content farms were
proliferating justice.gov.
                                                         36

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The idea behind Q* is to algorithmically assess factors like a site’s reputation,
authority, and compliance with quality guidelines, independent of any specific
query.

Kim explained that this quality signal is “generally static across multiple queries and
not connected to a specific query”, meaning if a site is deemed high-quality and
reliable, that status boosts its rankings for all relevant searches justice.gov.

(However, query context can be factored in at times - for example, even a generally
high-quality site might be outranked by a more expert site for a very niche technical
query justice.gov.)

Crucially, Google’s modern quality score integrates PageRank as one input.

Kim confirmed that “PageRank…is used as an input to the Quality score.” justice.gov
In other words, a page’s base PageRank (its link-based importance) contributes to its
overall “authority” score Q*, alongside other factors (possibly site reputation, expert
reviews, etc.).

The Quality score thus acts as an aggregate authority metric - sometimes called an
“authority score” - that can boost or dampen a page’s search rankings.

Pages with strong Q scores (earned via trusted backlinks, original content, good user
signals, etc.) are systematically favored.

This became especially important after Google’s 2011 “Panda” update, which targeted
shallow content. Kim alluded to this, noting the team was started to tackle content
farms that “paid students 50 cents per article”, flooding Google with thin pages
justice.gov.

The solution was to algorithmically identify “the authoritative source” for a
given topic and reward it justice.gov.

In effect, Google began demoting pages that had decent link popularity but poor
overall quality, and promoting those with true authority. Kim emphasized that “Quality
score is hugely important even today. Page quality is something people complain
about the most.” justice.gov

                                                          37

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Indeed, with the rise of generative AI content, Google’s reliance on such quality
signals has only grown (“nowadays, people still complain about [quality] and AI makes
it worse”, he noted justice.gov).

How Q works internally: Google treats the quality score as a mostly
query-independent ranking factor attached to pages or sites.

It is “largely static and largely related to the site rather than the query” justice.gov -
essentially a measure of a site’s authoritative strength.

At query time, this quality score is combined with the query-dependent relevance
score. While Google hasn’t publicly detailed the formula, one can think of the ranking
system as first evaluating relevance (does the page match the keywords/intents?) and
then adjusting results based on authority/quality.

A high Q* can significantly boost a page’s position, while a low-quality score
can sink an otherwise relevant page.

In practice, Google’s regular updates and ranking tweaks often boil down to
recalibrating this “authority” component.

Notably, many signals feed into Q*: PageRank and link signals (for authority),
content assessments (for expertise), TrustRank-like signals (for
trustworthiness), and even user engagement data.

For example, internal documents indicate Google also uses a “popularity signal that
uses Chrome data” (likely aggregated Chrome usage statistics) as well as click
feedback loops like NavBoost justice.govstradiji.com.

(NavBoost, described by Google’s Dr. Eric Lehman, is essentially a big table counting
how often users click on a result for a given query over the past year stradiji.com - a
way to boost pages that searchers consistently prefer).

These additional signals are beyond PageRank, but they complement the goal of Q*:
to measure overall quality and user satisfaction.

PageRank itself, once the star of Google’s algorithm, now works behind the scenes as
one signal feeding into these broader quality and ranking frameworks.
                                                          38

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Navboost System

The Confirmed Role of Large-Scale User Interaction Data
Perhaps the most impactful revelation from the trial was the detailed exposition of the
Navboost system. This system provides the crucial 'C' (Clicks) signal for the T* score
we discussed earlier,

For years, the search engine optimisation (SEO) community has debated the role of
user clicks in ranking, with Google's public statements often being evasive or
dismissive.

The trial testimony, particularly from Google VP Pandu Nayak, ended this debate.
Navboost was confirmed to be "one of the important signals" that Google uses
to refine and prioritize search results based on a massive, historical repository
of user interaction data.

The system operates on a vast time horizon, storing and analyzing 13 months of user
interaction data to inform its signals.
This extended timeframe allows it to look beyond short-term fluctuations and identify
persistent, long-term patterns of user satisfaction, effectively using the collective
wisdom of billions of past searches to guide future rankings.
Navboost's analysis is highly nuanced, moving beyond a simple click count to classify
different types of user interactions.




                                                         39

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Leaked documents and testimony point to several key click metrics:
 ●​ Good Clicks vs. Bad Clicks: The system distinguishes between positive and
    negative interactions. A "bad click" is probably a "pogo-stick" event, where a user
    clicks a result and then immediately returns to the SERP, signalling dissatisfaction.
    A "good click," conversely, indicates that the user's need was met.
 ●​ Last Longest Click: This metric appears to be of particular importance. It
    identifies the final result a user clicks on in a search session and dwells on for a
    significant period. This interaction is interpreted as the ultimate signal of a
    successfully completed search task, making the page that received the "last
    longest click" a highly valuable result for that query context.

To provide contextually relevant results, Navboost employs several sub-systems:
 ●​ Slicing: The system segments, or "slices," its vast repository of click data by
    critical contextual factors, most notably the user's geographic location and device
    type (e.g., mobile or desktop).15 This allows Navboost to prioritise results that
    have performed well for users in a similar situation, for example, boosting a local
    business's website for mobile users in a specific city.
 ●​ Glue: This is a related, more real-time system that works alongside Navboost. The
    "Glue" system specifically monitors user interactions with non-traditional SERP
    features like knowledge panels, image carousels, and featured snippets. By
    analyzing signals such as hovers and scrolls on these elements, Glue helps
    Google determine which features to display and how to rank them, especially for
    fresh or trending queries where historical click data may be sparse.15

The primary function of Navboost within the overall ranking pipeline is to act as a
powerful, user-behavior-driven filter.

According to Pandu Nayak's testimony, after an initial retrieval stage identifies a large
pool of potentially relevant documents, Navboost is used to dramatically reduce this
set from tens of thousands down to a few hundred.

This much smaller, higher-quality set of documents is then passed on to more
computationally expensive and nuanced machine learning systems for final ranking.

A key limitation acknowledged in the testimony is that Navboost can only influence the
ranking of documents that have already accumulated click data; it cannot help rank
                                                          40

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




brand-new pages or those in niches with very low search volume.


“Authoritative, Reliable” Results Priority: Google’s witnesses underscored that the
search engine deliberately prioritizes authoritative sources in rankings. Nayak
explained that Google’s “page quality signals” are “tremendously important” because
the goal is to “surface authoritative, reliable search results” for users regmedia.co.uk.


In the same vein, Dr. Lehman testified that “our goal is to show - when someone
issues a query - to give them information that’s relevant and from authoritative,
reputable sources.”.


This philosophy was echoed throughout the trial: Google wants trustworthy content
(e.g. official sites, established experts, high-quality publishers) to rank at the top,
rather than sketchy or unverified pages, even if the latter are more crudely optimised
for a keyword.

Key Takeaways


In essence, these three systems work in concert: T establishes relevance, Q assesses
trust, and Navboost refines the results based on user satisfaction.




                                                          41

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




PageRank




Original PageRank: Link-Based Importance (Late 1990s)

Google’s original PageRank algorithm, developed by Larry Page and Sergey Brin at
Stanford, assigns an importance score to each webpage based on the web’s link
structure.

The basic idea is that a page is considered more important if many other
important pages link to it.

As Google’s early patent (Lawrence Page, U.S. Patent 6,285,999) explains, “a
document should be important (regardless of its content) if it is highly cited by
other documents. Not all citations, however, are necessarily of equal significance.

A citation from an important document is more important than a citation from a
relatively unimportant document… [Thus] the rank of a document is a function of the
ranks of the documents which cite it.” patents.google.com
                                                         42

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




In practice, the PageRank of a page A is defined recursively:

r(A)=1−dN + d∑i=1nr(Bi)L(Bi), r(A) = \frac{1 - d}{N} \;+\; d \sum_{i=1}^{n}
\frac{r(B_i)}{L(B_i)} ,r(A)=N1−d​+d∑i=1n​L(Bi​)r(Bi​)​,

where B1…BnB_1 \ldots B_nB1​…Bn​are pages linking to A, L(Bi)L(B_i)L(Bi​) is the number
of outgoing links from page BiB_iBi​, N is the total number of pages, and d is a damping
factor (usually set around 0.85): patentimages.storage.googleapis.com and
snap.stanford.edu.

In other words, “the ranks form a probability distribution over web pages, so that the
sum of all Web pages’ PageRanks will be one,” and the rank of a page can be
interpreted as “the probability that a random web surfer ends up at the page after
following a large number of forward inks.”: patentimages.storage.googleapis.com

Because a random surfer occasionally jumps to a random page with probability (1–d),
even pages with few links can get some baseline rank.

This elegant link analysis makes PageRank an objective measure of a page’s citation
importance.

As Brin and Page noted in their 1998 research paper, “PageRank…corresponds well
with people’s subjective idea of importance. Because of this correspondence,
PageRank is an excellent way to prioritize the results of web keyword searches.”:
snap.stanford.edu

How it was used: In Google’s early search engine, PageRank was a core ranking
signal used to “prioritize” or weight search results. Google even had Google Toolbar
Updates back in the day.

Pages with higher PageRank (i.e. more or better-quality backlinks) tended to rank
higher in the “10 blue links” results, all else being equal.

PageRank was computed offline by iteratively propagating link weights, and Google
updated these scores periodically.

By the early 2000s, Google even exposed a rough 0–10 PageRank score via the
browser Toolbar, underscoring how central it was to ranking.
                                                          43

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Importantly, even from the start Google recognized that PageRank was one signal
among many - it improves relevance when combined with content-based scoring.

Nonetheless, it became the foundation of Google’s ranking, embodying the principle
that “links…are votes of support” and that pages “endorsed by many high-quality
sites” should be ranked as more authoritative.

PageRank (Link-Based Authority) - What SEOs said at the time

   ●​ Key Internal Details (Google): Google’s original ranking algorithm PageRank
      assigns each page a numerical importance score based on backlinks. In Larry
      Page’s formulation, a page’s rank is calculated from the ranks of pages linking
      to it hobo-web.co.uk. PageRank is query-independent - it condenses the entire
      web’s link graph into a “global ranking of all Web pages, regardless of content,
      based solely on backlinks” patents.google.com. Early on, Google noted that
      even low-quality pages contribute a minimum PageRank, so creating many
      interlinked dummy pages could artificially inflate a target page’s score
      patents.google.compatents.google.com. Google addressed link spam with later
      patent tweaks (e.g. weighting links from sites with many pages)
      patents.google.com, but PageRank remained a core baseline in the ranking
      system gofishdigital.com.​

   ●​ Observations (Bill Slawski, Jim Boykin) : Some SEOs recognised PageRank’s
      central role and pitfalls. As early as 2007 on an article I commented on, Jim
      Boykin discussed “old BackRub techniques with some TrustRank thrown in,”
      acknowledging the link-vote model behind ranking
      internetmarketingninjas.com. Bill Slawski frequently analyzed Google’s link
      algorithms, noting the vulnerability of PageRank to spam farms and reciprocal
      “endorsement” loops patents.google.com. He explained that many low-value
      links can still boost a page since “every linking page is guaranteed to have a
      minimum PageRank… links from many such low quality pages can still inflate the
      PageRank score.” patents.google.com Slawski also highlighted Google’s
      attempts to dampen manipulation, like the “reasonable surfer” model giving
      different weight to links patents.google.com. At the time, we advised that
      PageRank is essentially a measure of link-derived authority - “rank assigned to

                                                         44

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      a document is calculated from the ranks of documents citing it”
      hobo-web.co.uk - a point that aligned exactly with Google’s own definition.​

   ●​ Notable Quotes/Metaphors: Bill often described backlinks as votes or peer
      reviews. He quoted Google’s description that PageRank uses “information
      external to webpages - their backlinks - which provide a kind of peer review.
      Backlinks from ‘important’ pages are considered more significant… by
      recursive definition.”patents.google.com This metaphor of link votes
      anticipated Google’s internal thinking. Myself, in practical guides, emphasized
      that “Google has long worked [by displaying] organic results based on
      KEYWORDS and LINKS” hobo-web.co.uk - effectively telling SEOs that link
      authority (PageRank) still underpins rankings.​

   ●​ Accuracy in Hindsight: While we didn't know for sure at the time, our
      perspectives on PageRank were highly accurate. PageRank indeed proved to
      be the foundational ranking factor Google used, and their advice to acquire
      quality backlinks was prescient. Slawski’s early warnings about link spam mirror
      the tactics Google fought internally patents.google.com. Over time Google
      integrated many other signals, but as late as the DOJ trial (2023) it was
      confirmed that PageRank (or its derivatives) remains in use.

TrustRank: Incorporating Link Trust & Spam Protection (2004–2000s)

As the web grew, link spam (artificial link networks or “link farms”) began to
undermine PageRank’s reliability. In response, researchers (including some later
Googlers) developed TrustRank, an evolution of PageRank that emphasizes link
trustworthiness over raw link popularity.

A Google patent on link-spam detection defines TrustRank as “a link analysis
technique related to PageRank” and “a method for separating reputable, good pages
on the Web from web spam.” It works on the presumption that good (non-spam)
websites seldom link to spam sites patents.google.com.

TrustRank involves two steps: first, human experts identify a small seed set of highly
trustworthy pages; second, a score propagation algorithm spreads a “trust score”
outwards through the link graph.
                                                         45

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




As the patent explains, “TrustRank involves two steps, one of seed selection and
another of score propagation. [Thus] the TrustRank of a document is a measure of the
likelihood that the document is a reputable (i.e., non-spam) document.”
patents.google.com

Google implemented this concept internally to downweight webspam and promote
authoritative content.

Rather than counting all backlinks equally, links from a trusted seed page confer more
value.

In effect, this is like running a biased PageRank that starts from trusted nodes.

A later Google patent describes “select[ing] a few ‘trusted’ pages (also referred to as
seed pages) and [finding] other pages likely to be good by following the links from the
trusted pages.” patents.google.com

By crawling outward from a set of “high-quality seed pages” and measuring link
distance (hops or weighted path length) to other pages, Google can compute a trust
score for each page based on proximity to trusted sites.

Pages closely linked to the trusted seeds receive higher trust scores, while those deep
in the link graph or mainly linked from untrusted sources are deemed less reliable.

This distance ranking approach was patented by Google and reduces the influence
of spam farms: “good documents on the Web seldom link to spam” and thus spam
pages end up many link-hops away from the reputable core patents.google.com.

In practice, Google could use TrustRank to demote or filter pages with high PageRank
but low trust.

One Google filing notes that the system may compute a “discrepancy between the
link-based popularity (e.g. PageRank) and the trustworthiness (e.g. TrustRank) of a
given web document” to catch artificially boosted pages patents.google.com.

In essence, a page with many inbound links might still rank poorly if those links come
from low-trust sources.

                                                          46

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By the late 2000s, Google’s ranking algorithm quietly incorporated such link quality
assessments to complement raw link count, reinforcing the mantra that not all links
are equal.

Usage: TrustRank (and related “link distance” signals) are used internally as part of
Google’s ranking and anti-spam systems. Though Google did not publicly call it
“TrustRank” by name, Google engineers have affirmed the concept. For example, a
Google patent by the company’s researchers explicitly describes using “a seed set of
reputable documents” with trust values, then propagating those trust values to linked
pages.

This helps assign each page a trust score that can modify its ranking. In summary,
TrustRank evolved PageRank by adding a notion of link reliability, ensuring that a
page’s rank reflects not just quantity of links, but the quality and trustworthiness of
those link sources.

   ●​ Observations (Bill Slawski): Bill Slawski closely followed Google’s moves on
      trust. He noted that “Google Trustrank is very different from Yahoo TrustRank…
      Yahoo’s TrustRank [identifies] spam, whereas Google developed a system for
      reordering rankings of web pages” based on trust signals
      seonorth.caseonorth.ca. Years before Google’s trial revelations, Slawski
      discussed patents on using trusted seed sites to influence rank. He cited one
      Google patent wherein “the system…assigns lengths to links…computes the
      shortest distances from seed pages to each page…[and] determines a ranking
      score for each page based on the computed shortest distances.”
      gofishdigital.com In plainer terms, Bill explained that pages closer (in link hops)
      to authoritative sites would rank higher, capturing the essence of “TrustRank”
      as “distance from authority sites”.
   ●​ Slawski explicitly connected Google’s trust metrics to the “distance
      between documents.” He highlighted that Yahoo’s TrustRank “diminishes with
      increased distance between documents”, requiring carefully chosen seed sets
      seonorth.caseonorth.ca - a concept Google mirrored. In a 2019 analysis, Bill
      wrote that Google had a patent for ranking based on how “close or distant
      [pages] might be to a set of trusted seed sites” gofishdigital.com. This “seed
      set distance” metaphor was essentially Bill translating Google’s internal method
      into SEO-friendly terms. I often spoke of “authority” in a similar vein - often
                                                         47

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      referencing Bill (and Jim Boykin) - and recommending getting links from .gov or
      .edu sites and communities’ hubs because those confer trust (a notion very
      aligned with TrustRank).​

   ●​ Bill Slawski (RIP) effectively reverse-engineered Google’s thinking through
      patents, identifying that Google sought a “trust score” to combat low-quality
      results. This was confirmed when Google’s Pandu Nayak later revealed the
      addition of an explicit quality/trust metric around 2011 to address content farm
      issues hobo-web.co.uk. My own long-standing emphasis on site credibility,
      authoritative backlinks, and user trust anticipated Google’s E-A-T (Expertise,
      Authoritativeness, Trustworthiness) philosophy. In hindsight, this guidance
      some of us shared at the time to “be closer to trusted authorities” (both
      literally in link graphs and figuratively in reputation) was accurate enough. We
      (along with many others like Rand Fishkin at the time accurately
      predicted/documented that Google was integrating trust evaluations into
      ranking - something the DOJ trial exhibits and Google patents have since made
      evident.

PageRank’s Role Today

Even as Google’s algorithm has become vastly more complex, it still uses PageRank
internally in 2025 - but as one factor among hundreds, and usually mediated through
higher-level scores like Q*.

Google’s own public documents affirm this.

In a 2019 white paper on combating disinformation, Google noted that “the best
known of these signals is PageRank, which uses links on the web to understand
authoritativeness.” searchengineland.com

In other words, link-based authority (PageRank) remains a fundamental signal for
evaluating a page’s trust and expertise.

Google’s search engineers continue to value the “distance from a known good
source” that PageRank-style algorithms provide justice.gov.


                                                         48

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




However, they no longer rely on raw PageRank alone. As Google’s John Mueller
explained, modern ranking is “not just PageRank of course…there are lots of different
topics in there and PageRank is more or less a side comment.” searchengineland.com

PageRank has effectively been subsumed into composite metrics like quality score
and into specific applications (e.g. identifying authoritative seed sites, boosting
trusted domains, etc.).

In summary, PageRank’s evolution over two decades reflects Google’s shifting focus
from quantity of links to quality of content and trust.

The original PageRank algorithm (circa 1998) introduced the paradigm of ranking by
link popularity (with damping factor ~0.85 to model random surfing)
snap.stanford.edu.

TrustRank and related link-distance algorithms (mid-2000s) built on this by
prioritizing links from vetted “trusted” pages and demoting spam, under the principle
that “good pages seldom link to bad ones.”patents.google.com

And in the 2010s, “QRank” or page quality scores further blended PageRank with
numerous other signals to measure a page’s true authority and reliability, addressing
content quality issues beyond links.

Today, Google’s ranking uses a sophisticated mix of these factors:

PageRank is still there under the hood, informing the algorithm about the link-based
authority of pages justice.gov, but it operates in concert with semantic relevance
models, machine learning systems (like RankBrain and BERT-based RankEmbed
stradiji.com), user feedback metrics, and domain-level quality evaluations.

As a result, Google Search can surface results that are not only popular in the link
graph, but also trusted, expert, and satisfying - fulfilling the original goal of
PageRank (“bringing order to the web” by leveraging links snap.stanford.edu) while
adapting to the modern web’s challenges.




                                                         49

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Key Take-aways

PageRank & Link-Based Authority: One of the oldest authority signals is Google’s
famous PageRank algorithm, which treats links as “votes” of confidence.

Kim described PageRank as “a single signal relating to distance from a known good
source” - essentially measuring how far removed a webpage is from trusted,
reputable sites on the web justice.gov.

In the trial, he confirmed that Google “uses [PageRank] as an input to the Quality
score.” justice.gov In practice, a page linked by many high-authority sites will inherit
some authority itself.

This link-based authority is one component of the overall page quality/Q★
score. (For example, a university or government site linking to a page conveys a level
of trustworthiness to that page.) By feeding PageRank into the quality metric, Google
combines traditional link popularity with other quality assessments to rank
authoritative content higher.

“I asked Gary (Illyes from Google) about E-A-T. He said it’s largely based on links
and mentions on authoritative sites. i.e. if the Washington post mentions you, that’s
good. He recommended reading the sections in the QRG on E-A-T as it outlines things
well.” Marie Haynes, Pubcon 2018




                                                         50

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Panda and Google’s Site-Level Quality Score (QScore)

One of the revelations from the U.S. Department of Justice antitrust trial against
Google (2023) was confirmation of an internal metric often called “QScore” or “Q”* -
essentially the continuation of Panda’s site quality score concept.

In trial exhibits, a Google search engineer described Google’s ranking signals at a high
level, explicitly highlighting a Quality signal (Q) that is “generally static across
multiple queries and not connected to a specific query” justice.gov.

This quality score incorporates various factors to gauge a site’s trustworthiness and
authority.

The engineer stressed that “Q … is incredibly important”* and that competitors would
love to decode it justice.gov. In fact, he noted “Quality score is hugely important even
today. Page quality is something people complain about the most.”justice.gov -
underscoring that Google invests heavily in getting this right because low-quality
content undermines user trust in search results.

Crucially, the testimony linked QScore’s origin to the Panda era: “HJ [Hyung-Jin]
started the page quality team 17 years ago… around the time when the issue with
content farms appeared… Google had a huge problem with that. That’s why Google
started the team to figure out the authoritative source.”justice.gov. This places the
genesis of the quality score around 2008 or so, leading up to Panda’s launch in 2011
to fight content farms. It confirms that Panda was essentially the first implementation
of Google’s site-wide quality scoring. The QScore (often denoted internally as Q or
Q*) is the modern incarnation of Panda’s score, now deeply integrated into ranking.

Another insight from the trial is that the quality score can be “easily reverse
engineered” if one had access to enough data, because it is “largely static and
largely related to the site rather than the query.” justice.gov. This static nature is what
makes it powerful - it acts as a constant site reputation metric. But it also means if an
outsider could figure out what Google’s quality score for each site is (e.g., by
analyzing large amounts of search results across queries), they might infer
which sites Google algorithmically trusts the most.

Google guards this closely.
                                                          51

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The quality score (Q) isn’t just Panda’s content analysis now; it appears to be a
composite metric. The trial doc mentions that “PageRank… is used as an input to the
Quality score.”justice.gov. It also alludes to a “popularity signal that uses Chrome
data” justice.gov likely feeding into quality. In other words, QScore today probably
combines: content quality signals (Panda), link-based authority (PageRank), user
engagement in serps or traffic signals (Chrome/Android data, etc.), and perhaps other
trust signals (brand recognition, factual accuracy measures, etc.). Think of QScore as
Google’s internal measure of a site’s overall value to users. Panda was the
foundation of that score, focusing on content quality. Over time, Google has layered
on more inputs. But when SEOs talk about “Panda” or “site-wide quality,” they are
essentially talking about this QScore system.

It’s notable that even with modern AI advancements in search, Google still relies on a
concept of site quality.

The engineer in 2025 said, “Nowadays, people still complain about [content] quality
and AI makes it worse.” justice.gov - indicating Google’s quality algorithms
(Panda/QScore) are continually being challenged by new waves of low-effort content
(like AI-generated spam).

Yet the core principle remains: trustworthy, authoritative sites are algorithmically
scored higher; deceptive or low-value sites get scored lower.

Google’s ranking system then uses this as a significant factor. In the same exhibit, the
importance of authority is emphasized: if competitors learned Google’s quality scores,
“they have a notion of ‘authority’ for a given site” justice.gov, implying that QScore
correlates with a site’s perceived authority/trust.

So, Panda’s legacy is that “authority” is not just about links anymore, but about
content quality and user trust at the site level.

Impact and Legacy of Panda

The Panda update had immediate and far-reaching effects on the web.

Many well-known “content farm” style sites saw dramatic drops in visibility overnight
in 2011. For example, Demand Media’s eHow (which had tens of thousands of short
                                                         52

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




how-to articles) was reportedly hit, as were sites like Suite101 and Mahalo - to the
point that “the web [was] still buzzing about its implications” weeks after, noted Wired
in March 2011 wired.com.

Conversely, Panda benefited “established sites known for high-quality information”
wired.com - e.g., mainstream news outlets, government sites, medical journals, etc.,
saw their rankings improve relative to lower-quality competitors.

One anecdote shared by Cutts: Before Panda, someone searching about a medical
condition found “content farms were ranking above government sites” for that query.
After Panda, “the government sites are ranking higher,” which was a desired outcome
wired.com.

This highlights how Panda shifted the balance towards authority and reliability.

There were some unintended casualties as well. Some sites with mostly good content
but a few weak spots got caught in the dragnet.

For instance, affiliate websites with thin product pages, forums or Q&A sites with
lots of user-generated content (some of which might be low quality), or small
businesses with mostly great pages but a few duplicate pages - some of these felt
Panda’s sting.

Google’s advice to them was consistent: improve the overall quality of your site (or
remove the bad parts) and you can gradually recover as the algorithm reassesses you
developers.google.com.

Over time, many such sites did recover by following quality best practices.

From an industry perspective, Panda was a wake-up call. It put an end to the era of
spammy SEO tricks like mass-producing keyword-stuffed pages or scraping content
from other sites to get easy traffic.

It pushed publishers to focus on content excellence and user experience. It also
spawned the concept of “Panda-proof” content strategies, emphasizing depth,
originality, and user trust.


                                                         53

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




By integrating Panda into the core ranking system, Google essentially made quality a
permanent, always-on ranking factor.

Today, whenever Google rolls out a “core update” (as it does several times a year),
sites that see gains or losses often are feeling the effect of tweaks to these quality
evaluations (among other things).

Google itself has said core updates “may cause some sites to notice drops or gains”
and that “there’s nothing wrong with pages that may now perform less well… Instead,
it’s that changes to our systems are benefiting pages that were previously
under-rewarded” - often referring to quality improvements in the algo.

This is very much in line with Panda’s original mission.

In summary, Panda’s legacy is the notion that “overall site quality” matters
tremendously for SEO, not just the relevance of a single page or the number of links.

It ushered in an era where Google is far better at weeding out thin content and
boosting authoritative sources, and it laid the groundwork for future improvements in
evaluating content quality at scale.


Google’s Panda Algorithm - Site-Level Quality Scoring
Google’s Panda algorithm was a major search ranking system introduced in early 2011
with the goal of dramatically improving search result quality.

It was launched to reduce rankings for “low-quality sites” - pages that are
“low-value add for users, copy content from other websites or… just not very useful” -
while rewarding high-quality sites with original, in-depth content
googleblog.blogspot.com.

The initial Panda update impacted nearly 12% of all Google queries
googleblog.blogspot.com, making it one of the most significant algorithmic changes
in Google’s history.




                                                          54

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Internally, Google engineers actually nicknamed the project “Panda” after one of the
key engineers (Navneet Panda) who developed a breakthrough technique for
evaluating site quality wired.com.

This was brought to all our attention at the time, again by Bill Slawski.

Panda fundamentally changed how Google assesses website content quality and
introduced a new site-wide quality score into the ranking process, complementing
traditional signals like PageRank.

Origins and Purpose of Panda

By 2010, Google’s search team was facing widespread criticism that “content farms” -
sites churning out large volumes of shallow, low-value content - were dominating
search results at the expense of higher-quality sites wired.com.

Google’s own Amit Singhal (then head of Search Quality) described how after the
2009 Caffeine indexing update, Google’s fresher and bigger index began to surface a
new class of problem: “The problem had shifted from random gibberish, which the
spam team had… taken care of, into… written prose, but the content was shallow”
wired.com.

As Google’s spam chief Matt Cutts put it, content farms were essentially looking for
“what’s the bare minimum that I can do that’s not quite spam?”, slipping through the
cracks of earlier spam filters wired.com.

These sites weren’t outright violating old rules, but produced thin content that
frustrated users.

In early 2011, Google assembled a team (led by Singhal and Cutts) to tackle this gap in
quality.

     “We’ve been tackling these issues for more than a year… working on this
     specific change for the past few months.” googleblog.blogspot.com

Google’s solution was the Panda algorithm update (initially nicknamed “Farmer”
externally, until Google revealed the internal name Panda).

                                                          55

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Panda’s original purpose was to algorithmically assess website quality and
down-rank sites with thin or low-quality content, especially content farms -
googleblog.blogspot.com and wired.com.

“This update is designed to reduce rankings for low-quality sites… At the same time, it
will provide better rankings for high-quality sites - sites with original content and
information such as research, in-depth reports, thoughtful analysis and so on.”
googleblog.blogspot.com explained Google’s official blog when Panda first launched.

In other words, Panda introduced a site-level quality classifier into Google’s ranking
algorithms - something very different from earlier ranking systems that had mostly
focused on individual page relevance and link-based authority signals.

What Panda introduced was new: Prior to Panda, Google’s ranking relied heavily on
signals like PageRank (link popularity), topical relevance to the query, and a variety of
spam filters for blatant abuses.

There was no robust mechanism to judge the overall quality of content on a site.

Panda changed that by introducing a sort of “content quality score” at the site level.

This meant that if a site had a lot of low-quality pages, the entire site’s rankings
could be demoted - a sharp departure from the earlier page-by-page focus.

Google explicitly acknowledged this shift: “Our site quality algorithms are aimed at…
reducing the rankings of low-quality content.

The recent ‘Panda’ change tackles the difficult task of algorithmically assessing
website quality.” developers.google.com and developers.google.com.

In a Q&A, Matt Cutts confirmed that Panda was developed to catch what earlier
algorithms missed: “It sort of fell between our respective groups [the search quality
team and the spam team]. And then we decided, okay, we’ve got to come together
and figure out how to address this.” wired.com.

Notably, Google has said Panda was initially aimed squarely at content farms and
similar low-quality sites.

                                                          56

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




In a DOJ antitrust trial exhibit, a Google engineer (HJ Kim) reflected that Panda’s
beginnings coincided with “the time when the issue with content farms appeared.
Content farms paid students 50 cents per article… Google had a huge problem with
that. That’s why Google started the [page] quality team to figure out the authoritative
source.” justice.gov and justice.gov.

Panda was the result of that effort. Over time, its scope expanded beyond just
“content farms” to any site with poor-quality content.

But its core purpose remained: ensure that useful, trustworthy, and authoritative
websites rank above those with thin or unsatisfying content.




                                                         57

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Assessing Site Quality

Panda works by assigning a quality score to an entire site (or a large section of a
site), and using that score as a ranking factor.

Unlike keyword relevance or link-based metrics, this is a broader measure of how
beneficial and trustworthy a site’s content is to users.

According to a Google patent (co-invented by Navneet Panda) on “predicting site
quality,” Google’s system can “determine a score for a site… that represents a
measure of quality for the site”, and this “site quality score for a site can be used as
a signal to rank… search results… found on one site relative to… another site.”
patents.google.com and patents.google.com.

In other words, Panda’s output is essentially a site-wide quality score (like QScore or
*Q internally) that can boost or dampen all pages from that site in search rankings.

Training the Quality Classifier: To build Panda, Google took a very data-driven,
“scientific” approach. Amit Singhal explained that Google first defined what “high
quality” vs “low quality” means by using human quality raters. “We used our standard
evaluation system… we sent out documents to outside testers (raters). Then we asked
the raters questions like: ‘Would you be comfortable giving this site your credit card?
Would you be comfortable giving medicine prescribed by this site to your kids?’”
wired.com. Google’s engineers compiled a rigorous list of questions to probe a site’s
credibility and value.

According to Matt Cutts, “There was an engineer who came up with a rigorous set of
questions, everything from ‘Do you consider this site to be authoritative? Would it be
okay if this was in a magazine? Does this site have excessive ads?’” wired.com. These
and similar questions (which Google later shared publicly as guidance) cover things
like: Is the content written by an expert? Is it original, insightful, and more than just
superficial? Does the site have duplicate or overlapping pages on the same topics? Is
the content free of stylistic or factual errors? Would you trust this site for your money
or your life? Would you expect to see this in a reputable publication? Are there too
many ads? Is the content short or lacking in substance? developers.google.com and


                                                          58

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




developers.google.com. By collecting many such ratings, Google essentially built a
dataset of websites labeled “high quality” or “low quality” based on human judgment.

Next, machine learning was applied to this data. As Cutts described, “we actually
came up with a classifier to say, okay, IRS or Wikipedia or New York Times is over on
this side, and the low-quality sites are over on this side” wired.com. In simple terms,
Google extracted a variety of measurable features from websites (page content,
patterns of word usage, duplication, user engagement signals, etc.) and trained a
classifier that could predict a site’s quality rating.

Singhal gave a metaphor: “You can imagine in a hyperspace a bunch of points, some
points are red [low quality], some points are green [high quality]… Your job is to find a
plane which says that most things on this side… are red, and most… on that side… are
green.” wired.com. This is essentially how the Panda algorithm works internally - it
uses a machine-learned model to separate good vs. bad, based on many input
signals.

One specific approach revealed in Google’s patent is a phrase-based site quality
model.

The patent describes generating a “phrase model” that looks at the relative frequency
of various n-grams (word sequences) on a site patents.google.com and
patents.google.com.

Certain phrases or patterns of phrasing tend to correlate with higher or lower quality
content. (For example, one could imagine “how to make money fast” appearing
frequently on low-quality sites, whereas “references” or “methodology” might
correlate with higher-quality, research-oriented sites - this is a hypothetical
illustration.)

The system uses a large set of “previously scored” sites (from the human ratings) to
learn the phrase frequency characteristics of good vs bad sites patents.google.com
and patents.google.com.

Then for any new site, Google can compute a predicted quality score by analyzing its
content in terms of these phrase-based features patents.google.com and
patents.google.com.
                                                          59

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Importantly, this process is fully automated: “Site quality scores representing a
measure of quality for sites… can be computed fully automatically”
patents.google.com and then used by Google’s ranking engine as an input
patents.google.com.

Panda does not require manual intervention once the model is in place; it continuously
evaluates sites as Google crawls and indexes their content.

What signals does Panda specifically use? Google has never published the exact
formula (to prevent gaming the system), but the guiding questions and patents give
strong clues.

Content depth and originality are clearly important - sites with “shallow” or
“short, unsubstantial” content are flagged as low quality developers.google.com.

Duplication or mass-produced content is a negative signal - e.g. “duplicate,
overlapping, or redundant articles on the same topics with slightly different keywords”
hurts quality developers.google.com.

Trust and authority signals matter - if experts or authoritative sources write the
content, that’s positive developers.google.com.

If the site is recognised as a go-to authority in its field (or would be cited in print),
that’s a plus developers.google.com.

User experience factors like excessive advertising, poor layout, or lots of distracting
elements can indicate low quality developers.google.com.

Basic writing quality - correct grammar, no blatant factual errors - also feeds into
perceived quality developers.google.com.

Panda likely also considers engagement metrics indirectly (Google has hinted that it
did not directly use Chrome toolbar or Analytics bounce rates for Panda, but it’s
plausible that sites users tend to block or avoid correlate with Panda scores - indeed
Google found an 84% overlap between sites that users most frequently manually
blocked via a Chrome extension and the sites Panda flagged as low quality
wired.com).

                                                          60

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Crucially, Panda’s quality score is applied site-wide (or section-wide). This means if
a significant portion of your site’s pages are low quality, the entire site can be
demoted in Google results.

Google warned that “low-quality content on some parts of a website can impact the
whole site’s rankings” developers.google.com.

In practice, Panda acts as a sort of penalty (or dampener) on an entire domain if the
overall quality is judged to be poor.

Conversely, high quality sites get a boost across all their pages. This site-level
approach was new - earlier algorithms mostly evaluated pages individually.

A Google engineer in the antitrust trial described this quality signal (internally called
QScore or Q): “Q (page quality, i.e. the notion of trustworthiness) is incredibly
important… Q is largely static and largely related to the site rather than the query.”*
justice.gov and justice.gov. “Static” here means the quality score doesn’t change
based on each query; it’s an overall property of the site. So if Panda deems a site
low-quality, that site will tend to rank lower on all queries, no matter the topic, until the
quality improves. This was a significant change that incentivized webmasters to
improve the entirety of their site’s content, not just individual pages.

It’s worth noting that Google’s PageRank (link popularity) was even folded into this
quality scoring mechanism. The trial documents reveal that “PageRank… is used as an
input to the Quality score.”justice.gov In other words, Google’s site quality classifier
doesn’t ignore links - a site widely cited on the web (high PR) likely gets some benefit
in the quality score as well, perhaps as a proxy for authority. And Google likely uses
many other signals (possibly user satisfaction metrics, brand mentions, etc.) in the
quality score beyond just the content analysis that Panda started with. Panda was the
pioneering system for this kind of site-level evaluation, and over time Google has
continued to refine it into a broader “quality” framework.




                                                          61

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Evolution of the Panda Algorithm

After its initial launch in February 2011 (sometimes referred to as Panda 1.0), the
Panda algorithm went through numerous iterations and improvements over the years.

In the beginning, Panda updates were run periodically as “data refreshes” or new
versions that Google would announce every so often (monthly or bi-monthly in
2011-2012).

Notable milestones in Panda’s evolution include:

   ●​ Panda 2.0 (April 2011) - This update extended Panda’s impact beyond the U.S.
      and also started incorporating new signals, including user feedback signals.
      Google said at the time: “We’ve rolled out this improvement globally to all
      English-language Google users, and we’ve also incorporated new user
      feedback signals… In some high-confidence situations, we are beginning to
      incorporate data about the sites that users block into our algorithms.”
      developers.google.comdevelopers.google.com. This showed that Google was
      fine-tuning Panda by using real user behavior (perhaps like the Chrome
      blocklist data) to validate and adjust the algorithm. Panda 2.0 also “goes
      deeper into the ‘long tail’ of low-quality websites” to catch poorer results that
      the first version might have missed developers.google.com. The impact of
      these tweaks was smaller (~2% of queries affected, vs ~12% for the original)
      developers.google.com.​

   ●​ Ongoing Panda Updates (2011–2013) - Google continued to release Panda
      updates, numbered sequentially (Panda 3.0, 3.1, … etc.), improving the classifier
      and refreshing the data. Many of these were minor adjustments. Google
      sometimes quietly rolled them out; webmasters would notice ranking
      turbulence, and Google would later confirm a Panda update happened. The
      goal remained the same: refine the quality signals to more precisely demote
      only the truly “low-value” sites and let genuine quality sites rise. For example, a
      Panda update in 2012 targeted scraper sites (sites that plagiarize content)
      more effectively. By 2013, there were over two dozen Panda iterations.​



                                                         62

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




●​ Major Panda 4.0 (May 2014) - This was a significant update to Panda’s
   algorithm. Google’s Pierre Far described it as “a new Panda update” that
   incorporated some new signals and was supposed to be gentler, allowing some
   previously penalized sites to escape if they had improved. He mentioned it
   “added a few more signals to help Panda identify low-quality content more
   precisely” sitecenter.com. Panda 4.0 impacted roughly ~7.5% of English queries
   (per Search Engine Land reports) - still a big change. Notably, some large sites
   were hit hard or saw gains. For instance, eBay famously lost rankings in this
   timeframe (likely due to thin content on many eBay pages), while sites with
   robust content saw improvements sitecenter.comsitecenter.com.​

●​ Panda 4.2 (July 2015) - Google announced what turned out to be one of the
   last discrete Panda updates. Uniquely, Panda 4.2 was a very slow, gradual
   rollout, taking months to fully propagate. It affected an estimated 2–3% of
   queries sitecenter.com. Google hinted that this slow rollout was to minimise
   shock and perhaps to integrate Panda more deeply into the “core” ranking
   system.​

●​ Integration into Core Algorithm (January 2016) - At the start of 2016,
   Google confirmed that Panda had been incorporated into Google’s core
   ranking algorithm. This means Panda was no longer a separate filter run
   occasionally; it became part of the main ranking pipeline, evaluating sites
   continuously. Practically, this implied that Panda’s quality scoring would be
   updated in real-time (or near real-time) as Google crawls the web, rather than
   in big waves. “In January 2016, Google integrated Panda updates into its ‘core’
   algorithm, signaling that changes in the way they prevent poor-quality websites
   from ranking would now happen on an incremental, ongoing basis,” rather than
   sudden large updates sitecenter.com.​
   ​
   However, “core integration” did not mean Panda started to act instantly on
   every new piece of content. Gary Illyes of Google clarified that Panda in core
   still isn’t purely real-time in the way something like indexing is. It is more that
   Panda’s data gets refreshed more continuously, but there may still be some
   delay as the system accumulates enough data about a site. Still, from 2016
   onward, Google stopped announcing Panda hits or recoveries - it’s always
                                                      63

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   running in the background.​

●​ Post-2016 and Modern Updates - After Panda became part of the core
   algorithm, Google shifted to talking about broader “core updates” which can
   encompass multiple factors (including quality). Panda as a standalone name
   faded from public discussion, but its concept lives on strongly in Google’s
   approach to search. In fact, internal testimony in 2023–2024 (DOJ v. Google
   trial) makes clear that site quality scoring is still a crucial part of Google’s
   ranking formula. One Google search engineer noted in 2023 that “Quality score
   is hugely important even today. Page quality is something people complain
   about the most.” justice.gov and that Google continuously works on it
   (especially as new problems like AI-generated spam arise justice.gov). In the
   years since Panda’s integration, Google also introduced other quality-related
   algorithms - for example, the “Medic” update (August 2018) which seemed
   to emphasize E-A-T (Expertise, Authority, Trustworthiness) on “Your Money or
   Your Life” sites, and the Helpful Content Update (2022) which targets
   unhelpful, low-value content. These can be seen as spiritual successors to
   Panda, targeting content quality issues in more modern contexts. But it’s likely
   that much of the original Panda philosophy (and perhaps even code) underpins
   these systems, all contributing to that overall quality score (QScore) for sites.​

●​ Continuous Improvements - Google has repeatedly stated that it keeps
   refining its quality algorithms. “We will continue testing and refining the
   change… as we have more to share,” wrote Singhal during Panda’s rollout
   developers.google.com. This includes adjusting the weighting of the quality
   score, tuning what features the classifier pays attention to, and making it
   harder to game. Google also uses core updates to address edge cases or false
   positives. For instance, some sites that were unfairly hit by Panda (because
   they had a few thin pages dragging down an otherwise decent site) might
   recover in subsequent updates as the algorithm improved. By integrating Panda
   into core, Google essentially made quality assessment a permanent,
   ever-evolving part of search ranking.​




                                                      64

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




One important aspect of Panda’s evolution is how Google handles manual
exceptions or overrides. Google has been adamant that Panda (and its successors)
are purely algorithmic.

In the early Panda days, Google allowed webmasters to submit a reconsideration
request if they thought they were hit unfairly, but Google would use that feedback
only to improve the algorithm, not to manually boost individual sites. “We aren’t
making any manual exceptions [for Panda], we will consider [feedback] as we
continue to refine our algorithms,” Google said developers.google.com.

This has largely remained true - recovery from Panda comes from fixing your site, not
from appealing to Google.




                                                         65

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Deconstructing Site Quality via Exploit
Mark Williams Cook, of Candour did some exceptional work in this area and I was lucky
enough to chat with him about it at the time - “The endpoint exploit we found
literally had a metric called "site_quality" which at minimum determined if you
got some kind of rich results”.

Insights from a fascinating talk on "Conceptual Models of SEO" reveal a deeper, more
nuanced layer to how Google evaluates websites, moving far beyond traditional
metrics like keyword density or backlink counts.

Based on data allegedly retrieved from a Google exploit, the speaker outlines a
compelling case for a master metric: a "Site Quality Score."

This score appears to function as a foundational assessment of a site's authority,
directly impacting its ranking potential and eligibility for prominent search features.

A Foundational Ranking Gate

The core of the discovery is a "Site Quality Score" that Google allegedly
calculates for every single website, scored on a scale from 0 to 1 at the
subdomain level.

This isn't just another data point; it acts as a critical qualifier.

The speaker revealed a specific threshold: sites with a quality score below 0.4 were
found to be ineligible for Rich Results like Featured Snippets or "People Also Ask"
boxes.

This implies that no amount of on-page optimization for these features will succeed if
a site hasn't first passed this fundamental quality check.

It's a heat race you must qualify for before you can even compete.




                                                          66

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Measuring Trust: How Site Quality is Calculated

So, what constitutes this all-important score? According to Mark, who likes to
reference Google patents, like myself, the calculation depends on whether Google has
sufficient user data for a site.

   1.​ For Established Sites: The score is calculated based on signals that measure a
       site's real-world brand authority. Google looks at how many times users
       specifically search for your brand or domain name, how often they select
       your site in the search results even when it isn't ranked number one, and
       how often your brand name appears in anchor text across the web. In
       essence, Google is measuring your reputation and the trust users place in you.
   2.​ For New or Obscure Sites: When user data is scarce, Google uses a predictive
       model. It analyzes the content on your pages to create a "phrase model"—a
       numerical representation or "shape" of your website. It then compares this
       profile to the profiles of sites for which it already has established quality
       scores. It predicts how good your site is likely to be based on its resemblance
       to known, trusted entities.

The AI Dilemma and the Helpful Content Correction

This predictive model had a significant vulnerability, which the speaker argues led to a
massive influx of low-quality, AI-generated content ranking highly in 2022.

Because Large Language Models (LLMs) are trained on vast amounts of high-quality
text, the content they produce naturally mimics the "numerical shape" of a good site,
effectively tricking the predictive site quality model. Brand new sites could publish
thousands of AI-generated pages and be initially judged as high-quality, leading to a
surge in traffic.

Google's fix, according to this model, was the Helpful Content Update.

This update was a system-wide correction designed to close the loophole.

It heavily penalised sites that exhibited high traditional authority metrics (like a large
backlink profile) but had very low brand authority signals.

                                                          67

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The update was a clear signal that Google was doubling down on genuine, established
authority, making it immensely difficult for unknown sites to rank for competitive
topics, regardless of their content's superficial quality.

Mark reinforces this with a quote from former Google CEO Eric Schmidt I've used in
many SEO books now…: "Brands are the solution, not the problem. Brands are how
you sort out the cesspool."

Other Key Concepts from Mark’s presentation:

   ●​ Query Intent Classifiers: Google attaches labels to queries. The talk revealed
      classifiers like isDebunkingQuery (e.g., "is the earth flat"),
      medicalClassifier, and newsScore. The type of query dictates the type of
      results Google wants to show.
   ●​ The Eight Semantic Classes: The talk unveiled eight "Refined Query Semantic
      Classes" that seem to cover almost all queries, with the largest being "short
      fact or bool" (a question with a yes/no or simple factual answer). This is a
      critical insight, Mark predicts these are the queries most likely to be lost to AI
      Overviews - and I agree 100%.
   ●​ Content and Consensus: Google was found to generate a "consensus score"
      by counting the number of passages in content that agree with, contradict, or
      are neutral to the prevailing view. For a debunking query, only high-consensus
      content will rank. For a political query, Google may intentionally seek a mix of
      consensus and non-consensus results to provide balance. This means your
      content might be perfect, but if it doesn't fit the specific "recipe" of results
      Google wants for that query type, it won't rank.




                                                         68

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Conclusion: Brand is the New Bottom Line

The ultimate takeaway from the video is that in the modern SEO landscape, "site
quality" is largely synonymous with "brand authority."

The days of outranking authoritative sites with clever technical SEO alone are
dwindling.

The provided example of a rehabilitation-focused client outranking the NHS and other
established entities - despite having a fraction of the backlinks - illustrates this
perfectly.

Their success was attributed to a higher site quality score, earned through signals
that proved their authority and trustworthiness in a critical "Your Money Your Life"
(YMYL) category.

Ultimately, building a high-quality site in Google's eyes means building a real brand
that users seek out, trust, and mention.

This conceptual model suggests that long-term SEO success is now intrinsically linked
to genuine brand-building efforts that resonate with real people, not just algorithms.

For myself Mark’s work brings a long journey to a satisfying conclusion in this area.




                                                         69

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Practical Steps for Webmasters to Improve Site Quality
QUOTE: “One piece of advice I tend to give people is to aim for a niche within your
niche where you can be the best by a long stretch. Find something where people
explicitly seek YOU out, not just “cheap X” (where even if you rank, chances are
they’ll click around to other sites anyway).” John Mueller, Google 2018

Google’s quality scoring can seem intimidating, but its principles actually align with
common-sense best practices for building a great website.

Google has given extensive guidance - both in 2011 and in recent years - on how
webmasters can ensure their sites are seen as high-quality.

Here are practical, evidence-based steps to take:

   1.​ Audit Your Content and Remove or Improve Low-Quality Pages: Start by
      identifying pages on your site that are “thin”, redundant, or not useful to
      users. This includes very short articles with little info, copied content, duplicate
      or near-duplicate pages (e.g. same content targeting different keywords),
      auto-generated content, or pages that simply aggregate info from elsewhere
      without adding value. Google warns that “low-quality content on some parts of
      a website can impact the whole site’s rankings”developers.google.com. To
      avoid that site-wide Panda drag, either remove these pages, merge or expand
      them into more substantial resources, or “noindex” them if they must exist (so
      Google doesn’t count them against your site). Be careful: In the past, some site
      owners deleted hundreds of pages and saw no immediate improvement -
      Google’s Gary Illyes has noted that simply removing content isn’t a guaranteed
      fix seroundtable.com and seroundtable.com. If the content has any value, it
      may be better to “thicken” it - “Thin content: make it better, make it… thick,
      and add more high [quality] stuff,” Illyes advised twitter.com. In short, prune the
      truly junk content, but for borderline pages, beef them up rather than tossing
      them out. Over time, having a cleaner, richer content profile will lift your site’s
      quality score.​
      ​



                                                          70

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




2.​ Focus on E-A-T - Expertise, Authoritativeness, Trustworthiness: Panda is
   effectively a machine approximation of these traits. Ensure that qualified
   experts or enthusiasts write your content, and demonstrate their credentials.
   For example, include author bios that highlight expertise for YMYL (Your Money
   or Your Life) topics like health or finance. Make sure your content is factually
   accurate and cite trustworthy sources. Eliminate obvious errors - Google’s
   guidelines ask “Does this article have spelling, stylistic, or factual errors?”
   developers.google.com. Build authority by covering topics in-depth and
   becoming a go-to source in your niche. If your site is a recognized brand or
   cited by others, that boosts quality signals. As Google’s questions ask: “Is the
   site a recognized authority on its topic? Would you trust this site for a medical
   query? Would you be comfortable giving your credit card here?”
   developers.google.com and developers.google.com. Strive to earn “yes”
   answers to those questions. This might involve publishing original research,
   getting expert reviews, or simply demonstrating deep knowledge and
   transparency. In practice, improving E-A-T might mean adding author bylines
   and credentials, listing your physical business address and customer service
   info (for trust), getting mentions or links from other authoritative sites, and so
   on. These all contribute to how the algorithm perceives your site’s
   trustworthiness.​

3.​ Provide Substantial, Valuable Content (Avoid “Shallow” Text): Every page
   on your site should have a clear purpose and deliver value that stands out from
   the competition. Before publishing a page, ask yourself: “Does this page
   provide substantial value compared to other pages in search results?”
   developers.google.com and developers.google.com. If it’s basically a rehash of
   a Wikipedia article or a generic template with a few keywords swapped, it’s at
   risk. Panda loves **“original content, original research, original analysis”*
   developers.google.com. So put in the effort to make your content unique. This
   could mean including real-case studies, insightful commentary, data
   visualization, images or videos you created, etc. Longer content is not
   automatically better, but in many cases you should aim for comprehensiveness.
   A 300-word stub article on a broad topic is likely “thin”; a 2000-word
   well-structured article with facts, examples, and insights is more likely to be
   seen as high quality (assuming it’s well-written). Remember, one of Panda’s key
                                                      71

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   questions: “Does this article provide a complete or comprehensive description
   of the topic? Is this the sort of page you’d want to bookmark, share with a
   friend, or recommend?” developers.google.com and developers.google.com. If
   you can honestly answer yes, you’re on the right track.​

4.​ Improve User Experience - Usability, Ads, and Engagement: Panda doesn’t
   just look at what you publish, but also how you present it. A site overloaded
   with ads, especially above the fold, or with pop-ups that frustrate users, sends
   a negative quality signal (Google had a related “Top Heavy” algorithm for too
   many top-of-page ads, which aligns with Panda). Ask: “Does this site have an
   excessive amount of ads that distract from or interfere with the main content?”
   developers.google.com. Make sure the answer is no. Keep your page layout
   clean and reader-friendly - well below 30% ad density, in my opinion - based
   on Better Initial Ads Standards. Ensure fast page load times. Organize content
   with clear headings and avoid intrusive interstitials. Another subtle factor:
   navigation and site architecture. High-quality sites make it easy for users to
   find what they need; low-quality sites might be a maze of “content farm” links.
   Also, engagement metrics (though not explicitly confirmed as Panda signals)
   often correlate with quality. If users frequently bounce back to Google
   quickly from your page, that’s a sign something isn’t satisfying. While
   Google says Panda didn’t directly use “bounce rate”, it did use human
   judgments that correlate with it. So aim to keep users engaged: use images,
   break up text, make content scannable, and answer the query intent
   thoroughly. All these improvements not only please Panda but also your human
   visitors (which is the ultimate goal).​

5.​ Avoid “Gaming” the System - Don’t Chase Algorithm Loopholes: Panda
   (and its successors) are explicitly designed to catch sites that try to cheat their
   way to higher rankings without genuinely earning it. If you find a tactic that
   seems to boost rankings but doesn’t actually improve user experience, be wary
   - it might work short-term, but Panda will likely catch up. As Google’s Gary
   Illyes warned, if Google figures out that a site is successfully “gaming our
   systems” they will “push the site back just to make sure that it’s not working
   anymore.” sitecenter.com. In practice, this means don’t spam keywords, don’t
   auto-generate a thousand doorway pages, don’t copy content from elsewhere,
                                                      72

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   and don’t publish content solely for search engines instead of users. Google’s
   algorithms (and quality raters) are getting increasingly sophisticated at
   identifying these tactics. It’s far better to invest that energy in legitimately
   better content. In Google’s own words, “rather than focusing on specific
   algorithmic tweaks, we encourage you to focus on delivering the best possible
   experience for users” developers.google.com. If you do that, Panda shouldn’t
   be a problem.​

6.​ Monitor Your Site’s Quality Continuously: Quality is not a one-time fix. It’s an
   ongoing commitment. Regularly review your site: as it grows, ensure new pages
   meet your quality standards. Update or prune outdated content (keeping
   content fresh can be seen as a sign of care and quality). Watch metrics like
   what pages have high exit rates or very low time-on-page - these might be
   content to improve. Solicit user feedback - if users complain about certain
   pages, take that seriously. Google’s own core update advice is to “focus on
   content” and provides questions similar to Panda’s to self-assess your site.
   Using Google’s Search Quality Rater Guidelines (a document Google
   published that mirrors many Panda concepts) can be insightful - it describes in
   detail what Google considers a high “Page Quality” vs a low one (for example,
   YMYL pages with no author info and poor expertise are rated lowest). Aligning
   your site with those guidelines is effectively optimizing for Panda.​

7.​ Be Patient and Consistent: If your site was hit by quality algorithms (i.e., a
   broad drop in rankings coinciding with a known quality update or core update),
   making fixes will not yield an immediate rebound. Google’s quality scoring
   (especially when it was run periodically) can take weeks or months to recognize
   improvements. Even now, with continuous updates, it might take a major core
   update for your recovery to fully materialize. Google has said “as sites change,
   our algorithmic rankings will update to reflect that” developers.google.com -
   implying that recovery will come if you truly improve, but you must consistently
   maintain quality and wait for Google to re-crawl and re-evaluate enough of
   your site. Don’t be discouraged by lack of instant results; focus on making your
   site objectively better. Many webmasters have reported successful Panda
   recoveries after a few months by following the above steps diligently.​

                                                      73

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




   8.​ Use Google’s Tools and Feedback Channels: Make sure to leverage Google
      Search Console for any technical issues that could affect quality (crawl errors,
      mobile usability, Core Web Vitals, etc.). Sometimes what appears to be a Panda
      issue could be compounded by technical SEO problems (for instance, a
      misconfiguration causing duplicate pages). Fix those too - a well-run site is
      part of quality. Additionally, Google’s Webmaster Forum can be a place to get
      advice if you’re not sure why your site is perceived as low quality. While Google
      won’t tell you exactly what to do, you may get useful insights from product
      experts or see if a Google representative has given any specific guidance. In
      the Panda launch period, Google even invited affected site owners to provide
      feedback for the engineers developers.google.com. While they don’t do that
      publicly now, the Search Liaison on Twitter and Google’s blog posts often
      address common issues - keep an eye on those to understand Google’s
      expectations.​


By following these steps, you’re essentially aligning your site with what quality scoring
is designed to reward.

As Google’s Amit Singhal summed up: “Focus on delivering the best possible user
experience on your websites and not on what you think are Google’s current ranking
algorithms or signals.” developers.google.com

If you do that, the rest (rankings) will eventually follow. Panda taught the SEO world
that quality is king - a lesson that is even more true today.

References: Google’s official announcements and blog posts, patents, and internal
documents have all consistently pointed to the above principles.

The 2011 Google blog post “More guidance on building high-quality sites”
developers.google.com and developers.google.com is essentially a checklist that
foreshadows today’s best practices. Google’s patent on site quality scoring confirms
the technical underpinning of Panda’s site-wide classifier
patents.google.compatents.google.com.

                                                          74

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The DOJ trial exhibits from 2023 reaffirm the lasting importance of Panda’s site quality
score (QScore) in modern ranking justice.gov and justice.gov.

And quotes from Googlers like Amit Singhal and Matt Cutts give a transparent look at
Panda’s intent and mechanics, straight from the source wired.com and wired.com.

Following this evidence-based advice will not only help you avoid quality related
penalties but also improve your site’s overall SEO performance and user satisfaction in
the long run.

Google’s communications about Quality Score
Prominent spokespeople like John Mueller and Gary Illyes have explicitly stated on
multiple occasions that Google does not use an "overall domain authority" or "website
authority score" but they have all but also told us… they do.

On the surface this discrepancy between the internal reality of the Q* metric and the
public-facing narrative represents one of the most significant disconnects uncovered
by the trial.

On further examination of these statements, with full context, we get a more nuanced
answer from John, where he even explicitly mentions “Quality Score”.

When John Mueller was asked "could it be that when old pages were published, we
had a higher website authority or something that Google memorised and we don't
have anymore or for new pages, Google is applying more strict rules than for old
content?"

He answered:




                                                         75

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




“In general, when we have something that's kind of like a sitewide score, then the
current sitewide score applies to everything for that website. So from my point of
view, we don't have anything like a website authority score. But if we did have
something like that or if we have like when we're looking at, for example, like
quality signals that are more sitewide, then that's something that applies
across the whole website in the state that it's at now. So it's not the case that we
would say, oh, five years ago, you had this score for your website, therefore your
content will be rated like this forever. Rather we look at your website overall now
and we apply the current score to all of your pages on the website. So that's kind
of what we do when it comes to sitewide signals…. It's always based on what has
happened in the past. That's definitely something that kind of gets collected over
time, but that's the current score based on our understanding of the current website,
which of course, that understanding is based on things that have happened in the
past. So it's not so much that the score, uh, that we had maybe I don't know, last year
is applied to different parts of the website, but rather as we understand the website
now, the current score based on that is what we apply across the website now.

John continues: “In general, I recommend folding things together and putting things
on one strong website because that's something where you can kind of, um,
concentrate all of the information that we have about your website onto one
one domain or one website, which makes a lot easier for you to maintain and also
makes it easier for us to understand like overall, this is a really strong website.”

John reiterated: “There's always history there because we can't recrawl the whole
website now. So we can't recalculate the current quality score now.”

As you can see, the context around John’s answer is illuminating, even though he
confirms the don’t really have a “website authority score” he clarifies that some
signals “like quality signals that are more sitewide” and “quality score” - yes, he
even uses the term in the same conversation - is based on historic metrics and that
quality calculations like those mentioned in Google Panda, that are always a “rolling”
calculation.

Your “quality score”, we are told, is dynamic. It can go up, or down.

​

                                                         76

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By this point in time Googler spokespeople a lot higher up than John were on record
confirming this idea of a site-wide quality score:

"We made a series of changes ...that reduced the quality scores of certain types
of websites...We didn’t want users clicking on crummy sites." Eric Shmidt, Google
2014

Notice that while John Mueller of Google was criticised for denying a “website
authority score”, he did confirm, in the same discussion seconds later, the
nature of a website's “quality score”.

Folk overlook lots of context when criticising Googlers and their statements around
these issues. One can often easily overlook the context of the audience they are
talking to too.

​




                                                         77

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Role of Machine Learning - RankBrain, DeepRank, and the AI
Layer
While the trial emphasized the foundational role of human-engineered systems, it also
provided critical context for how Google deploys its formidable machine learning (ML)
capabilities.

The evidence suggests that Google's use of AI is not as an all-encompassing,
autonomous "ranking brain," but rather as a set of highly specialized tools applied to
solve specific, complex problems that are intractable for hand-crafted rules.

This approach reflects a pragmatic, risk-averse engineering culture focused on
integrating powerful ML models within a controllable and understandable framework.

RankBrain: The Query Interpreter
Testimony from Google engineers Eric Lehman and Pandu Nayak clarified the primary
and specific function of RankBrain, one of Google's earliest and most famous ML
systems.

Its core purpose is not to rank documents directly, but to interpret search
queries, particularly those that are novel, ambiguous, or part of the long tail of
search - the vast number of unique queries that Google has never seen before.

When simple keyword matching is insufficient to determine a user's intent, RankBrain
helps the search engine understand the underlying meaning and concepts within the
query, allowing it to retrieve a more relevant set of results.

A crucial detail for the antitrust proceedings emerged from Eric Lehman's testimony:
RankBrain is trained on historical search data, not on live, real-time user data.

While the sheer scale of this historical data is itself a product of Google's market
dominance, this distinction subtly mitigates the argument that the system relies on a
moment-to-moment data advantage that competitors lack.

Although its relative importance within the full suite of ranking signals may have
evolved since it was first introduced and famously described as the "third most
important signal," RankBrain's role as a sophisticated query interpretation model
                                                         78

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




remains a cornerstone of Google's ML layer.

DeepRank and the Pursuit of Transparency
The trial also shed light on other deep learning models like DeepRank, which are used
in the ranking process.

More importantly, the evidence revealed a concerted engineering effort at Google to
maintain transparency and control even over these complex systems.

One trial exhibit noted that BERT-based DeepRank signals could be "decomposed
into signals that resembled the traditional signals".

Another document explained that a system called "eDeepRank" attempts to "take
LLM-based signals and break them down into components to make them more
transparent".

This effort to deconstruct the outputs of ML models is a profound indicator of
Google's engineering philosophy.

It shows that Google engineers are not simply deploying black-box models and
trusting their outputs.

Instead, they are actively building parallel systems to understand why an ML model
made a particular ranking decision.

This aligns perfectly with the rationale for hand-crafting foundational signals like T*
and Q*: the institutional imperative to maintain human understanding, control, and the
ability to debug the system's behavior.




                                                         79

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Human-ML Symbiosis
The complete picture that emerges from the trial is not one of machine learning
replacing human engineers, but of a deeply symbiotic relationship. The architecture is
layered to leverage the strengths of each approach:
 1.​ Human-Engineered Foundation: Systems like T* (Topicality) and Q* (Quality)
     provides a stable, predictable, and understandable foundation for relevance and
     trust.
 2.​ Data-Driven Refinement: The Navboost system acts as a powerful refinement
     layer, using the scaled intelligence of past user behavior to improve upon the
     foundational scores.
 3.​ Specialized ML Tools: ML models like RankBrain and DeepRank are then applied
     to solve specific, high-complexity problems like query understanding and
     nuanced ranking adjustments that are difficult to address with hand-crafted
     rules.21

This symbiosis is also evident in the challenges Google faces. One engineer's
testimony acknowledged that the rise of AI-generated content is making the
problem of search quality worse, not better.

This highlights the ongoing "cat and mouse game" where human oversight and
continuous engineering are essential to develop new signals and systems to combat
the abuse of new technologies that aim to manipulate search rankings.

This reality runs counter to the simplistic notion that Google can simply deploy more
AI to solve the problem of AI-generated spam.

Google's approach to machine learning is thus revealed to be highly pragmatic.

Rather than ceding control to a single, end-to-end ML model, it integrates specialized
models as components within a broader, more traditional software engineering
architecture.

The strategic decision to invest in making these models' outputs more transparent
and understandable demonstrates a mature, risk-averse culture.

This suggests that Google's durable competitive advantage lies not just in possessing
                                                         80

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




superior ML models, but in having the engineering discipline and robust infrastructure
to deploy them safely and effectively in a high-stakes, global production environment
without sacrificing control or accountability.#


Overview of Google's RankBrain and BERT-based RankEmbed
Bill Slawski’s quote from 2019 succinctly debunked misconceptions: “Semantic search
at Google is not powered by LSI… You cannot optimize pages for…RankBrain or BERT.”
hobo-web.co.uk - using it to bust SEO myths and illustrate that Google’s NLP is far
beyond old techniques.

RankBrain

Launch: Officially confirmed by Google in 2015.

What RankBrain Does

RankBrain is Google's first significant implementation of machine learning to
understand search queries. It specifically addresses queries Google hasn't seen
before, helping interpret user intent and meaning.

     Google (Greg Corrado, Senior Research Scientist):​
     "`RankBrain' uses artificial intelligence to filter results. RankBrain has
     become the third-most important signal contributing to the result of a
     search query." (Bloomberg, Oct 2015)

Technical Mechanism (Neural Embeddings)

RankBrain converts words into mathematical vectors (embeddings), allowing Google
to understand semantic relationships between terms.

     Google Patent US9245078B1 ("Word embedding and phrase
     embedding generation"):​
      "Words are represented as embeddings in continuous vector spaces.
     These embeddings encode semantic information by capturing
     relationships between words based on context. Query terms and phrases
     are represented as embeddings, enabling semantic matching."
                                                         81

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




RankBrain evaluates ambiguous or complex queries by identifying similar past
searches and user behaviors to interpret intent. The system learns continuously from
user interaction (clicks, dwell time).

Integration with Ranking Signals

RankBrain doesn't replace PageRank or topicality signals but integrates as a
complementary signal.

     Google (Gary Illyes, Webmaster Trends Analyst):​
     "RankBrain will understand better what results work for queries. It’ll
     understand that certain stop words should not be dropped. Sometimes the
     word “with” is dropped from a query, but RankBrain will understand that we
     need to keep it." (Q&A, 2016)

Current Usage

As of 2025, RankBrain remains actively used by Google for query interpretation and
semantic relevance.




                                                         82

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




BERT-based RankEmbed

Launch: BERT officially integrated into Google Search in October 2019.

What BERT (RankEmbed) Does

BERT (Bidirectional Encoder Representations from Transformers) significantly
advances Google's understanding of language context. RankEmbed refers specifically
to Google's implementation of BERT in ranking algorithms.

     Google Blog (Pandu Nayak, VP Search):​
     "BERT models can therefore consider the full context of a word by looking
     at the words that come before and after it - particularly useful for
     understanding the intent behind search queries"​
     (Google Official Blog, 2019)

Technical Mechanism (Transformer-based Neural Network)

BERT is a deep-learning neural network architecture designed around the
Transformer model. Unlike previous models, BERT is bidirectional, meaning it
considers the full context of a word by looking at words before and after it.

     Google Paper ("BERT: Pre-training of Deep Bidirectional Transformers
     for Language Understanding"):​
      "BERT uses a multi-layer bidirectional Transformer encoder. The model is
     pre-trained on large text corpora, using masked language modeling (MLM)
     and next sentence prediction (NSP) tasks. It effectively captures
     context-dependent meanings of words."​
      (Devlin et al., Google AI, 2018)

Google further describes BERT as being especially useful for understanding complex
queries that depend on subtle nuances.

     Google Blog (Pandu Nayak, VP Search):​
     "BERT will help Search better understand one in 10 searches in the U.S. in
     English”​
     (Google Official Blog, 2019)
                                                         83

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




RankEmbed and Semantic Matching

RankEmbed specifically refers to embedding-based methods using BERT-style neural
embeddings to match user queries with relevant documents.

This patent explicitly describes the use of neural embeddings, like those from BERT, in
ranking systems:

   ●​ Queries and documents are converted into embeddings.
   ●​ Semantic similarity scores (cosine similarity, dot product) determine ranking
      positions.

Integration with Ranking Signals

BERT-based RankEmbed works alongside traditional signals (PageRank, topicality,
freshness) to refine Google's understanding of user queries and document content.

     Google (Danny Sullivan):​
     "A key signal, RankEmbed, is a “dual encoder model” that embeds queries and
     documents into an “embedding space.” This space considers semantic
     properties and other signals. Retrieval and ranking are based on a “dot
     product” or “distance measure in the embedding space. 2019




                                                         84

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Current Usage
As of 2025, BERT-based RankEmbed remains integral to Google's ranking algorithms. It is
particularly important for handling long-tail queries, questions, and conversational searches.




 Factor                                  RankBrain                                 BERT-based RankEmbed


 Launch Date                             2015                                      2019


 Core Mechanism                          Neural embeddings, ML                     Bidirectional Transformer
                                                                                   embeddings


 Function                                Query intent & semantic                   Contextual
                                         interpretation                            query-document matching


 Integration                             Complements traditional                   Complements traditional
                                         signals                                   signals


 Main Use                                Ambiguous or rare queries                 Complex, conversational,
                                                                                   long-tail queries


 Patents / Papers                        US20200364850A1 (neural                   Devlin et al. (BERT)
                                         ranking)


 Current Status                          Active                                    Active




                                                          85

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Key Takeaway

RankBrain and BERT-based RankEmbed have revolutionized Google's ability to
understand and process user queries.

While RankBrain introduced machine learning into query processing for ambiguous
searches, BERT-based RankEmbed provided a significant leap forward in contextual
semantic matching, especially for conversational searches.

Both systems function integratively, enhancing Google's traditional signals (topicality,
authority, PageRank) to provide more accurate, contextually relevant search results.




                                                         86

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Reconciling the New Model of Google Search
The collective evidence presented in the U.S. v. Google trial necessitates a
fundamental revision of the public understanding of Google's search engine.

The testimonies and internal documents provide the constituent parts of a new, more
granular architectural model.

This model reveals a logical pipeline that leverages human engineering, massive user
data, and specialized machine learning in distinct stages.

Scrutinising these components not only clarifies how Google Search works but also
exposes the deep chasm between this internal reality and the carefully curated public
narrative the company has maintained for over a decade.




                                                         87

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Unified Ranking Pipeline
Based on the trial evidence, the process of generating a search result can be modeled
as a multi-stage pipeline:
 1.​ Query Interpretation: When a user enters a search, the query is first processed
     to discern its underlying intent. For common queries, this may be straightforward.
     For novel, ambiguous, or long-tail queries, specialized machine learning systems
     like RankBrain are employed to interpret the user's need beyond simple keyword
     matching.
 2.​ Initial Retrieval & Foundational Scoring: The system retrieves a broad set of
     potentially relevant documents from its massive index, which contained an
     estimated 400 billion documents as of 2020. Each of these documents is then
     given a set of base scores by the foundational, hand-crafted systems. The​
     T* (Topicality) system provides a score for query-dependent relevance based on
     the ABC signals (Anchors, Body, Clicks). Concurrently, the​
     Q* (Quality) system provides a largely static, query-independent score for the
     overall trustworthiness and authority of the document's source domain, with
     PageRank as a key input.
 3.​ User-Behavior Refinement: This is arguably the most critical and competitively
     significant stage. The Navboost system takes the list of documents scored by T*
     and Q* and subjects it to a powerful re-ranking and filtering process. Drawing on
     13 months of aggregated user click data, Navboost dramatically promotes results
     that have historically satisfied users for similar queries or in similar contexts (e.g.,
     same location, same device type) and demotes those with poor engagement
     signals. This step reduces the candidate set from tens of thousands to a few
     hundred elite contenders.
 4.​ Final Ranking & SERP Construction: The final, refined list of documents may
     undergo further scoring adjustments by more computationally expensive deep
     learning systems like DeepRank. Simultaneously, the Glue system analyzes
     real-time interaction signals to determine the optimal layout of the Search Engine
     Results Page, deciding which SERP features (maps, images, knowledge panels) to
     display and where to place them. The final, assembled page is then delivered to
     the user.



                                                          88

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




The Data Feedback Loop as a Competitive Moat
This detailed architectural model provides a clear mechanical explanation for the
"data feedback loop" that was at the heart of the DOJ's antitrust case.
The trial evidence validates the theory that Google's dominance is a self-reinforcing
cycle.
The process is as follows:
 1.​ Google's exclusive default placement deals with companies like Apple and
       Samsung guarantee it a dominant market share (over 90%) and an unparalleled
       volume of search queries.4
 2.​   This massive query volume generates an equally massive and proprietary stream
       of user interaction data (clicks, dwell time, etc.).
 3.​   This data is the exclusive fuel for the Navboost system.
 4.​   Navboost uses this data to significantly improve the quality and relevance of
       Google's search results in a way no competitor, starved of similar data, can
       replicate.
 5.​   This superior search quality is then used by Google as the primary business
       justification for its partners to continue signing exclusive default deals.

This cycle, it is claimed by those pursuing Google, creates an insurmountable barrier
to entry.

The claim is that a potential competitor cannot develop a search engine of
comparable quality without access to large-scale user data, but it cannot acquire that
user data without first achieving a scale that Google's exclusionary contracts are
designed to prevent.




                                                            89

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Chasm Between Public Narrative and Internal Reality
The most profound implication of the trial is the claims of the exposure of a significant
and deliberate gap between Google's public communications about its search engine
and the operational reality revealed under oath.

The discrepancies are not minor clarifications but sometimes contradictions on core
aspects of the ranking system.

This has somewhat damaged the credibility of Google's public-facing representatives
and will reshape the relationship between the company and the technical community
that analyzes its products.

It is worth pointing out from my experience, as a professional SEO investigating
Google’s statements for almost 20 years, Google spokespeople almost all but told us
how the search giant worked — this is undeniable.

Almost every aspect of search was shared by Google over the years.

It's clear why they aimed to focus webmasters' attention all these years on user
experience (like Core Web Vitals)and not Clicks or Links.




                                                          90

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




 Topic                                    Google's Historical Public                Revelation from U.S. v.
                                          Stance                                    Google Testimony/Exhibits


 Use of Click Data for                    Publicly evasive or dismissive.           Clicks are a core, foundational
 Ranking                                  Often described clicks as a               signal. Signal 'C' (user dwell
                                          "noisy" signal used for                   time) is a component of the T*
                                          evaluation and                            (Topicality) score. The
                                          experimentation, not direct               powerful Navboost system is a
                                          ranking.                                  major ranking refiner based
                                                                                    entirely on 13 months of
                                                                                    aggregated user click data.


 Site-Level Authority Score               Explicitly and repeatedly                 Google uses Q*, an internal,
                                          avoided. Statements include:              largely static, domain-level
                                          "we don't really have 'overall            score to measure
                                          domain authority'" and "we                trustworthiness and quality.
                                          don't have anything like a                PageRank, a measure of link
                                          website authority score."                 authority, is a key input to this
                                          would be semantically correct             score. This functions as a site
                                          but technically, not.                     authority metric.


 Nature of Ranking                        Often portrayed as                        The foundational systems (T*,
 Algorithm                                increasingly driven by                    Q*) are "hand-crafted" by
                                          inscrutable, end-to-end                   engineers for control and
                                          Machine Learning and AI, with             debuggability. ML is used for
                                          systems like RankBrain taking             specific, targeted tasks like
                                          center stage.                             query interpretation, not as a
                                                                                    monolithic ranking brain.




This evidence demonstrates that the simplified, and at times misleading, model of
search presented to the public, SEOs, and webmasters was a strategic choice.

The long-standing paradigm of "trust but verify" in relation to Google's public
guidance has been negatively impacted.
                                                           91

         © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




For the sophisticated technical community, the trial record - comprising sworn
testimony and internal documents - now stands as the canonical source of truth.

Future public statements from the company will inevitably be viewed through a lens of
some scepticism and cross-referenced against the legally compelled facts.

The dynamic has shifted permanently from one of interpreting guidance to one of
reverse-engineering a blueprint.

The "black box" of Google Search, while not fully transparent, is now more illuminated
than ever before, and the insights gained will redefine the field of search analysis for
years to come.

In my opinion, when a SEO or webmaster (or anyone for that matter for the former 2
are always listening) it is akin to a bank robber asking a bank teller if the bank has
a safe where it is, what its called and what is the combination. The bank teller,
to be transparent and helpful, will answer those questions with varying degrees
of opacity. Something, I think, was always to be expected from any private
company.

Again in my opinion if Google didn’t control their SERPS, then SEOs, Blackhats
spammers and mainly hackers would dominate every SERP. Small publishers would
have an equally hard time, if not worse, is what I am saying.

The current situation though, and again this is just my opinion, is that this path kind of
ensures the “house always wins” when small publishers do not.




                                                          92

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Unlocked Warehouse
While the DOJ trial revealed the existence of Navboost, the Content Warehouse leak
gave us an unprecedented look at its mechanics, including metrics like goodClicks and
lastLongestClicks.

What Google's Accidental Leak Tells Us About Search, Secrecy, and
Strategy
In the spring of 2024, the digital world was simmering. A tension had been building for
months between Google and the global community of search engine optimisation
(SEO) professionals, marketers, and independent publishers who depend on its traffic
for their livelihoods - especially after the impact of September 2023 HCU Update.

It was in this climate of uncertainty that a simple, automated mistake became the
spark that ignited a firestorm of revelation.

This was not a dramatic, cloak-and-dagger operation.

There was no shadowy whistleblower or sophisticated cyberattack. Instead, on March
13, 2024, an automated software bot named yoshi-code-bot made a routine update to
a public GitHub repository.

In doing so, it inadvertently published thousands of pages of Google's highly
sensitive, internal API documentation.

For weeks, these documents sat in plain sight, largely unnoticed. On May 5, Erfan
Azimi discovered the repository and shared it: Rand Fishkin, founder SparkToro, and
Michael King, of iPullRank.

After weeks of verification, they unleashed their findings on May 27, and the digital
marketing world was irrevocably changed.

What was exposed was not the algorithm's source code - the complex, proprietary
recipe for ranking web pages.

Rather, it was something arguably more valuable for strategic analysis: the internal
documentation for Google's "Content Warehouse API".

This was the blueprint of the system, a detailed inventory of the ingredients Google
uses.
                                                          93

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




It outlined over 14,000 attributes across nearly 2,600 modules, revealing the specific
types of data Google collects, the metrics it measures, and the systems it employs to
make sense of the entire internet.

While it didn't reveal the precise weighting of each factor, it provided an
unprecedented look at the menu of options available to Google's engineers.

The leak's true significance lies in the potential chasm it exposed between what
Google has publicly told the world for over a decade and what its own internal
documentation revealed.

For years, SEO professionals had operated on a combination of official guidance,
experimentation, and hard-won intuition.

Many of their core beliefs - that a website's overall authority matters, that user clicks
influence rankings, that new sites face a probationary period - were consistently and
publicly dismissed by Google's representatives.

The leak served as a stunning vindication for this community, confirming that their
instincts, honed through years of observation, were largely correct.

For Google, it triggered a crisis of credibility.

The ultimate value of this accidental revelation is not a simple checklist of technical
tricks to climb the search rankings. It is the profound strategic realignment it
demands.

The unlocked warehouse confirms that sustainable success in Google's ecosystem is
less about manipulating an opaque algorithm and more about building a genuine,
authoritative brand that users actively seek, trust, and engage with.

It proves that the focus must shift from attempting to please a secretive machine to
demonstrably satisfying a now-quantifiable human user.

This chapter will dissect the anatomy of the leak, explore its most significant
contradictions, and lay out the new strategic playbook for any business that wishes to
thrive in a post-leak world.




                                                          94

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Anatomy of a Leak
This library confirmed that the "Google algorithm" is not a monolithic entity but a
complex, multi-layered ecosystem of specialized systems working in concert.

Subsection 1.1: What Was Actually Leaked?
The story of the leak begins with a timeline.

On March 13, 2024 (some reports cite March 29), an automated bot, yoshi-code-bot,
appears to have accidentally published a copy of Google's internal Content
Warehouse API documentation to a public GitHub repository. This repository remained
public until it was removed on May 7, 2024. During this window, the information was
indexed and circulated, eventually finding its way to SEO professional Erfan Azimi, who
then shared it with industry veterans Rand Fishkin and Michael King. It was their
coordinated analysis and publication on May 27 that brought the leak to global
attention.

The source of the leak is crucial; it came directly from Google's own infrastructure.

The documentation was for the internal version of what appears to be its
Content Warehouse API, a system for storing and managing the vast amounts of
data Google collects from the web.

The files contained links to private Google repositories and internal corporate pages,
and multiple former Google employees who reviewed the documents confirmed their
legitimacy, stating they had "all the hallmarks of an internal Google API".

The sheer technical density of the material - filled with definitions for protocol buffers
(protobufs) and thousands of module attributes - further cemented its authenticity.

It was not a curated "false flag" designed to mislead, but a messy, genuine, and
accidental glimpse into Google's engineering world.

The scale of the leak was immense.

The documentation spanned over 2,500 pages, detailing 14,014 distinct attributes, or
"features," organized into 2,596 modules.

These attributes represent the specific types of data that Google's systems are
designed to collect and consider, covering everything from search and YouTube to

                                                          95

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




local services and news.

Google's official response was swift but cautious.

In a statement to media outlets, a Google spokesperson confirmed the
documents were authentic but urged the public to avoid making "inaccurate
assumptions about Search based on out-of-context, outdated, or incomplete
information".

This was widely interpreted by the SEO community as a standard non-denial denial, an
attempt to downplay the significance of the revelations without explicitly refuting
them.

A Glimpse Inside the Machine: Core Systems and "Twiddlers"
Perhaps the most fundamental insight from the leak is that the popular conception of
a single, monolithic "Google Algorithm" is a fiction.

The documentation confirms a far more complex reality: a layered ecosystem of
interconnected microservices, each with a specialized function, working together in a
processing pipeline.

This structure means there isn't one thing to "optimise for"; rather, a successful
strategy must address signals relevant to each stage of the process.

The journey of a web page through Google's systems can be understood through
several core components named in the leak:

  ●​ Crawling: The process begins with systems like Trawler, which are responsible
     for discovering and fetching content from across the web.
  ●​ Indexing: Once content is fetched, it is processed and stored by a suite of
     indexing systems. Alexandria and TeraGoogle appear to be the primary and
     long-term storage systems, respectively. Critically, a system named SegIndexer
     is responsible for placing documents into different tiers within the index. This
     confirms the long-held theory that Google maintains different levels of its index,
     with links from documents in higher-quality tiers carrying more weight.
  ●​ Ranking: The initial scoring and ranking of documents is handled by a primary
     system called Mustang. This system performs the first pass, creating a
     provisional set of results based on a multitude of signals.

However, the process does not end with Mustang. The leak sheds significant light on a

                                                         96

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




subsequent and powerful layer of the system known as "Twiddlers."

This concept is critical for any business leader to understand, as it represents
Google's final editorial control over its search results.

Twiddlers are re-ranking functions that adjust the order of search results after
the main Mustang system has completed its initial ranking.

They act as a fine-tuning mechanism, applying boosts or demotions based on
specific, often real-time, criteria. Unlike the primary ranking system, which evaluates
documents in isolation, Twiddlers operate on the entire ranked sequence of
results, making strategic adjustments before the final list is presented to the
user.

The leaked documents reference several types of these re-ranking functions,
illustrating their power and versatility.

Examples include FreshnessTwiddler, which boosts newer content; QualityBoost,
which enhances quality signals; and RealTimeBoost, which likely adjusts rankings
based on current events or trends.

The most frequently mentioned and strategically significant of these systems is
NavBoost, a powerful Twiddler that re-ranks results based on user click behavior,
which will be explored in detail in the next section.

The existence of this multi-stage architecture - crawling, tiered indexing, initial
ranking, and multiple layers of re-ranking - proves that Google's process is far more
nuanced and dynamic than a simple mathematical formula.




                                                          97

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Four Great Vindications - Where SEO Theory Met
Google's Reality
The core impact of the Google Warehouse leak was not in revealing entirely new
concepts, but in confirming, with documentary evidence, what many experienced SEO
professionals had long suspected to be true.

For years, a significant gap existed between the public guidance offered by Google's
representatives and the real-world results observed by practitioners.

The leak bridged this gap, vindicating long-held theories and exposing a pattern of
strategic obfuscation from the search giant.

This section deconstructs the four most significant areas where Google's public
narrative crumbled in the face of external interpretation of its own internal
documentation.

The Ghost in the Machine: siteAuthority is Real
For years, the concept of "domain authority" - the idea that Google assigns an overall
quality or trust score to an entire website - was a central point of contention.

Google's public-facing representatives, most notably John Mueller, repeatedly and
explicitly denied its existence.

They framed it as a metric invented by third-party tool providers like Moz and Ahrefs,
stating unequivocally that Google does not use "Domain Authority at all" for ranking
purposes.

While occasionally hinting at "site-wide level metrics," the official line was a firm denial
of a holistic authority score akin to Domain Authority - or at least - called that…
(semantics).

The leaked documentation obliterated this narrative.

Contained within a module for "Compressed Quality Signals" - a set of fundamental
quality scores stored for every document Google crawls - is an attribute explicitly
named siteAuthority.

This discovery was a watershed moment for the SEO community, providing concrete
proof that Google does, in fact, calculate a site-wide authority metric that influences
                                                          98

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




the ranking potential of every page on a domain.

This confirmation fundamentally changes the strategic calculus. It means that
optimizing individual pages in isolation is an incomplete strategy.

The overall reputation, trustworthiness, and authority of the entire domain are critical,
acting as a foundational element upon which individual page performance is built.

The leak suggests this siteAuthority score is likely a composite metric, influenced by a
variety of signals.

These almost certainly include the PageRank of the site's homepage (which the
documents state is considered for every single document on the site), brand-related
signals, and, crucially, the user interaction data collected by systems like NavBoost.

A site is not just a collection of pages; it is an entity with a reputation, and Google is
measuring it.

The All-Seeing Eye: Clicks, Chrome, and NavBoost
No topic has been more contentiously debated than the role of user behavior in
Google's rankings.

The official narrative from Google has been consistent: user engagement signals like
click-through rate (CTR) are too "noisy" and easily manipulated to be used as a direct
ranking factor.

Furthermore, Google representatives have explicitly denied using data from the
company's own Chrome browser to inform search rankings.

The leak reveals this to be, at best, a semantic misdirection.

The documentation is saturated with references to a system named NavBoost,
described as a powerful "re-ranking system based on click logs of user behavior".

This system, also referred to as "Glue," was first mentioned in testimony during the
U.S. Department of Justice antitrust trial but was detailed extensively in the leak.
NavBoost analyzes a sophisticated array of click metrics to gauge user satisfaction,
including:




                                                          99

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




  ●​ goodClicks and badClicks: To differentiate between successful and unsuccessful
     user interactions.
  ●​ lastLongestClicks: A critical metric that identifies the search result a user dwells
     on before ending their search session, serving as a powerful proxy for
     satisfaction.
  ●​ unsquashedClicks: A normalized click metric, likely designed to "squash" or
     discount inorganic patterns and prevent manipulation from click-bots.

The fuel for this massive data-processing engine is the Google Chrome browser.

The leak exposed a module named ChromeInTotal and attributes like
uniqueChromeViews and chrome_trans_clicks, providing undeniable evidence that
Google leverages clickstream data from its billions of browser users to power its
ranking systems.

This revelation represents a true paradigm shift.

User behavior is not a passive outcome of good rankings; it is a primary, active input
into the ranking algorithm.

Every click is a vote. When a user clicks a result and stays on that page (a "long click"),
they are sending a positive signal to Google that their query was satisfied.

When they click a result and immediately return to the search page to choose another
option ("pogo-sticking"), they are sending a powerful negative signal. This was a
signal brought to my attention by A.J. Kohn in 2008.

This elevates the strategic importance of on-site user experience (UX), headline and
meta description optimization, and brand recognition to the level of core ranking
imperatives.




                                                         100

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Walled Garden: The "Sandbox" and hostAge
For as long as SEO has existed, practitioners have observed a phenomenon known as
the "Google Sandbox," a supposed probationary period during which new websites
struggle to gain visibility, regardless of their content quality.

Officially, Google has always denied the existence of such a mechanism.

The leak provided (to some) clear validation for this long-standing theory. Within the
PerDocData module, which contains information about individual documents, is an
attribute named hostAge.

The documentation's description is unambiguous: this attribute is used to "sandbox
fresh spam in serving time".

This confirms that a website's age is a factor Google considers and that new domains
are treated with algorithmic suspicion until they have had time to establish a track
record of credibility and trustworthiness.

This has profound strategic implications, particularly for startups, new product
launches, and any business entering the digital space for the first time.

It means that expecting immediate SEO success is unrealistic.

The "sandbox" effect necessitates a long-term approach.

