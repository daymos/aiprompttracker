# Building trust and authority through high-quality content, positive user signals,

Building trust and authority through high-quality content, positive user signals,
and legitimate backlinks from day one is not merely good practice; it is an
essential requirement to eventually escape this initial period of algorithmic
probation and compete on a level playing field.




                                                         101

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Unspoken Hierarchy - Whitelists and Special Treatment
Google's public image is that of an objective, impartial organizer of information, with
its algorithm serving as a neutral arbiter of relevance and quality.

The leak, however, reveals that for the most sensitive and critical topics, the playing
field is anything but neutral.

The documents detail the existence of potential whitelists, which are essentially
lists of domains that are manually elevated for certain types of queries.

Attributes such as isElectionAuthority and isCovidLocalAuthority demonstrate that for
high-stakes "Your Money or Your Life" (YMYL) topics like elections and public health,
Google actively intervenes to promote sources it deems authoritative and suppress
potential misinformation.

This is a logical and arguably necessary measure for public safety, but it
confirms that for these queries, authority is not algorithmically earned but editorially
assigned by Google.

Furthermore, the system appears to apply special classifications to different types of
websites.

The documentation includes flags for smallPersonalSite and identifies sites based on
their business model, such as blogs, e-commerce stores, or travel sites.

This suggests that Google may apply different ranking adjustments or even limit the
number of sites of a certain type that can appear in a single search result, enforcing a
kind of algorithmic diversity.

The existence of these systems - siteAuthority, NavBoost, hostAge, and whitelists -
are not independent phenomena.

They are deeply interconnected, forming a self-reinforcing system that creates a
virtuous cycle for established, trusted brands.

A new site begins its life in the hostAge sandbox. To escape, it must build trust, which
is measured by its overall siteAuthority.

A significant component of this authority score is derived from positive user
engagement signals captured by NavBoost - real users, tracked via Chrome, clicking
on and trusting the site.
                                                          102

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The path to authority is paved with positive user interactions.
This creates a powerful feedback loop: a strong brand drives user searches and clicks,
which boosts NavBoost scores, which in turn increases siteAuthority, which helps the
site rank for even more terms, further reinforcing the brand's visibility and dominance.
This system inherently favors those who have already won the battle for user trust.

This also provides context for Google's history of public denials.

It is unlikely that these statements were born of simple malice, and often, the context
of who Google is talking to is often missed.

Rather, they can be interpreted as a form of strategic obfuscation intended to
protect the integrity of the search results.

If Google were to publicly announce, "We use clicks to rank websites," it would
instantly spawn a massive, illicit market for click-bots, rendering the signal useless.

By denying it, they discouraged this manipulative behavior.

Similarly, by stating, "We don't use Domain Authority," they were being semantically
precise - they do not use the specific metric from the company Moz - while
conveniently obscuring the existence of their own possible internal equivalent,
siteAuthority.

For business strategists, the lesson is clear: Google's public communications are a
component of its algorithmic defense system and must be treated with a healthy
degree of interpretation - and always remember who Google is usually talking to
about this kind of stuff - remember about the bank teller analogy from earlier in this
book.




                                                          103

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Area of Contention          Google's Public                 Key Leaked Attributes The New Reality
                            Narrative (Pre-Leak)            & Systems             (Post-Leak Insight)

Site-Wide Authority         "We don't have a 'domain siteAuthority,                        Google calculates a
                            authority' score. It's not authorityPromotion,                 holistic, site-wide
                            something we use at all." Homepage PageRank                    authority metric that
                                                                                           influences rankings. A
                                                                                           site's overall reputation
                                                                                           matters immensely.

User Click Signals          "Clicks are a 'noisy'     NavBoost, goodClicks,                User click behavior is a
                            signal; we don't use them badClicks,                           primary input for a
                            directly for ranking."    lastLongestClicks                    powerful re-ranking
                                                                                           system (NavBoost). User
                                                                                           satisfaction is a direct
                                                                                           ranking factor.

Chrome Browser Data         "I don't think we use           ChromeInTotal,                 Data from billions of
                            anything from Google            uniqueChromeViews,             Chrome users is
                            Chrome for ranking."            chrome_trans_clicks            collected and used to
                                                                                           power ranking systems
                                                                                           like NavBoost and
                                                                                           evaluate site popularity.

New Site "Sandbox"          "There is no 'sandbox' for hostAge, PerDocData                 A sandboxing mechanism
                            new websites."             module                              exists to temporarily limit
                                                                                           the visibility of new
                                                                                           domains until they
                                                                                           establish
                                                                                           trustworthiness.



​




                                                         104

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Strategic Imperatives in a Post-Leak World
The revelations from the Google Warehouse leak are not merely academic.

They demand a fundamental re-evaluation of digital strategy. The era of focusing on
narrow, technical SEO tactics, or any SEO tactics, in a vacuum is over.

The new playbook requires a holistic, integrated approach where brand marketing,
user experience design, and content strategy are no longer separate disciplines but
core components of a successful search presence.

The leak provides the blueprint for four key strategic imperatives.

Strategy 1: Brand is the Ultimate Ranking Factor
The single most important strategic takeaway from the leak is that a strong brand is
the most durable competitive advantage in search.

The documentation connects the dots between the abstract concept of brand equity
and the concrete metrics Google uses to rank websites.

The existence of siteAuthority confirms that Google measures a domain's overall
reputation.

The dominance of NavBoost proves that Google rewards what users click on and
trust.

The ultimate expression of this trust is when a user bypasses generic queries
and searches directly for a brand name.

This action is the strongest possible signal of authority and relevance. The leak
confirms what many have long argued: search is, and always has been, a branding
channel.




                                                        105

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




This understanding necessitates a shift in marketing priorities and resource allocation.

  ●​ Actionable Recommendations:
       ○​ Organizations must rebalance their marketing investments, moving beyond
          a narrow focus on technical SEO and link acquisition to embrace broader
          brand-building initiatives. This includes public relations, advertising that
          drives brand recall, community engagement, and establishing genuine
          thought leadership in their industry.
       ○​ Success measurement must evolve. While non-branded keyword rankings
          remain important, a key performance indicator (KPI) should be the growth
          of branded search volume. An increase in users searching directly for a
          company's name is a direct input into the NavBoost system and a clear
          indicator of growing siteAuthority.
       ○​ The strategy should also account for unlinked "mentions." The leak
          contains numerous features that reference entity mentions - the
          appearance of a brand name across the web, even without a hyperlink.
          This suggests that Google tracks these mentions as a signal of prominence
          and authority, similar to how it treats links. I go into Mentions in my article
          on Hobo Web - Mentions: The New Economy in the Age of AI Overviews?

Strategy 2: User Experience is Undeniably the New Core SEO
The leak effectively dissolves the wall between user experience (UX) design and
search engine optimization.

The NavBoost system, with its detailed click metrics, makes the user the ultimate
arbiter of a page's quality.

A poor user experience, which causes a user to quickly click the "back" button and
return to the search results - a behavior known as "pogo-sticking" - is a direct,
negative ranking signal registered as a badClick.

Conversely, an experience that is so helpful and satisfying that the user ends their
search journey on that page generates a lastLongestClick, one of the most powerful
positive signals.




                                                        106

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Therefore, optimizing for the user is optimizing for Google's most important systems.

  ●​ Actionable Recommendations:
       ○​ Invest significantly in foundational UX elements. This includes fast page
          load speeds, intuitive site navigation, and flawless mobile usability. The
          primary goal is to remove all friction that might prevent a user from
          accomplishing their task and send them back to the search results.
       ○​ Frame all content creation around the principle of profound user
          satisfaction. The objective is no longer just to rank for a keyword, but to be
          the definitive answer that ends the user's search.
       ○​ Integrate Conversion Rate Optimization (CRO) directly into the SEO
          workflow. Activities like crafting compelling page titles and meta
          descriptions to maximise click-through rates are no longer just about
          driving traffic; they are a direct ranking activity that feeds the NavBoost
          system. A higher CTR signals to Google that a result is more relevant and
          appealing to users.

Strategy 3 - Build Topical Authority, Not Just Keyword-Optimised Pages
The leak provides a technical look into how Google understands expertise.

It reveals features like siteFocusScore, pageEmbeddings, and siteRadius, which use
sophisticated mathematical representations (vector embeddings) to measure a
website's topical focus.

The siteFocusScore gauges how concentrated a site's content is on a core set of
topics, while siteRadius measures how much a single page deviates from that central
theme.

A page that is topically aligned with its parent site is seen as more authoritative.

This confirms that a scattergun approach to content - chasing disparate keywords
without a coherent strategy - is detrimental.

I have been a proponent for topical authority for many years, with many other SEO
including Koray Tuğberk GÜBÜR.




                                                          107

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




  ●​ Actionable Recommendations:
       ○​ Develop deep and comprehensive "content hubs" or "topic clusters" that
          thoroughly cover a business's core areas of expertise. This demonstrates a
          clear topical focus to Google's systems and establishes the site as an
          authority in its niche.
       ○​ Conduct ruthless and regular content audits. Low-quality, outdated, or
          off-topic pages should be pruned or significantly updated. These pages
          dilute a site's siteFocusScore, drag down its overall siteAuthority, and send
          negative signals to Google.
       ○​ Emphasize and showcase authorship. The leak confirms that Google stores
          author information and may even calculate an authorReputationScore.
          Associating content with named, credible experts who have a
          demonstrated history in a topic strengthens the site's E-E-A-T (Experience,
          Expertise, Authoritativeness, Trust) signals and reinforces topical authority.

Strategy 4: Adopt a Scientist's Mindset - Test, Don't Just Trust
The most significant casualty of the Google leak is trust.

The chasm between Google's public statements and its internal systems has created a
deep and likely permanent credibility deficit in some circles.

For business leaders and marketers, the era of passively accepting Google's guidance
as gospel is perhaps over.

The path forward requires a more empirical and skeptical approach.

The leak provides the technical underpinnings for the strategic advice Google has
been promoting around "people-first" and "helpful content."

The recent, often painful, "Helpful Content Updates" (HCU) can now be understood as
the public-facing application of these internal systems.

The HCU may penalise unhelpful content by measuring NavBoost signals like
badClicks and short dwell times.

It might demote sites that lack focus, which is measured by siteFocusScore. It elevates
sites with strong siteAuthority.


                                                          108

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The leak reveals the enforcement mechanisms behind the advice.

  ●​ Actionable Recommendations:
       ○​ Cultivate a culture of experimentation. The leak's revelations should not be
          treated as immutable laws but as a powerful set of hypotheses. Businesses
          must test these concepts against their own data to see what drives results
          in their specific market.
       ○​ Diversify information sources. Rely on a combination of independent
          research, peer-reviewed case studies from the SEO community, and
          internal data analytics rather than solely on Google's official blogs and
          spokespeople.
       ○​ Invest in sophisticated analytics that move beyond simple rank tracking.
          The focus should be on measuring what Google is now known to measure:
          user engagement metrics, branded search growth, and performance
          across entire topic clusters.

What the Leak Truly Tells Us
The accidental unlocking of Google's Content Warehouse did not provide a simple
cheat sheet for gaming the search rankings.

Instead, it offered something far more valuable: a moment of education.

It forced a fundamental strategic reset, sweeping away years of myth and misdirection
and replacing them with a new, evidence-based understanding of what it takes to
succeed in the world's most important digital marketplace.

The leak precipitated several critical shifts in strategic thinking.

The focus must move:

  ●​ From optimizing individual pages to building domain-level siteAuthority.
  ●​ From targeting isolated keywords to establishing deep topical authority.
  ●​ From pursuing technical tricks to building a durable brand that users know and
     trust.
  ●​ From placing blind trust in Google's public relations to engaging in empirical
     testing and healthy skepticism.




                                                         109

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Ultimately, this event dissolves the artificial silos that have long existed between SEO,
brand marketing, user experience design, and content strategy.

   ●​ To improve siteAuthority, one needs a strong brand.
   ●​ To improve NavBoost scores, one needs a flawless user experience.
   ●​ To improve siteFocusScore, one needs a coherent content strategy.

A successful digital leader can no longer treat these as separate functions; the leak
proves they are all inputs into a single, interconnected system of evaluation.

The ultimate lesson from the unlocked warehouse is that the path to sustainable
success on Google is no longer about trying to reverse-engineer a secret algorithm.

It is about building a business so genuinely valuable, authoritative, and user-focused
that the algorithm, in all its complex, multi-layered glory, has no choice but to
recognize and reward it.

The leak revealed that Google's trillions of calculations are all designed to answer a
few simple but profound questions about every business online:

  1.​ Is it an authority? (Measured by siteAuthority and siteFocusScore)
  2.​ Do its users agree? (Measured by NavBoost and Chrome data)
  3.​ Is it trustworthy and established? (Measured by hostAge and whitelists)

The game has not fundamentally changed. The rules have simply, and finally, been
revealed.

The leak doesn’t confirm everything for me. It cannot, without official Google
confirmation.

But it is a good starting point for SEO geeks.




                                                          110

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Trust in Google’s E-E-A-T, the Helpful Content Update, and the
Disconnected Entity Hypothesis
The evidence is now undeniable. The sworn testimonies from the U.S. v. Google trial
and the internal blueprints from the Content Warehouse leak have provided an
unprecedented, canonical model of Google's ranking architecture.

We know that foundational systems like Topicality (T*) and Quality (Q*) are
deliberately engineered.

We know that a powerful user-behavior engine, Navboost, refines rankings based on
13 months of click data.

And we know that a site-level authority score, long denied, is a reality. But how do
these pieces fit together?

How can a site with strong traditional signals still fail?

Trust is the Lever

Trust has emerged as a central pillar of SEO quality in Google’s guidelines and
algorithms.

In late 2022, Google expanded its well-known E-A-T (Expertise, Authoritativeness,
Trustworthiness) framework to E-E-A-T (adding Experience) and explicitly elevated
Trust as “the most important member of the E-E-A-T family”.

In parallel, Google’s Helpful Content Update (HCU) - now integrated into core
ranking systems - focuses on promoting content that is genuinely helpful, reliable, and
people-first - see hobo-web.co.uk and hobo-web.co.uk.

Based on these observations, I have developed what I call the Disconnected
Entity Hypothesis to explain these site-level quality issues and to explain why
sites lacking clear identity and trust signals have been disproportionately affected by
recent updates - here hobo-web.co.uk - but that is just part of the story for HCU
victims.


                                                          111

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Tom Capper has a nice article on Moz crystalising some of this too which might be
very pertinent, if indeed the vast majority of your traffic is non-branded traffic (which
means, it is at risk).

‘Trustworthiness’ in Google’s Quality Rater Guidelines (E-E-A-T and
Section 2.5.2)

Google’s Search Quality Rater Guidelines (QRG) make it clear that
trustworthiness is the critical factor in determining page quality.

As Google puts it: “Trust is the most important member of the E-E-A-T family because
untrustworthy pages have low E-E-A-T no matter how Experienced, Expert, or
Authoritative they may seem.” - a sentiment made clear in Paul Harr’s SMX
presentation in 2016.

In other words, no amount of expertise or authority can make up for a lack of
trust. But what exactly does “trust” mean in this context?

According to the QRG and Google’s documentation, raters evaluate trust by asking
whether a page is “accurate, honest, safe and reliable.”.

For example, YMYL (Your Money or Your Life) pages (like medical or financial
advice) must be factually accurate and reliable to be considered trustworthy,
e-commerce sites should have secure payment systems and customer service info,
and product reviews should be honest and not merely promotional - see
vertical-leap.uk.

A key instruction in the QRG is that raters should determine who is responsible for a
website and its content.

In fact, Section 2.5.2 of the guidelines - titled “Finding Who is Responsible for the
Website and Who Created the Content on the Page” - explicitly directs raters to look
for information about site ownership and content creators - see hobo-web.co.uk.

This section is essentially about identity transparency.



                                                          112

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google wants raters to find pages like “About Us” or author bios that clearly state who
owns the site, who operates it, and who authored the content.

If such information is missing when it ought to be present, the page (or site) is likely to
be judged untrustworthy.

I emphasised that this seemingly simple requirement is “quietly powerful” - it sits “at
the core of identity transparency” for a site and underpins many other trust-related
criteria.

Also that “a lack of this information when you should have it is at the core of the
Disconnected Entity Hypothesis.” hobo-web.co.uk




                                                          113

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




What “Trust” Signals Do Raters Look For?

Google’s guidelines outline several trust signals that quality raters (and by extension,
Google’s algorithms) consider when evaluating a page or site’s trustworthiness:

   ●​ Clear Website and Content Creator Identity: Raters are instructed to check
      what the website or content creators say about themselves - for instance, an
      About Us page or author profile. They ask: Is it clear who owns the site and who
      wrote the content? developers.google.com If a website is hiding its owners or
      authors without a good reason, that’s a red flag. Google has even said
      “Something that helps people intuitively understand the E-E-A-T of content is
      when it’s clear who created it… We strongly encourage adding accurate
      authorship information.” hobo-web.co.uk. In practice, this means pages should
      display author bylines (where appropriate) that link to a bio, and sites should
      have an accessible About page disclosing the organization or individuals
      behind the site. Hobo Web underlines this point: to comply with section 2.5.2,
      “your site should include: An About Page with legal and editorial ownership, a
      Contact Page with email, phone, and office hours, [and] a declared Site Editor
      or company representative” hobo-web.co.uk. In short - make it immediately
      obvious who is behind the content and website.​

   ●​ Background & Reputation (What Others Say - MOST IMPORTANT): Raters
      also look for independent information about the website or author - such as
      news articles, reviews, references, or forum discussions about them
      vertical-leap.uk. Positive mentions or reviews from credible sources can bolster
      trust, whereas scandals or numerous complaints will hurt. The QRG explicitly
      asks raters to consider “what others say about the website or content creators”
      and whether there is “independent, reliable evidence” of trustworthiness (or
      evidence of untrustworthiness). This means your site’s reputation on the wider
      web (e.g., customer reviews, expert recommendations, BBB listings, etc.)
      matters for trust. Hobo Web’s philosophy aligns with this: I advised webmasters
      to monitor and manage their online reputation, as it feeds into how Google
      assesses E-E-A-T. He notes that brand signals - like quality inbound links or
      mentions on authoritative sites can positively influence how Google treats a
      site, though they cannot compensate for a fundamentally weak or

                                                         114

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   untrusted entity - see hobo-web.co.uk andhobo-web.co.uk.​

●​ Content Accuracy and Transparency: Raters examine the content itself for
   signs of trustworthiness. Is the content factually correct and well-sourced?
   Does it cite evidence or sources for claims? Does it read as honest and
   objective, or deceptive and spammy? One of Google’s self-assessment
   questions for creators asks: “Does the content present information in a way
   that makes you want to trust it, such as clear sourcing, evidence of the
   expertise involved, [and] background about the author or the site…?”
   developers.google.com. Including references, citations, or evidence in your
   content can help establish trust, especially for YMYL topics. Additionally,
   Google encourages transparency about how content was created. In its official
   docs, Google suggests sharing “how a piece of content was produced”, for
   example, if you write a product review, you might disclose how many products
   you tested and how you tested them, possibly with photos as proof
   developers.google.com and developers.google.com. If content is AI-generated
   or heavily automated, Google advises being upfront about it when a user
   might wonder (the recent “Who, How, and Why” guidance)
   developers.google.com, since “lack of authorship transparency is considered
   unhelpful.” hobo-web.co.uk In essence, honesty about your content’s origins
   and your credentials fosters trust. Tip Add AI disclaimers to content that
   requires it.​

●​ User Safety & Site Security: Trust also encompasses user safety, which
   includes having a secure browsing environment. For e-commerce or financial
   sites, this means proper use of HTTPS, secure checkout processes, and clear
   customer service/contact information. Google’s guidelines note, for example,
   that “Online stores need secure online payment systems and reliable customer
   service” as part of trust vertical-leap.uk. If a site asks users to input sensitive
   data, raters will check for things like SSL certificates and other signs the site is
   legitimate and safe. Webmasters should ensure their site is technically secure
   (no malware, HTTPS in place) and provide channels for customer support or
   user inquiries - these are critical trust signals especially on sites that handle
   transactions.​

                                                      115

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




In summary, Google’s conception of “trust” centers on transparency, accuracy,
safety, and reliability.

Make it clear who you are, what your credentials or qualifications are, why users
should trust your content, and how you operate in an honest, user-centric manner.

These qualities are baked into the QRG and influence how Google’s algorithms assess
site quality.




                                                         116

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Domino Effect of Section 2.5.2: Why Identity
Transparency Matters
Section 2.5.2 of the QRG might sound simple, but meeting its criteria creates a
domino effect of compliance across many trust and quality factors. Hobo Web
argues that “getting 2.5.2 right sets the stage for passing nearly every E-E-A-T and
YMYL trust requirement in the guidelines.” hobo-web.co.uk

By ensuring your site clearly discloses ownership and content creators, you inherently
start to satisfy related guidelines about having sufficient contact information, content
creator info, and avoiding the “Lowest Page Quality” triggers that stem from
anonymity or lack of accountability.

Consider the multiple sections of the QRG that are tied into this basic transparency:
there are sections on “About us” and contact info (2.5.3), website reputation
(3.3.1), creator reputation (3.3.4), the definition of High E-E-A-T (Section 7.3)
versus Lowest E-E-A-T (Section 4.5.2), and specific flags for “Inadequate
information about the website or content creator” (4.5.1) and “Deceptive or
misleading website information” (4.5.3).

All of these can be directly or indirectly addressed by doing a thorough job of
disclosing who you are and taking accountability for your content

It’s no surprise that a site lacking an About page, with no author names on articles,
and no way to contact the owner would likely be labeled “Lowest E-E-A-T” in rater
evaluations hobo-web.co.uk.




                                                         117

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




And while rater evaluations don’t directly determine rankings, Google’s engineers
use that data to train the algorithms on what low-quality sites look like
hobo-web.co.uk and hobo-web.co.uk.

     Hobo Web insight: “If you’re serious about trust, rankings, or
     reconsideration requests, start with 2.5.2. It’s not just a box to check - it’s
     the foundation for the rest of your site’s credibility.” In other words,
     establishing trust via clear identity and responsibility is Step 1 for any site
     looking to perform well in Google. Hobo Web notes that complying with
     these transparency guidelines often aligns with legal requirements (e.g.
     publishing a business’s legal name, address, and terms of use) - so a side
     benefit is that you’ll likely be following the law as well. And if you aren’t
     doing these things, your site might not just be missing trust signals in
     Google’s eyes; it might be operating in a gray area legally, which is
     certainly not the kind of site Google wants to send users to
     hobo-web.co.uk.

In short, Google wants 1. trustworthy sites that 2. don’t produce search engine
first content, and a hallmark of a trustworthy site is being forthcoming about who
runs it.

This is the philosophy behind section 2.5.2.

Webmasters should ensure that every page clearly indicates who’s responsible (the
site itself can be considered the “author” if it’s a brand, but even then the brand’s
legal entity should be clea rhobo-web.co.uk).

Common best practices include an About Us page with company details, author
pages or footers with bios on content pages, easily findable contact information
(email, phone, physical address if applicable), and even schema markup
(Organization, Person) to tie the site to known entities in a machine-readable way
hobo-web.co.uk.

These steps are fundamental to establishing trust.




                                                         118

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Trust as a Ranking Factor - From Quality Guidelines to Core
Updates
Google frequently reminds us that E-E-A-T (and thus trust) is not a direct ranking
factor in itself - there’s no “Trust score” number that gets added to your ranking
calculation.

Instead, Google uses a “mix of factors” as proxies for E-E-A-T developers.google.com.
However, those factors absolutely feed into Google’s ranking algorithms. In particular,
trust-related signals have become increasingly important in core algorithm
updates.

Google’s Helpful Content Update in 2022-2023 underscored this by introducing a
site-wide classifier to identify “unhelpful content”. Initially a separate algorithm, the
Helpful Content system was “rolled into the core ranking systems” in 2024
hobo-web.co.uk.

Google’s Search Liaison Danny Sullivan described the evolution, noting that the
helpful content classifier continually evaluates sites and that “It is now part of a core
ranking system that’s assessing helpfulness on all types of aspects.” hobo-web.co.uk.

In essence, helpfulness (which strongly correlates with trustworthiness and quality) is
baked into core updates. Sullivan explained that starting in 2022, Google tuned its
ranking systems to “reduce unhelpful, unoriginal content” and has “brought what we
learned from that work into the March 2024 core update.” hobo-web.co.uk.

So how does trust play into “helpful content”? Google’s public communications
and QRG make it clear that content cannot be “helpful” if it’s untrustworthy.

A truly helpful piece of content should satisfy the user’s query in a reliable way -
which implies that the content comes from a trustworthy source and is accurate.

Danny Sullivan has emphasized that Google’s algorithms aim to reward content with
strong quality signals (like high E-E-A-T) and that “quality signals - like helpfulness -
matter more” in determining what ranks hobo-web.co.uk.


                                                          119

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Indeed, Google even directs site owners to the Quality Rater Guidelines as a “key”
reference for understanding core update impacts: “If you want a better idea of what
we consider great content, read our raters guidelines.” hobo-web.co.uk

This suggests that many of the things raters look for - especially trust indicators - are
reflected in what the core algorithm rewards or penalizes.

One important point that emerged from the HCU era is that “trust” problems can
outweigh “content” quality.

Many site owners were puzzled when the September 2023 Helpful Content Update
and subsequent spam updates hit their sites, even though their content was
well-written and user-focused.

Google personnel hinted at the answer: often the issue wasn’t the content itself, but
the site’s overall trust signals.

As John Mueller explained in one context, “with the core updates we don’t focus so
much on just individual issues, but rather the relevance of the website overall…
the way you’re making it clear to users what’s behind the content… all of those
things… play in.” hobo-web.co.uk.

In other words, core updates look at the site holistically, including whether the site
is transparent and trustworthy, not just whether a particular blog post reads well.

I observed the same pattern in sites impacted by HCU: “Content isn’t even the primary
problem,” I noted, pointing out that many affected sites had perfectly fine content
hobo-web.co.uk.

Instead, a common thread was weak trust/identity signals - although naturally, we
can’t see any user satisfaction data on this front.

Google’s Danny Sullivan himself acknowledged that some niche sites with great
content were nonetheless not performing well post-HCU, and that Google needed to
“do a better job for [creators] where their hearts are in the right place”
hobo-web.co.uk and hobo-web.co.uk - implying that the current systems might be
unintentionally penalizing some good-content sites because they lack other markers
of establishment or trust.
                                                          120

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Sullivan contrasted “niche blogs built with genuine passion/expertise” with those
“purely for search rankings”, stressing that Google “doesn’t want to reward” the latter
hobo-web.co.uk and hobo-web.co.uk.

One could interpret this as: sites that look “thin” on trust - possibly newer, not
well-established brands, minimal transparency - might get algorithmically
lumped in with low-quality sites, even if their content is decent.

Google’s challenge (and intention) is to separate the truly valuable independent sites
from the fly-by-night spam. But until they perfect that, being clearly trustworthy is a
small publisher’s best defense.

The “Helpful = Trustworthy” Principle

I explain the relationship between “helpful content” and trust: “You cannot have
‘helpful content’ if you fail [the] quality signal - helpfulness - which is on the whole
based on TRUST - no matter how good your content really is, or even how good your
links are.”.

In this analysis, a site that lacks clarity about its authorship, ownership, intent or
value will “completely violate Google’s helpfulness standard,” even if the actual
on-page content is well-written hobo-web.co.uk.

Put plainly, content cannot be deemed helpful if it comes from an untrusted
source. Imagine a website offering medical advice with great detail and correctness,
but nowhere does it state who the doctors or publishers are behind the site - from
Google’s perspective, that site fails the trust test, so the content cannot be fully
valued. Hobo Web gives a vivid example: “Imagine even a site with 100 real doctors on
it but no information about the creator or entity responsible for the site itself. Now in
Google’s world, the doctors aren’t real doctors, as far as Google is concerned… It
doesn’t matter who the authors are... or the quality of their content [if the site doesn’t
establish trust].” This drives home the point that without an identifiable,
accountable entity, your excellent content may as well be anonymous chatter.

From Google’s side, this makes sense.



                                                          121

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google’s fundamental mission is to satisfy users - and part of that is ensuring the
results they deliver won’t mislead or harm those users.

If Google can’t figure out “who is behind this site if something goes wrong?”, it’s less
likely to trust the site.

Why would Google send users to a site “where responsibility for the domain is not
clearly defined or [users can’t] easily interact with [the responsible entity]?”
hobo-web.co.uk.

If something were to go awry- say, bad financial advice leading to loss, or a scam
transaction - Google’s own reputation is at stake for having ranked that site.

Thus, trust and helpfulness are inextricably linked: a site that isn’t demonstrably
trustworthy cannot truly be deemed helpful to users, no matter the intrinsic quality of
its pages.




                                                          122

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Disconnected Entity Hypothesis
The Disconnected Entity Hypothesis (DEH) is my attempt to explain a particular
pattern seen with Google’s Helpful Content and Spam updates, and what I see is a
logical conclusion for sites that fail to meet section 2.5.2 of the Quality Rater
Guidelines..

In essence, the hypothesis posits that Google is classifying some sites as
“unhealthy” or “disconnected” entities when they lack sufficient transparency
and trust signals connecting them to a credible entity hobo-web.co.uk and
hobo-web.co.uk.

These sites then experience ranking declines regardless of content quality, as if
Google has flipped a switch that limits their visibility.

They are “disconnected” in that Google cannot connect the site to a known,
trustworthy entity (or any entity at all), which triggers distrust in the algorithm.

According to Hobo Web’s DEH analysis, if your site was hit by the late 2023 HCU/Spam
updates, “you are at best (and hopefully) a Disconnected Entity and at worst an
Unknown or Spam Entity.”hobo-web.co.uk In other words, Google’s systems might be
treating your domain as an entity with no trust backing or as an outright spam
producer.




                                                          123

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




A “Weak or Disconnected Entity,” is a domain that either:

   ●​ has become disconnected from the entity that originally built its reputation
      (e.g. a domain that expired or was sold - the new owner is essentially a new
      entity with no inherited trust), and/or​

   ●​ has insufficient transparency about the entity responsible for it (e.g. an
      independent publisher site that never clearly identifies its owner or authors),
      and/or​

   ●​ has an entity behind it that is not clearly defined and managed in a way that
      users (and Google) can easily verify hobo-web.co.uk and hobo-web.co.uk.​


Most small independent websites fail in the second or third way: they might build a
nice brand name or have good content, but they neglect to make the actual entity
(person or company) explicitly known and accessible: hobo-web.co.uk.

They focus on building a brand (which used to be enough in SEO), but not a trusted
entity. “Old SEO = brand. New SEO = brand + healthy entity status,”.

A healthy entity status means Google recognizes who you are and has some level of
confidence in you.

One striking claim from my own research is that Google’s ranking systems might
effectively be gating sites based on “Entity Health” before even considering
traditional signals like backlinks or content relevance.

Perhaps a hierarchy: “Google’s ranking systems: Entity Health Status > Links >
Relevance.” hobo-web.co.uk

If your domain is flagged as an “Unhealthy, Unknown or SPAM entity,” then “it
does not matter how good your links or content is” - the site will struggle to rank for
YMYL queries. This aligns with anecdotes of HCU-hit sites where even publishing new
high-quality content or earning new links doesn’t move the needle; the site seems to
have an invisible penalty. In Hobo’s words, “eventually = SEO will just turn OFF” for
disconnected entities until they fix their trust issues.
                                                         124

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The “disconnected” terminology also ties into being “unverified” in Google’s eyes.

Google’s constant endeavor with its Knowledge Graph and entity-based indexing is to
know what entity (organization, person, etc.) is associated with a website and content.

When a site clearly establishes that (via about pages, schema, external profiles, etc.),
it’s “connected” to an entity with some history or reputation.

When a site is vague or anonymous, Google might treat it as an island - a
disconnected entity - and thus risky or low-priority.

It’s worth noting that Hobo Web differentiates a “Disconnected Entity” from a
full-blown “Spam Entity.” A disconnected entity can be rehabilitated (it’s not
inherently bad; it’s just not well integrated into Google’s web of entity information). In
fact, Google may be giving some sites time to sort these issues out.

Sites that are genuinely spam or malicious likely get hit hardest and may never
recover. Disconnected but otherwise legitimate sites often see a slow decline rather
than an overnight crash - possibly as Google’s algorithms give them months or even
years of declining traffic as a grace period to improve trust signals before
“terminating” them completely hobo-web.co.uk.

This could explain why some sites see gradual drops across multiple core updates. It’s
a hypothesis, but one seemingly backed by the patterns I’ve observed.

So how do you “reconnect” a disconnected entity? Essentially by providing the
signals that Google found lacking - which takes us right back to Section 2.5.2
compliance and E-E-A-T principles. Hobo Web’s case studies showed that applying
“E-E-A-T principles” can revive a disconnected entity’s fortunes.

That means making the site’s real-world identity and trust factors crystal clear:
who runs it, who writes it, why it’s credible, etc. Anderson asserts, “when you take a
Disconnected Entity and reconnect it using E-E-A-T principles, it works!” - although -
that may not be the case if publishing search engine first content. In fact, in my
conclusion I state robust entity health - which consists of clear identity,
transparency, and verified expertise - is now crucial for sustainable SEO
success.

                                                          125

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google is increasingly prioritizing “entity trustworthiness,” making “clear and
verifiable entity definitions essential” if you want to rank well long-term
hobo-web.co.uk.

Let’s tie this explicitly to trust: The Disconnected Entity Hypothesis basically
highlights lack of trust (in the form of unclear entity information) as a primary reason
for sites being algorithmically downgraded. Anderson says it plainly: “A Disconnected
Entity by definition, is Unhelpful Content, and ... since Trust is the most important
factor of E-E-A-T, then it looks to me as if a lack of trust is the biggest lever (at
least one of them) to decimate anyone’s rankings.” hobo-web.co.uk.

In HCU and spam update aftermath, many affected sites were those with weak
“About” sections, no author info, or operating under brand names with no public face.

These sites got swept up as collateral damage in Google’s fight against low-quality
content. DEH suggests Google needs websites to prove their trustworthiness as
entities to avoid being misclassified.

From Google’s perspective, this isn’t punitive - it’s protective. Google is trying to
safeguard users (and its own reputation) by not surfacing content from entities it
deems untrustworthy or anonymous.




                                                          126

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Trust Signals vs. “Disconnected” Sites

It’s illuminating to map how Google’s trust signals correspond with the DEH
framework:

   ●​ Ownership & Author Transparency: Google: “Who is responsible” (Section
      2.5.2), clear author bylines, about pages hobo-web.co.uk and
      developers.google.com. Hobo: make ownership and editorial control explicit
      (full company name, real individuals, site editor, etc.) hobo-web.co.uk. DEH says
      lack of this is the definition of a disconnected (untrusted) entity
      hobo-web.co.uk. Action: Add detailed About Us, author bios, and don’t hide
      who’s behind the site.​

   ●​ Contact and Accountability: Google: Sites should have contact info and
      customer support info where appropriate (especially for YMYL and commerce)
      vertical-leap.uk. Lack of contact info is cited in QRG as a sign of low trust.
      Hobo: include Contact pages with email, phone, address, and even business
      registrations if applicable hobo-web.co.uk and hobo-web.co.uk. Action: Provide
      a means for users (and Google) to reach the responsible entity - it signals you
      stand behind your content.​

   ●​ Content Quality & Accuracy: Google: Quality raters check if content is
      accurate, well-sourced, and satisfies user needs. Trust is damaged by factual
      errors or deceptive/misleading information. Hobo: while content quality alone
      isn’t enough if trust fundamentals are broken, you still need to ensure your
      content is actually helpful and correct. Part of building trust is reviewing your
      content for accuracy and updating it. Hobo Web often stresses aligning content
      with E-E-A-T - e.g., demonstrating experience or expertise where you have it
      (don’t write outside your knowledge area just for traffic)
      developers.google.com and developers.google.com. Action: Audit your content
      for any trust killers - remove or fix poorly sourced material, cite evidence for
      claims, and stick to topics where you can provide real value.​




                                                         127

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




●​ External Reputation (“What others say”): Google: Raters search for outside
   reviews or mentions; a trustworthy site usually has some positive footprint
   (unless very new). Hobo: suggests that brand building and links help, but only
   insofar as they indicate a real presence - a brand without an identified entity is
   hollow: hobo-web.co.uk. Action: Encourage satisfied users to leave reviews
   (e.g., on Google Business Profile for local, or industry sites), get mentioned by
   other reputable sites (press, collaborations), and address negative feedback to
   improve your online sentiment. This isn’t quick SEO magic, but over time it
   contributes to Google’s perception of your trustworthiness.​

●​ Technical Trust Signals: Google: Secure your site (HTTPS), avoid excessive or
   disruptive ads, and generally provide a good user experience - these indirectly
   relate to trust (a safe, professional site vs. a shady, ad-ridden one). While not
   explicitly our focus, these are table stakes in site quality. Hobo: in his checklists
   and audits, he also covers these basics. Action: Use HTTPS, have a clean site
   free of malware, and be transparent if you use cookies/trackers (privacy
   policies) - these things show professionalism and care for users.​

●​ Honesty about Content Creation: Google: If you use AI or automate content,
   say so when appropriate; don’t publish ghost-written or AI-written content
   under a fake persona - that’s a trust breaker (and against Google’s spam
   policies if done to mislead): developers.google.com and
   developers.google.com. Hobo: strongly criticizes “faking E-E-A-T” with fake
   author profiles or fake credentials: hobo-web.co.uk. He calls that a “grey…
   Black even!” tactic: hobo-web.co.uk - it may temporarily fool some, but it’s
   unethical and could backfire. Action: Be authentic. If you must use a pen name
   or brand name as author, make sure there’s still an accountable entity behind it
   (and explain the reasoning to users, e.g. “Editorial Team” authorship with a
   team page). Don’t invent a fake expert; either use real experts or at least real,
   transparent identities for your content creators.​




                                                      128

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By comparing these perspectives, it’s clear that Google’s official guidance and
Hobo Web’s SEO philosophy are closely aligned on trust signals. Hobo Web’s DEH
basically says “Do everything Google tells raters a trustworthy site should do - or risk
being filtered out.”

Both emphasise genuine transparency and an investment in credibility.




                                                         129

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




