Strategic SEO       v.2




      2025
   Shaun Anderson
                                                   Strategic SEO 2025




Table of Contents

Strategic SEO v.2
2025
Shaun Anderson
Introduction
Section 1: The New Reality
   How Google Works
   United States et al. v. Google LLC
       The Architecture of Google Search Ranking
   The Two Pillars of Ranking: An Overview
   Deconstructing the Signals - The Core Systems
       Quality Score (Q*) - The Engine of the 'Quality' Signal
       Navboost and Topicality (T*) - The Engines of the 'Popularity' Signal
       Information Retrieval and the Primacy of "Hand-Crafted" Signals
   Trustworthiness
       Freshness (Timeliness of Content)
       Linking Behaviour (Link Signals and Page Reputation)
       The Final Layers: Location and Personalisation
       Beyond RankBrain: The Shift to Semantic Understanding
       A Forward-Looking Note on Legal Remedies
       How Human Quality Raters Are Used
            Key Takeaways
       The Role of Human Quality Raters
       Direct Training Data for Ranking Models
       Improving Performance on Difficult Queries
       Providing Foundational Data for Machine Learning
       Deviation From Google's Past Statements
       Google’s Public Stance
       The Deviation Revealed in Court
   DOJ v. Google Disclosure: The Popularity Signal (P*)
       What This Means
       SEO Relevance
       Topicality (T*) or The ABCs of Relevance
       Clicks vs. Quality
   The Q* Metric - A New Understanding of Site-Level Quality
       “QRank”, Quality Scores & Authority Signals (2010s)
   The Navboost System
       The Confirmed Role of Large-Scale User Interaction Data
                                                            2

         © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




       Key Takeaways
PageRank
       Original PageRank: Link-Based Importance (Late 1990s)
       PageRank (Link-Based Authority) - What SEOs said at the time
       TrustRank: Incorporating Link Trust & Spam Protection (2004–2000s)
       PageRank’s Role Today
       Key Take-aways
       Panda and Google’s Site-Level Quality Score (QScore)
       Impact and Legacy of Panda
   Google’s Panda Algorithm - Site-Level Quality Scoring
       Origins and Purpose of Panda
       Assessing Site Quality
       Evolution of the Panda Algorithm
   Deconstructing Site Quality via Exploit
       A Foundational Ranking Gate
       Measuring Trust: How Site Quality is Calculated
       The AI Dilemma and the Helpful Content Correction
       Conclusion: Brand is the New Bottom Line
   Practical Steps for Webmasters to Improve Site Quality
   Google’s communications about Quality Score
   The Role of Machine Learning - RankBrain, DeepRank, and the AI Layer
       RankBrain: The Query Interpreter
       DeepRank and the Pursuit of Transparency
       The Human-ML Symbiosis
       Overview of Google's RankBrain and BERT-based RankEmbed
       RankBrain
           What RankBrain Does
           Technical Mechanism (Neural Embeddings)
           Integration with Ranking Signals
           Current Usage
       BERT-based RankEmbed
           What BERT (RankEmbed) Does
           Technical Mechanism (Transformer-based Neural Network)
       RankEmbed and Semantic Matching
       Integration with Ranking Signals
       Current Usage
       Key Takeaway
   Reconciling the New Model of Google Search
       The Unified Ranking Pipeline
                                                           3

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




       The Data Feedback Loop as a Competitive Moat
       The Chasm Between Public Narrative and Internal Reality
   The Unlocked Warehouse
       What Google's Accidental Leak Tells Us About Search, Secrecy, and Strategy
   The Anatomy of a Leak
       Subsection 1.1: What Was Actually Leaked?
       A Glimpse Inside the Machine: Core Systems and "Twiddlers"
   The Four Great Vindications - Where SEO Theory Met Google's Reality
       The Ghost in the Machine: siteAuthority is Real
       The All-Seeing Eye: Clicks, Chrome, and NavBoost
       The Walled Garden: The "Sandbox" and hostAge
       The Unspoken Hierarchy - Whitelists and Special Treatment
       The path to authority is paved with positive user interactions.
   Strategic Imperatives in a Post-Leak World
       Strategy 1: Brand is the Ultimate Ranking Factor
       Strategy 2: User Experience is Undeniably the New Core SEO
       Strategy 3 - Build Topical Authority, Not Just Keyword-Optimised Pages
       Strategy 4: Adopt a Scientist's Mindset - Test, Don't Just Trust
   What the Leak Truly Tells Us
   Trust in Google’s E-E-A-T, the Helpful Content Update, and the Disconnected Entity Hypothesis
       Trust is the Lever
       ‘Trustworthiness’ in Google’s Quality Rater Guidelines (E-E-A-T and Section 2.5.2)
       What “Trust” Signals Do Raters Look For?
   The Domino Effect of Section 2.5.2: Why Identity Transparency Matters
   Trust as a Ranking Factor - From Quality Guidelines to Core Updates
       The “Helpful = Trustworthy” Principle
   The Disconnected Entity Hypothesis
       Trust Signals vs. “Disconnected” Sites
Section 2: The Strategic Playbook
   Entity SEO: How to Get Your Business Recognised as an Entity by Google
       What Are “Entities” in Google’s Eyes?
       Why Entity SEO Matters (Especially for Small Businesses)
       Practical Steps to Implement Entity SEO for Your Business
       Secure Your Business’s Presence in Authoritative Databases
       Designate an “Entity Home” on Your Website and Use Schema Markup
       Build a Cohesive Topical Content Structure (Site-Wide Entity Optimisation)
       Optimise On-Page Content with Entities in Mind (Semantic SEO)
       Highlight Authors and Build E-E-A-T Signals Around Your Entity
       Maintain Consistency and Corroborate Information Across the Web
                                                            4

         © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                               Strategic SEO 2025




    Leverage Entity-Oriented SEO Tools (Optional, Advanced)
Building Trust and Recovering from Trust Deficits
    “Trust” is the linchpin
    Key Takeaway
Search Engine-First Content - The Impact on the Web and Small Businesses
    Why Google Targeted “Search Engine-First” Content
The Helpful Content Update (HCU)
Impact on Small Businesses and Publishers
    A Hard Road to Recovery (Or None at All)
The E-E-A-T Factor
Ongoing Debate: The Future of Content and Small Businesses
Prepare for the era of Zero-Click Marketing
    The Mechanics of the Clickless SERP: A Rogues' Gallery of Features
The Great Divide - A Digital Cold War for the Future of the Web
    Two Competing Realities
    The Critics' Corner: "A Slow, Brutal Asphyxiation of Organic Traffic"
    The Core Argument: Traffic and Revenue Annihilation
    Another Wound: Loss of Strategic Data
    Pushing Back on the "Adapt or Die" Narrative
    The Proponents' Platform: "Influence Has Always Been Better Than Traffic"
    The Core Argument: Shift from Clicks to Influence
    The Silver Lining: Higher Quality Traffic
    The Opportunity: On-SERP Brand Building
    Table 2: The Zero-Click Debate: Threat vs. Opportunity
Inside the "Walled Garden" and Google's Official Rationale and Rebuttals
    The View from Mountain View
    The Prime Directive: Enhancing the User Experience
    The "More and Better Clicks" Rebuttal
    The "Fair Use" Defense: Our Position on Content Scraping
The Ripple Effect: Sector-by-Sector Impact Analysis
    No Business Left Untouched
    The Small Business Squeeze: A Double-Edged Sword
    The Downside: Traffic Evaporation and Lost Opportunities
    The Upside: The Power of the Local Pack
    The E-commerce Conundrum: Collapsing the Funnel
    Informational vs. Transactional Queries
    The Publishers' Plight: An Existential Crisis
The Legal Battlefield - Copyright, Antitrust, and the Fight for Fair Use
    The Web Goes to Court
                                                        5

     © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




        The Antitrust Front: Accusations of Monopoly Abuse
        The Copyright Front - The "Fair Use" Standoff
    The Zero-Click Playbook: A New SEO Framework for 2025 and Beyond
        From Optimisation to Influence
        Pillar 1: On-SERP SEO - Winning on Google's Turf
        Pillar 2: Building a Moat - Becoming Click-Independent
        Pillar 3: Rethinking Measurement - A New Scorecard for Success
        Key Takeaway – The Future of marketing is Influence
Section 3 - A Practical Framework for SEO in the Post-Trial Era
    The Practitioner's Dilemma in a Post-Secrecy World
        Mastering Foundational Signals (T* & Q*): The Need for a Foundational Diagnostic Audit
        Engineering for User Satisfaction (Navboost): The Role of Longitudinal Performance Monitoring
        Building Verifiable Trust (The Q* & E-E-A-T Nexus): The Entity Trust & Verification Process
        Future-Proofing for the AI-Driven SERP: AI-Readiness and Narrative Defence
        Conclusion: An Engineered Approach to Modern SEO
        About the Author & The Path Forward
    SEO Evidence Brief
        DOJ v. Google – Case No. 20-cv-3010 (Remedial Phase Opinion)
            1. User Data as a Core Input
            2. Learning from User Feedback
            3. Page Scoring in Crawling
            4. Quality & Popularity Signals in Crawling
            5. PageRank as an Input to Quality Score
            6. Source of Quality Signals
            7. Human Raters as Training Data for RankEmbed
            8. Direct Role of Rater Scores
            9. Rater Data Improves Long-Tail Search
            10. Raw vs. Deep-Learning Ranking Signals
            11. Spam Score as a Quality Signal
            12. User Queries as Training Data
            13. Signals from Device & Context
            14. Chrome Visit Data as a Popularity Signal
            15. Freshness & Long-Tail Queries
            16. Scale, Quality, and User Trust
            17. Glue & Navboost Data (Super Query Logs)
            18. RankEmbed Training Data = Quality Edge
            19. User-Side Data Defined (Click & Query Logs)
            20. Quality Measures & PageRank as Authoritativeness
            21. Spam Scores & Device-Type Flags in Indexing
                                                            6

         © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                           Strategic SEO 2025




    22. User Interaction Signals (Clicks, Hovers, Pogo-Sticking, Dwell Time)
    23. Required Disclosure of Quality, Popularity, Spam & Device-Type Flags
    24. Spam & Low-Value Pages Excluded from Index
    25. Filtering Training Data for Quality (Duplicates, Spam, Garbage)
    26. Index Size & the “80–20 Problem”
    27. Popularity Signal from Chrome Visit Data
    28. Popularity as a Top-Level Signal
    29. Popularity & Quality Guide Crawling Frequency
    30. Plaintiffs Sought Disclosure of Popularity (via Navboost/Glue)
    31. Popularity = Chrome Data + Anchors
    31. Popularity & Quality as Fundamental Ranking Signals
SEO Community: The People on the Front Lines
    References:




                                                    7

 © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Introduction
My name is Shaun Anderson, known online as @Hobo_Web. I have been a specialist
SEO consultant since 2006, and worked online as a website designer and developer
since the early 2000s.

This book is the 6th (the first published in 2009) in a series on SEO strategy, but it
marks a significant departure from the past 5 books. This is a book of advanced SEO
concepts for professional SEOs.

The companion book to this book is Hobo: Beginner SEO 2025. That is for beginners;
this book is not. The third book in the trilogy is Hobo: Strategic AI SEO 2025 again, a
more advanced book.

Following the landmark U.S. v. Google antitrust case disclosures, and the Content
Warehouse leak in early 2024, the foundational rules of search have been rewritten.
Long speculated ranking factors have been all but confirmed.

This book is an advanced strategic playbook for the professional SEO who
needs to understand and master this new reality.

My analysis is grounded in primary source evidence: sworn testimony from the DOJ
trial, Google's own patents, official documentation, exploits and recent data leaks.

We will move beyond common knowledge and deconstruct the systems that truly
define search ranking in 2025.

The goal is to provide a durable, evidence-based framework for making high-stakes
strategic decisions. Listen to an audio overview of this book. See previous editions.

Let's begin.




                                                           8

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




Section 1: The New Reality
How Google Works




The Google Algorithm Update Correction Hypothesis (Shaun Anderson, Hobo)


Google’s DOJ trial disclosures confirm that search ranking hinges on two top-level
signals - Quality (Q*) and Popularity (P*) - powered by modular systems like
Navboost, Topicality, and PageRank, with user interaction data at the core.

SEO success now depends on trust, authority, user engagement, freshness, and
adapting to potential legal-driven changes in Google’s architecture.

SEO really can be broken down, based on Google V DOJ evidence, into "Mastering
your P*s and Q*s". "Knowing your A, B and Cs", "dotting your I's" and "crossing
your T*s".




                                                             9

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Key Takeaways: DOJ v. Google – SEO Insights (2025)

   ●​ Two Core Signals Drive Ranking: Google’s system reduces to Quality (Q*)
      and Popularity (P*), revealed in DOJ trial documents.
   ●​ Modular Architecture: Underlying systems like Topicality (T*), Navboost, and
      RankBrain feed into those top-level signals.
   ●​ User Data Is Central: Clicks, scrolls, Chrome visit data, dwell time, and
      pogo-sticking are leveraged as critical ranking feedback.
   ●​ Hand-Crafted Signals Dominate: Most ranking factors are engineered
      manually, not black-box ML, for control and stability.
   ●​ Freshness Matters: Google boosts recency for queries where timeliness is
      essential (news, events), balancing against historical clicks.
   ●​ Links Still Core: PageRank (distance from trusted sources), anchor text, and
      backlink quality remain crucial authority signals.
   ●​ Context Layers Refine Results: Location and personalisation heavily shape
      what individual users see beyond the “universal” rank.
   ●​ AI as Final Layer: RankBrain, BERT, and MUM don’t replace hand-crafted
      signals — they refine them with semantic understanding.
   ●​ ⚖️ Legal Impact Ahead: DOJ remedies may force changes to Google’s
      ranking systems, shaping SEO strategy going forward.




                                                         10

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




United States et al. v. Google LLC




The antitrust case, United States et al. v. Google LLC, initiated by the US Department
of Justice (DOJ) in 2020, represents the most significant legal challenge to Google’s
market power in a generation.

While the legal arguments focused on market monopolisation, the proceedings
inadvertently became a crucible for technical disclosure, forcing Google to all but
reveal the long-guarded secrets of its search engine architecture.

The trial's technical revelations were not incidental; they were central to the core legal
conflict.

The DOJ's case rested on the premise that Google unlawfully maintained its monopoly
in general search and search advertising through a web of anticompetitive and
exclusionary agreements with device manufacturers and browser developers,
including Apple, Samsung, and Mozilla.




                                                          11

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




These contracts, often involving payments of billions of dollars annually, ensured
Google was the pre-set, default search engine for the vast majority of users, thereby
foreclosing competition by denying rivals the scale and data necessary to build a
viable alternative.

This legal challenge created a strategic paradox for Google.

To counter the DOJ's accusation that its dominance was the result of illegal
exclusionary contracts, Google's primary defence was to argue that its success is a
product of superior quality and continuous innovation - that users and partners
choose Google because it is simply the best search engine available.

This "superior product" defence, however, could not be asserted in a vacuum.

To substantiate the claim, Google was compelled to present evidence of this
superiority, which necessitated putting its top engineers and executives on the
witness stand. Individuals like Pandu Nayak, Google's Vice President of Search, and
Elizabeth Reid, Google's Head of Search, were tasked with explaining, under oath, the
very systems that produce this acclaimed quality.

Consequently, the act of defending its market position legally forced Google to
compromise its most valuable intellectual property and its long-held strategic secrecy.

The sworn testimonies and internal documents entered as evidence provided an
unprecedented, canonical blueprint of Google's key competitive advantages.

At the heart of these revelations is the central role of user interaction data.

A recurring theme throughout the testimony was that Google's "magic" is not merely
a static algorithm but a dynamic, learning system engaged in a "two-way dialogue"
with its users.

Every click, every scroll, and every subsequent query is a signal that might teach the
system what users find valuable.

This continuous feedback loop, operating at a scale that Google's monopoly ensures
no competitor can replicate, is the foundational resource for the powerful ranking
systems detailed in the trial.
                                                         12

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Architecture of Google Search Ranking

The trial testimony and exhibits dismantle the popular conception of Google’s ranking
system as a single, monolithic algorithm. Instead, they reveal a sophisticated,
multi-stage pipeline composed of distinct, modular systems, each with a specific
function and data source.

This architecture is built upon a foundation of traditional information retrieval
principles and human-engineered logic, which is then powerfully refined by systems
that leverage user behaviour data at an immense scale.

While initial trial exhibits hinted at this modularity, the later unredacted remedial
opinion in the DOJ v. Google case provided the definitive, high-level blueprint. The
court revealed that Google has two "fundamental top-level ranking signals" that
are the primary inputs for a webpage's final score: Quality (Q*) and Popularity (P*).

These two signals also help Google determine how frequently to crawl webpages to
keep its index fresh.

This analysis details the core components of this architecture, showing how the
previously revealed systems of Topicality (T*), Navboost, and Q* are the essential
building blocks for Google's top-level signals.




                                                         13

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Two Pillars of Ranking: An Overview
The systems detailed in the original trial are now best understood as the underlying
components that feed into two fundamental signals. The following table summarises this
confirmed two-pillar architecture.


 System Name            Primary                  Key Data                  Engineering              Key Revelation
                        Function                 Inputs                    Approach                 Source


 Quality (Q*)           Assesses the             PageRank                  Hand-crafted by          Remedial
                        overall                  (distance from            engineers,               Opinion, Trial
                        trustworthiness,         seed sites),              largely static           Exhibits,
                        authoritativenes         content-derived           score.                   Engineer
                        s, and quality of        metrics (like the                                  Deposition
                        a                        'Body' signal),
                        website/domain.          spam scores,
                                                 and human rater
                                                 evaluations.


 Popularity (P*)        Measures how             Chrome visit              Data-driven              Remedial
                        widely visited           data, number of           system, refined          Opinion, Pandu
                        and                      anchors (the 'A'          by engineers.            Nayak
                        "well-linked" a          signal), user                                      Testimony
                        page is to               interaction data
                        promote popular          (from Navboost,
                        documents.               including
                                                 aggregated
                                                 good/bad/lastLo
                                                 ngest clicks).


 RankBrain              Interprets novel,        Historical search         Machine                  Eric
                        ambiguous, and           data (not live            Learning                 Lehman/Pandu
                        long-tail search         user data).               (unsupervised).          Nayak
                        queries.                                                                    Testimony




                                                          14

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Deconstructing the Signals - The Core Systems
The foundational systems revealed during the trial provide the mechanics for the
top-level signals.

Quality Score (Q*) - The Engine of the 'Quality' Signal

The Quality Score (Q*) is the internal system that assesses the overall
trustworthiness and quality of a website or domain. It is a hand-crafted, largely static
score that functions as the core of the top-level Quality signal. Its key data inputs
include PageRank and the link distance from trusted "seed" sites, which align
perfectly with the remedial opinion's description of the Quality signal.

Navboost and Topicality (T*) - The Engines of the 'Popularity' Signal

The top-level Popularity (P*) signal is powered by a combination of systems that
measure user engagement and link structures.

   ●​ Navboost: This is the primary user interaction engine. As revealed in Pandu
      Nayak’s testimony, Navboost is a data-driven system that refines rankings
      based on 13 months of aggregated user click data, including metrics like
      good, bad, and “last longest” clicks. This system draws its information from
      a vast, underlying data warehouse codenamed ‘Glue’, which logs the trillions
      of user interactions that Google processes. The explicit confirmation of
      Chrome visit data as a direct input for the Popularity signal was particularly
      significant, as Google had historically been less direct about the extent to
      which it leverages its browser's data for ranking purposes. It provides the
      “Chrome visit data” and “user interaction” components of the P* signal.




                                                          15

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




   ●​ Topicality (T*): This system computes a document's direct relevance to query
      terms and serves as a foundational score. Testimony from engineer HJ Kim
      revealed it is composed of "ABC" signals:
         ○​ Anchors (A), Body (B), and Clicks (C). Within the new framework:​

                     ■​ The Anchors (A) and Clicks (C) components serve as direct
                        inputs to the Popularity (P*) signal.
                     ■​ The Body (B) component, based on the text of the document
                        itself, is a "content-derived metric" that feeds into the Quality
                        signal.

Information Retrieval and the Primacy of "Hand-Crafted" Signals

Contrary to the prevailing narrative of an all-encompassing artificial intelligence, the
trial revealed that Google's search ranking systems are fundamentally grounded
in signals that are "hand-crafted" by its engineers.

This deliberate engineering philosophy prioritises control, transparency, and the
ability to diagnose and fix problems, a stark contrast to the opaque, "black box"
nature of more complex, end-to-end machine learning models.

The deposition of Google Engineer HJ Kim was particularly illuminating on this point.
He testified that "the vast majority of signals are hand-crafted," explaining that the
primary reason for this approach is so that "if anything breaks, Google knows what
to fix".

This methodology is seen as a significant competitive advantage over rivals like
Microsoft's Bing, which was described as using more complex and harder-to-debug
ML techniques.

The process of "hand-crafting" involves engineers analysing relevant data, such as
webpage content, user clicks, and feedback from human quality raters, and then
applying mathematical functions, like regressions, to define the "curves" and
"thresholds" that determine how a signal should respond to different inputs.

This human-in-the-loop system ensures that engineers can modify a signal's
behaviour to handle edge cases or respond to public challenges, such as the spread
                                                          16

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




of misinformation on a sensitive topic.

This foundational layer of human-engineered logic provides the stability and
predictability upon which more dynamic systems are built.




                                                         17

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




Trustworthiness
“Q* (page quality (i.e., the notion of trustworthiness)) is incredibly important. If
competitors see the logs, then they have a notion of "authority" for a given
site.” February 18, 2025, Call with Google Engineer HJ Kim (DOJ Case)

I agree - if this information were made available, it would be abused.




E-E-A-T


The emergence of these distinct systems - T* for query-specific relevance, Q* for
static site quality, and Navboost for dynamic user-behaviour refinement - paints
a clear picture of a modular, multi-stage ranking pipeline.

The process does not rely on a single, all-powerful algorithm.

Instead, it appears to be a logical sequence: initial document retrieval is followed by
foundational scoring based on relevance (T*) and trust (Q*).


                                                            18

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




This scored list is then subjected to a massive re-ranking and filtering process
by Navboost, which leverages the collective historical behaviour of users.

Only the small, refined set of results that survives this process is passed to the final,
most computationally intensive machine learning models.

This architecture elegantly balances the need for speed, scale, and accuracy, using
less expensive systems to do the initial heavy lifting before applying the most
powerful models.




Disconnected Entity




                                                            19

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Freshness (Timeliness of Content)

Google also considers freshness - how recent or up-to-date the information on a
page is, especially for queries where timeliness matters.

Trial testimony and exhibits detailed how freshness influences rankings:

   ●​ Freshness as a Relevance Signal: “Freshness is another signal that is
      ‘important as a notion of relevance’,” Pandu Nayak testified regmedia.co.uk. In
      queries seeking current information, newer content can be more relevant.
      Nayak gave an example: if you’re searching for the latest sports scores or
      today’s news, “you want the pages that were published maybe this morning or
      yesterday, not the ones that were published a year ago.” regmedia.co.uk Even if
      an older page might have been relevant in general, it won’t satisfy a user
      looking for the latest updates. Thus, Google’s ranking system will favour more
      recently published pages for fresh information queries. Conversely, for topics
      where age isn’t detrimental (say, a timeless recipe or a classic novel), an
      older authoritative page can still rank well. As Nayak put it, “deciding
      whether to use [freshness] or not is a crucial element” of delivering quality
      results regmedia.co.uk - Google must judge when recency should boost a
      result’s ranking and when it’s less important.
   ●​ Real-Time Updates for Breaking Queries: John Giannandrea (former head of
      Google Search) explained that “Freshness is about latency, not quantity.” It’s
      not just showing more new pages, but showing new information fast when it’s
      needed regmedia.co.uk. “Part of the challenge of freshness,” he testified, “is
      making sure that whatever gets surfaced to the top… is consistent with what
      people right now are interested in.” regmedia.co.uk For example, “if somebody
      famous dies, you kind of need to know that within seconds,” Giannandrea said
      regmedia.co.uk. Google built systems to handle such spikes in information
      demand. An internal 2021 Google document (presented in court) described a
      system called “Instant Glue” that feeds very fresh user-interaction data into
      rankings in near real-time. “One important aspect of freshness is ensuring that
      our ranking signals reflect the current state of the world,” the document stated.
      “Instant Glue is a real-time pipeline aggregating the same fractions of
      user-interaction signals as [the main] Glue, but only from the last 24 hours of
      logs, with a latency of ~10 minutes.” justice.gov In practice, this means if there’s
                                                         20

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   a sudden surge of interest in a new topic (e.g. breaking news), Google’s
   algorithms can respond within minutes by elevating fresh results (including
   news articles, recent forum posts, etc.) that match the new intent. Google also
   uses techniques (code-named “Tetris” in one exhibit) to demote stale
   content for queries that deserve fresh results and to promote newsy content
   (e.g. Top Stories) when appropriate justice.gov
●​ Balancing Freshness vs. Click History: One difficulty discussed at trial is that
   older pages naturally accumulate more clicks over time, which could bias
   ranking algorithms that learn from engagement data. Nayak noted that pages
   with a long history tend to have higher raw click counts than brand-new pages
   (simply by having been around longer) regmedia.co.uk. If the system naively
   preferred results with the most clicks, it might favour an outdated page that
   users have clicked on for years, over a fresher page that hasn’t had time to
   garner clicks. “Clicks tend to create staleness,” as one exhibit put it
   regmedia.co.uk. To address this, Google “compensates” by boosting fresh
   content for queries where recency matters, ensuring the top results aren’t just
   the most popular historically, but the most relevant now. In essence, Google’s
   ranking algorithms include special freshness adjustments so that new,
   pertinent information can outrank older but formerly popular pages when
   appropriate regmedia.co.uk. This keeps search results timely for the user’s
   context.




                                                      21

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Linking Behaviour (Link Signals and Page Reputation)

The trial also illuminated how Google uses the web’s linking behaviour - how pages
link to each other - as a core ranking factor. Links serve both as votes of authority
and as contextual relevance clues:

   ●​ Backlink Count & Page Reputation: Google evaluates the number and quality
      of links pointing to a page to gauge its prominence. Dr. Lehman explained
      during testimony that a ranking “signal might be how many links on the web are
      there that point to this web page or what is our estimate of the sort of
      authoritativeness of this page.”regmedia.co.uk In other words, Google’s
      algorithms look at the link graph of the web to estimate a page’s authority: if
      dozens of sites (especially reputable ones) link to Page X, that’s a strong
      indication that Page X is important or trustworthy on its topic. This
      principle underlies PageRank and other authority signals. By assessing “how
      many links… point to the page,” Google infers the page’s popularity and
      credibility within the web ecosystem regmedia.co.uk. (However, it’s not just raw
      counts - the quality of linking sites matters, as captured by PageRank’s
      “distance from a known good source” metric justice.gov.)
   ●​ Anchor Text (Link Context): Links don’t only confer authority; they also carry
      information. The anchor text (the clickable words of a hyperlink) tells Google
      what the linked page is about. As noted earlier, Pandu Nayak highlighted that
      anchor text provides a “valuable clue” to relevance regmedia.co.uk. For
      example, if dozens of sites hyperlink the text “best wireless headphones” to a
      particular review page, Google’s system learns that the page is likely about
      wireless headphones and is considered “best” by those sources, boosting its
      topical relevancy for that query. This context from linking behaviour helps
      Google align pages to queries beyond what the page’s own text says. It’s a way
      of leveraging the collective judgment of website creators: what phrases do
      others use to describe or reference your page? Those phrases become an
      external signal of the page’s content. Google combines this with on-page
      signals (as part of topicality scoring) to better understand a page’s subject
      matter regmedia.co.uk.




                                                         22

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




   ●​ Link Quality over Quantity: Not all links are equal. Through PageRank and
      related “authority” algorithms, Google gives more weight to links from
      reputable or established sites. One trial exhibit described PageRank as
      measuring a page’s proximity to trusted sites (a page linked by high-quality
      sites gains authority; one linked only by dubious sites gains much less)
      justice.gov. This shows that linking behaviour is evaluated qualitatively. A
      single backlink from, say, a respected news outlet or university might boost a
      page’s authority more than 100 backlinks from low-quality blogs. Google also
      works to ignore or devalue spammy linking schemes. (While specific anti-spam
      tactics weren’t detailed in the trial excerpts we saw, the focus on “authoritative,
      reputable sources” implies that links from spam networks or “content farms”
      are discounted - aligning with Google’s long-standing efforts to prevent link
      manipulation.) I go into link building more in my article on Link building for
      beginners.

In summary, the DOJ’s antitrust trial pulled back the curtain on Google’s ranking
system.

Topicality signals (page content and context from anchors) tell Google what a page
is about and how relevant it is to a query.

Authority signals (like PageRank and quality scores) gauge if the page comes from a
trustworthy, reputable source.

Freshness metrics ensure the information is up-to-date when timeliness matters. And
the web’s linking behaviour - both the number of links and the anchor text - feeds
into both relevance and authority calculations.

All these factors, largely handcrafted and fine-tuned by Google’s engineers
justice.gov, work in concert to rank the billions of pages on the web for any given
search.

As Pandu Nayak summed up in court, Google uses “several hundred signals” that
“work together to give [Google Search] the experience that is search today.”
regmedia.co.uk

Each factor - topical relevance, authority, freshness, links, and many more - plays a
                                                         23

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




part in Google’s complex, evolving ranking algorithm, with the aim of delivering the
most relevant and reliable results to users.

The Final Layers: Location and Personalisation

While the core systems produce a universal ranking based on quality and popularity,
the results a user actually sees are heavily tailored by a final, powerful layer of
context. The trial focused on the foundational architecture, but the live search
experience is profoundly shaped by signals specific to the individual user.

   ●​ Location as a Dominant Signal: For a vast number of queries, the user's
      physical location is the single most important ranking factor. For searches like
      "pubs near me" or "solicitors in Greenock," Google's core algorithm is
      secondary to its ability to identify relevant, local results. It determines a user’s
      location with high precision using device GPS, Wi-Fi signals, and IP addresses
      to transform a generic query into a geographically specific and immediately
      useful answer.
   ●​ Personalisation from Search History: Beyond location, Google refines
      rankings based on a user's individual search history. This system learns a user's
      interests and intent over time to resolve ambiguity. For instance, a user who
      frequently searches for software development topics will likely see results
      about the programming language if they search for "python," whereas a user
      whose history is filled with zoology queries will see results about the snake.
      This layer of personalisation ensures that the final search results page is not
      just a list of high-quality, popular documents, but a bespoke answer sheet
      tailored to the user's implicit context and previous behaviour.




                                                         24

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Beyond RankBrain: The Shift to Semantic Understanding

The trial rightly highlighted RankBrain as a pioneering use of machine learning in
search. However, to understand Google's modern capabilities, it is crucial to recognise
that its AI has since evolved from simply interpreting novel queries to fundamentally
understanding the meaning of language itself. This represents a shift towards
semantic search.

Subsequent models like BERT (Bidirectional Encoder Representations from
Transformers) changed the game entirely. Unlike earlier systems that processed
words in a query one by one, BERT analyses the full context of a word by looking at
the words that come before and after it. This is critical for understanding nuance and
intent. For example, in the query "can you get medicine for someone pharmacy," BERT
understands that the preposition "for" is the most important word, fundamentally
changing the query's meaning.

This evolution continued with later models like MUM (Multitask Unified Model),
designed to understand information across different languages and formats (like
images and text) simultaneously. These advanced AI systems do not replace the
foundational signals like Navboost or Q*. Instead, they act as a supremely intelligent
final analysis layer. They take the pool of high-quality, relevant results identified by the
core systems and re-rank them based on a deep, contextual comprehension of what
the user truly means, making the search engine feel less like a database and more like
a dialogue.

A Forward-Looking Note on Legal Remedies

Looking ahead, it is important to note that while this architecture represents Google's
current competitive advantage, the ongoing remedies phase of the DOJ trial could
mandate changes to these very systems. The future of search, therefore, may be
shaped as much by court rulings as by Google's own engineers.




                                                          25

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




How Human Quality Raters Are Used

The court document reveals that scores from human quality raters are a direct and
foundational input for training Google's core ranking models.

Key Takeaways

   ●​ Human quality rater scores are a direct training input for Google’s core
      ranking models (RankEmbed, RankEmbedBERT).
   ●​ These scores are foundational data, not peripheral feedback, combined with
      query and click logs.
   ●​ Court testimony shows rater-trained models improved Google’s performance
      on long-tail queries.
   ●​ This contradicts Google’s long-standing public stance that rater scores only
      serve as indirect benchmarks.

The Role of Human Quality Raters

The DOJ v. Google remedial opinion makes clear that human quality raters are not
just external evaluators - their judgments directly shape the very core of Google’s
ranking systems. The opinion reveals that the RankEmbed and RankEmbedBERT
models, which are central to Google’s AI-based ranking, are trained on two primary
sources of data: search logs and human rater scores. This elevates rater input
from “guidance” to direct training data.

The testimony of Google’s Vice President of Search, Dr. Pandu Nayak, further
highlights their impact: rater-trained RankEmbedBERT models significantly improved
Google’s ability to process complex, long-tail queries, where language
understanding is essential.

The court emphasised that these scores form a foundational dataset in combination
with user interaction logs. The data pipeline for RankEmbed models explicitly relies on
the scoring of web pages by raters, embedding their judgments into machine
learning systems that decide how billions of pages are ranked.

This stands in contrast to Google’s public communications, which have long
maintained that rater scores do not directly affect site rankings. While technically true
                                                          26

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




at the individual page level, the opinion shows that, in aggregate, rater scores are
systemic training inputs that define how the search engine learns to rank. The
models built from this data have “directly contributed to Google’s quality edge
over competitors,” underscoring just how central rater input is to the evolution of
Google Search.

Direct Training Data for Ranking Models

The document explicitly states that human rater scores are one of two primary data
sources used to train the RankEmbed and RankEmbedBERT models. These are
described as sophisticated, AI-based systems critical to Google's search quality:

     “RankEmbed and its later iteration RankEmbedBERT are ranking
     models that rely on two main sources of data: % of 70 days of search
     logs plus scores generated by human raters and used by Google to
     measure the quality of organic search results.”​
     View in PDF

Improving Performance on Difficult Queries

The impact of these rater-trained models is significant, particularly in improving
Google's ability to handle complex and less common searches. Testimony from
Google’s Vice President of Search confirms:

     “RankEmbedBERT was again one of those very strong impact things, and it
     particularly helped with long-tail queries where language understanding is
     that much more important.”​
     View in PDF

Providing Foundational Data for Machine Learning

The document clarifies that rater scores are not just casual feedback but a
fundamental dataset for these AI systems:

     “The data underlying RankEmbed models is a combination of
     click-and-query data and scoring of web pages by human raters.”​
     View in PDF
                                                         27

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




This establishes human judgments as a core component used to teach the models
how to rank search results.


Deviation From Google's Past Statements

The finding that rater scores are a direct training input for a core ranking model like
RankEmbed clarifies and arguably deviates from the spirit of Google’s long-held
public statements.

Google’s Public Stance

For years, Google has consistently stated that quality rater scores do not directly
impact the ranking of any individual website. The company’s official guidance
describes the raters’ role as providing feedback to help “benchmark the quality of our
results” and “evaluate changes.” This has often been interpreted to mean their
influence is indirect - more like feedback that helps engineers tune the system
overall, rather than a direct ranking signal.

The Deviation Revealed in Court

The court document clarifies that this influence is far more direct and systemic than
previously understood. While a single rater’s score may not manually move a page up
or down, the aggregated scores are a foundational dataset used to build and train
an automated ranking system that is a core part of the algorithm:

     “The RankEmbed models trained on that data have directly contributed to
     the company’s quality edge over competitors.”​
     View in PDF

In essence, Google’s statements are technically correct in that no single rater score
directly changes a site’s ranking. But the court’s findings show a direct, systemic link
where the collective judgment of raters is used to train core AI ranking models.

This role is far more influential than the “feedback” or “benchmarking” role Google
has historically emphasised.



                                                          28

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




DOJ v. Google Disclosure: The Popularity Signal (P*)




     “Google’s Popularity signal (P*) ‘uses Chrome data’ to capture the
     popularity of websites. The signal is a measure quantifying the number of
     links between pages, as reflected in anchor text and user interactions, and
     it is used to promote well-linked documents.”

In the DOJ v. Google remedial opinion, the court revealed that Google’s Popularity
signal (P*) is one of the company’s fundamental top-level ranking signals,
alongside quality. While Google has long emphasised PageRank as its primary
authority signal, the opinion shows that Popularity (P*) draws on Chrome browsing
data and the number of anchors (link connections between pages) to measure
how “well-linked” and widely visited a page is. This detail was not available in the
original trial exhibits, which only contained a redacted line noting a “popularity
signal that uses Chrome data.” The fuller explanation - including anchors and the role
of promoting well-linked documents — only emerged later in the unredacted
remedial opinion. This makes P* a core mechanism by which Google evaluates and
elevates content in search results, and demonstrates how user browsing data feeds
directly into ranking.

In the DOJ v. Google remedial opinion, the court identified Quality as one of Google’s
fundamental top-level ranking signals, paired with Popularity. Quality is described
as being derived largely from the webpage itself, supplemented by inputs such as

                                                         29

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




PageRank (distance from known authoritative sources), spam scores, and human
rater evaluations. In the original trial exhibits, much of this section was blacked out
under “Highly Confidential Information” protections, leaving only partial
references to PageRank and generic mentions of a “quality score.”

The later unredacted remedial opinion confirmed that Quality incorporates signals
of authoritativeness, content-derived metrics, and rater scoring, and is directly
used in determining a page’s ranking. This disclosure highlights Quality as a systemic
scoring function — not just an abstract concept — and underscores its central role in
shaping search results.


What This Means

   ●​ Chrome Visit Data → The signal directly taps into browsing behaviour from
      Google Chrome, giving Google visibility into what users are visiting across the
      web.
   ●​ Anchor Link Structures → It measures how pages are connected by links,
      including anchor text relevance.
   ●​ User Interaction Data → Beyond static link graphs, it incorporates behavioural
      evidence of popularity from actual user navigation.
   ●​ Promotion of Well-Linked Documents → Pages with stronger link
      connectivity and demonstrated popularity are pushed higher in ranking.


SEO Relevance

This is a significant confirmation because:

   1.​ It ties Chrome browsing data directly into organic ranking (something
       Google has denied or downplayed publicly).
   2.​ It shows popularity is not just PageRank, but a broader signal combining
       Chrome data + anchor link structures + user behaviour.
   3.​ It acts as a boost mechanism for documents seen as “well-linked” both by
       hyperlink structure and user engagement.




                                                         30

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Topicality (T*) or The ABCs of Relevance
"Topicality" was addressed with surprising specificity in the trial.

Far from being an abstract concept, "Topicality" was revealed to be a formal,
engineered system within Google, designated as T*.

The explicit function of the T* system is to compute a document's fundamental,
query-dependent relevance.

It serves as a "base score" that answers the question: how relevant is this document
to the specific terms used in this search query?

Google uses topicality signals to judge how well a page’s content matches a user’s
query. This is essentially the relevance of the page’s topic and text to the search
terms:

   ●​ On-Page Content: The actual words on a webpage are the foundation of
      topical relevance. “The most basic and in some ways the most important
      signal is the words on the page and where they occur,” testified Pandu Nayak
      (Google’s Vice President of Search) regmedia.co.uk. He emphasized that the
      presence of query terms in the content - whether in the title, headings, meta
      tags, or body text - is “actually kind of crucial” for ranking regmedia.co.uk. In
      short, what the document “says about itself” is central to determining its
      topicality. Nayak noted that signals such as term frequency and position (e.g.
      title vs. body) are “very important” relevance cues regmedia.co.uk.​

   ●​ Anchor Text (Context from Links): Google also evaluates what other websites
      say about a page. Nayak testified that “another very important signal is the
      [hyper]links between pages,” known as anchor text, which provides “a very
      valuable clue in deciding what the target page is relevant to.” regmedia.co.uk In
      other words, if many pages link to a webpage using certain keywords, it signals
      the topic or context of that page. (For example, a page heavily cited with the
      anchor “JavaScript tutorial” is likely about JavaScript tutorials, boosting its
      topical relevance for that query.) Importantly, Google clarified that it does
      not mix user click data into its link analysis. When asked if click data
      influences the anchor signal, Dr. Kenneth “Ken” Lehman (a Google search
                                                          31

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   quality witness) explained: “To generate the anchor signal, that’s just from
   links between web pages, and it doesn’t involve clicks.” regmedia.co.uk
   Anchors are purely derived from the web’s linking behaviour, independent of
   user interactions.​

●​ User Interaction Signals: Internal evidence shows Google also monitors
   aggregate user behaviour to refine relevance. A Google “Three Pillars of
   Ranking” slide (from 2016) listed User-interactions (“what users say about the
   document”) as a third pillar alongside Body and Anchors. These interactions
   can include clicks, attention (hover/scroll), swipes, and whether users quickly
   return to search results. While Google has long maintained publicly that
   clicks are not a direct rank booster, trial documents indicate Google does
   use such data in a feedback loop to evaluate search quality. Indeed, as HJ
   Kim noted, Google historically tracked dwell time (length of time on a result
   before returning) as part of topicality scoring justice.gov. However, Google
   witnesses stressed that user data is used carefully - primarily to learn and
   adjust algorithms, not to blindly promote whatever gets the most clicks. (See
   the discussion under Authority about pitfalls of click metrics.) Q “How does
   that relate to the question of user data or user interaction data? A. So the chart
   is a little bit complex, but what it’s illustrating is one of the problems with using
   click data in connection with ranking search results. It’s a very strong
   observation that people tend to click on lower-quality, less-authoritative
   content than we would like to show on our search engine. Our goal is to show
   -- when someone issues a query, to give them information that’s relevant and
   from authoritative, reputable sources. People tend not to click on those so
   much. So if we’re guided too much by clicks, our results would be of a
   lower quality than we’re targeting.




                                                      32

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The T* score is composed of three core signals, collectively referred to as the
"ABC signals," which are themselves developed and tuned by engineers.

“ABC” Signals - Anchors, Body, Clicks: Hyung-Jin “HJ” Kim (a Google search
engineer) explained in a February 2025 DOJ interview (Trial Exhibit PXR0356) that
Google’s “ABC signals are the key components of topicality (or a base score)”, which
is Google’s determination of a document’s relevance to a query justice.gov.

●​ A - Anchors: This signal is derived from the anchor text of hyperlinks pointing
   from a source page to the target document. This confirms the enduring
   importance of descriptive, relevant anchor text as a powerful signal of what
   another page on the web believes a document is about, a direct legacy of the
   principles that underpinned Google's original PageRank algorithm.
●​ B - Body: This is the most traditional information retrieval signal, based on the
   presence and prominence of the query terms within the text content of the
   document itself.
●​ C - Clicks: This signal was one of the most significant confirmations of the trial. It
   is derived directly from user behaviour, specifically defined in testimony as how
   long a user dwelled on a clicked-upon page before navigating back to the search
   engine results page (SERP). The inclusion of a direct user engagement metric at
   this foundational level of relevance scoring underscores the centrality of user
   feedback to Google's core ranking logic.

Clicks vs. Quality
One revelation was Google’s caution against using click metrics as a proxy for quality.
An internal evaluation found that “a large number of clicks on a link does not
necessarily mean that the page is of high quality.”regmedia.co.uk
Dr. Lehman explained a known issue: “It’s a very strong observation that people tend
to click on lower-quality, less-authoritative content” disproportionately. In other
words, popular clicks can sometimes go to clickbait or less trustworthy pages.
“If we were guided too much by clicks, our results would be of a lower quality
than we’re targeting,”
Lehman warned (discussing an internal slide. Google’s ranking engineers therefore
treat user click data with skepticism when it comes to authority - they use it to refine
                                                         33

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




algorithms but do not simply promote pages because they’re popular.
In fact, Pandu Nayak noted that page quality tends to be “anti-correlated” with
pure click-through rates in some cases - improving the quality of results in tests
sometimes led to fewer clicks, as users might chase clickbait even when
higher-quality info is available. This reinforces why authority signals (like PageRank
and quality scores) are crucial to keep search results genuinely trustworthy.
It is clear - Google uses some types of click signals (like dwell time for Navboost/T*) to
refine relevance but avoids using raw click volume as a direct measure of authority or
quality, as it can be misleading (e.g., clickbait).

These three were “fundamental signals” combined into a topicality score (T★) to
judge relevance justice.gov and justice.gov.

Notably, Kim said even historical user behavior - e.g. “how long a user stayed at a
particular linked page before bouncing back to the SERP” - was used as a
topical relevance signal in the past justice.gov.

Google’s ranking engineers hand-crafted the formulas for these signals (rather than
relying purely on ML) so they could understand and adjust how each factor
contributes to relevance justice.gov.

These three signals are combined in what was described as a "relatively hand-crafted
way" to generate the final T* score.

The user engagement data that powers the Navboost re-ranking system also provides
the foundational 'Clicks' signal for the T* topicality score.

The development of this system was a major engineering undertaking, described as
being in a "constant state of development" from its inception until approximately five
years prior to the testimony, indicating its maturity and foundational status within the
ranking stack.




                                                          34

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Q* Metric - A New Understanding of Site-Level Quality
The trial also brought to light a previously secret internal metric known as Q*
(pronounced "Q-star"), which functions as a measure of a website's overall quality
and trustworthiness.

This revelation is significant because it apparently confirms the existence of a
site-level quality score, something Google representatives have publicly and
repeatedly avoided confirming (in these exact terms) for years.

According to trial exhibits, Q* is "an internal metric that assesses the trustworthiness
of a whole website (most often the domain)".

A crucial characteristic of Q* is that it is largely static and query-independent.

If a website earns a high Q* score, it is considered a high-quality, reliable source
across all related topics for which it might rank.

This explains why certain authoritative domains consistently appear in search results
for a wide range of queries.

Like the T* system, Q* is described as being "deliberately engineered rather than
machine-learned," reinforcing the theme of human oversight in Google's foundational
ranking systems.

   ●​ Quality Score (Q★ - Trustworthiness): Google assigns pages a general
      quality score (often called “Q-star” or Q* internally) that reflects their
      overall credibility and utility, independent of any specific query. HJ Kim
      noted “Q (page quality, i.e., the notion of trustworthiness) is incredibly
      important”* in ranking justice.gov. This quality score is largely static (does not
      fluctuate per query) and “largely related to the site rather than the query”
      justice.gov. Kim testified that “Quality score is hugely important even today.
      Page quality is something people complain about the most.” justice.gov He
      recounted that Google formed a Page Quality team ~17 years ago (which he
      led) when “content farms” flooded search results with low-quality pages
      justice.gov. In response, Google developed methods to identify authoritative
      sources and demote the content-farm pages, improving the overall

                                                          35

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      trustworthiness of top results justice.gov. In short, Google tries to
      consistently reward pages that demonstrate experience, expertise,
      authority, and trust (E-E-A-T), and that reputation persists across
      queries. The slide contains the following text: Quality • Generally static across
      multiple queries and not connected to a specific query. • However, in some
      cases Quality signal incorporates information from the query in addition
      to the static signal. For example, a site may have high quality but general
      information so a query interpreted as seeking very narrow/technical information
      may be used to direct to a quality site that is more technical.

A key input into the Q* score is a modern, evolved version of Google's original
breakthrough algorithm, PageRank.

Testimony revealed that PageRank is still an important signal, but its function is
now framed as measuring the "distance from a known good source".

The system uses a set of trusted "seed" sites for a given topic; pages that are closer
to these authoritative seeds in the web's link graph receive a stronger PageRank
score, which in turn contributes to a higher Q*.

The confirmation of a domain-level authority score like Q* stands in stark
contradiction to years of public communications from Google.

“QRank”, Quality Scores & Authority Signals (2010s)
“‘Even the most fascinating content, if tied to an anonymous profile, simply won’t be
seen because of its excessively low rank.’ Cited to Eric Schmidt ex-Google, 2014.

In response, Google developed internal Page Quality metrics - sometimes referenced
as “QScore” or “QRank” - to judge the overall authority, expertise, and
trustworthiness of a page or site.

Google’s Hyung-Jin Kim (VP of Search) described this as a “page quality (i.e., the
notion of trustworthiness)” score, often denoted internally as Q* (“Q-star”).

He noted in testimony that “Q is incredibly important”* and that Google formed a
dedicated “Page Quality” team ~17 years ago when low-quality content farms were
proliferating justice.gov.
                                                         36

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The idea behind Q* is to algorithmically assess factors like a site’s reputation,
authority, and compliance with quality guidelines, independent of any specific
query.

Kim explained that this quality signal is “generally static across multiple queries and
not connected to a specific query”, meaning if a site is deemed high-quality and
reliable, that status boosts its rankings for all relevant searches justice.gov.

(However, query context can be factored in at times - for example, even a generally
high-quality site might be outranked by a more expert site for a very niche technical
query justice.gov.)

Crucially, Google’s modern quality score integrates PageRank as one input.

Kim confirmed that “PageRank…is used as an input to the Quality score.” justice.gov
In other words, a page’s base PageRank (its link-based importance) contributes to its
overall “authority” score Q*, alongside other factors (possibly site reputation, expert
reviews, etc.).

The Quality score thus acts as an aggregate authority metric - sometimes called an
“authority score” - that can boost or dampen a page’s search rankings.

Pages with strong Q scores (earned via trusted backlinks, original content, good user
signals, etc.) are systematically favored.

This became especially important after Google’s 2011 “Panda” update, which targeted
shallow content. Kim alluded to this, noting the team was started to tackle content
farms that “paid students 50 cents per article”, flooding Google with thin pages
justice.gov.

The solution was to algorithmically identify “the authoritative source” for a
given topic and reward it justice.gov.

In effect, Google began demoting pages that had decent link popularity but poor
overall quality, and promoting those with true authority. Kim emphasized that “Quality
score is hugely important even today. Page quality is something people complain
about the most.” justice.gov

                                                          37

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Indeed, with the rise of generative AI content, Google’s reliance on such quality
signals has only grown (“nowadays, people still complain about [quality] and AI makes
it worse”, he noted justice.gov).

How Q works internally: Google treats the quality score as a mostly
query-independent ranking factor attached to pages or sites.

It is “largely static and largely related to the site rather than the query” justice.gov -
essentially a measure of a site’s authoritative strength.

At query time, this quality score is combined with the query-dependent relevance
score. While Google hasn’t publicly detailed the formula, one can think of the ranking
system as first evaluating relevance (does the page match the keywords/intents?) and
then adjusting results based on authority/quality.

A high Q* can significantly boost a page’s position, while a low-quality score
can sink an otherwise relevant page.

In practice, Google’s regular updates and ranking tweaks often boil down to
recalibrating this “authority” component.

Notably, many signals feed into Q*: PageRank and link signals (for authority),
content assessments (for expertise), TrustRank-like signals (for
trustworthiness), and even user engagement data.

For example, internal documents indicate Google also uses a “popularity signal that
uses Chrome data” (likely aggregated Chrome usage statistics) as well as click
feedback loops like NavBoost justice.govstradiji.com.

(NavBoost, described by Google’s Dr. Eric Lehman, is essentially a big table counting
how often users click on a result for a given query over the past year stradiji.com - a
way to boost pages that searchers consistently prefer).

These additional signals are beyond PageRank, but they complement the goal of Q*:
to measure overall quality and user satisfaction.

PageRank itself, once the star of Google’s algorithm, now works behind the scenes as
one signal feeding into these broader quality and ranking frameworks.
                                                          38

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Navboost System

The Confirmed Role of Large-Scale User Interaction Data
Perhaps the most impactful revelation from the trial was the detailed exposition of the
Navboost system. This system provides the crucial 'C' (Clicks) signal for the T* score
we discussed earlier,

For years, the search engine optimisation (SEO) community has debated the role of
user clicks in ranking, with Google's public statements often being evasive or
dismissive.

The trial testimony, particularly from Google VP Pandu Nayak, ended this debate.
Navboost was confirmed to be "one of the important signals" that Google uses
to refine and prioritize search results based on a massive, historical repository
of user interaction data.

The system operates on a vast time horizon, storing and analyzing 13 months of user
interaction data to inform its signals.
This extended timeframe allows it to look beyond short-term fluctuations and identify
persistent, long-term patterns of user satisfaction, effectively using the collective
wisdom of billions of past searches to guide future rankings.
Navboost's analysis is highly nuanced, moving beyond a simple click count to classify
different types of user interactions.




                                                         39

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Leaked documents and testimony point to several key click metrics:
 ●​ Good Clicks vs. Bad Clicks: The system distinguishes between positive and
    negative interactions. A "bad click" is probably a "pogo-stick" event, where a user
    clicks a result and then immediately returns to the SERP, signalling dissatisfaction.
    A "good click," conversely, indicates that the user's need was met.
 ●​ Last Longest Click: This metric appears to be of particular importance. It
    identifies the final result a user clicks on in a search session and dwells on for a
    significant period. This interaction is interpreted as the ultimate signal of a
    successfully completed search task, making the page that received the "last
    longest click" a highly valuable result for that query context.

To provide contextually relevant results, Navboost employs several sub-systems:
 ●​ Slicing: The system segments, or "slices," its vast repository of click data by
    critical contextual factors, most notably the user's geographic location and device
    type (e.g., mobile or desktop).15 This allows Navboost to prioritise results that
    have performed well for users in a similar situation, for example, boosting a local
    business's website for mobile users in a specific city.
 ●​ Glue: This is a related, more real-time system that works alongside Navboost. The
    "Glue" system specifically monitors user interactions with non-traditional SERP
    features like knowledge panels, image carousels, and featured snippets. By
    analyzing signals such as hovers and scrolls on these elements, Glue helps
    Google determine which features to display and how to rank them, especially for
    fresh or trending queries where historical click data may be sparse.15

The primary function of Navboost within the overall ranking pipeline is to act as a
powerful, user-behavior-driven filter.

According to Pandu Nayak's testimony, after an initial retrieval stage identifies a large
pool of potentially relevant documents, Navboost is used to dramatically reduce this
set from tens of thousands down to a few hundred.

This much smaller, higher-quality set of documents is then passed on to more
computationally expensive and nuanced machine learning systems for final ranking.

A key limitation acknowledged in the testimony is that Navboost can only influence the
ranking of documents that have already accumulated click data; it cannot help rank
                                                          40

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




brand-new pages or those in niches with very low search volume.


“Authoritative, Reliable” Results Priority: Google’s witnesses underscored that the
search engine deliberately prioritizes authoritative sources in rankings. Nayak
explained that Google’s “page quality signals” are “tremendously important” because
the goal is to “surface authoritative, reliable search results” for users regmedia.co.uk.


In the same vein, Dr. Lehman testified that “our goal is to show - when someone
issues a query - to give them information that’s relevant and from authoritative,
reputable sources.”.


This philosophy was echoed throughout the trial: Google wants trustworthy content
(e.g. official sites, established experts, high-quality publishers) to rank at the top,
rather than sketchy or unverified pages, even if the latter are more crudely optimised
for a keyword.

Key Takeaways


In essence, these three systems work in concert: T establishes relevance, Q assesses
trust, and Navboost refines the results based on user satisfaction.




                                                          41

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




PageRank




Original PageRank: Link-Based Importance (Late 1990s)

Google’s original PageRank algorithm, developed by Larry Page and Sergey Brin at
Stanford, assigns an importance score to each webpage based on the web’s link
structure.

The basic idea is that a page is considered more important if many other
important pages link to it.

As Google’s early patent (Lawrence Page, U.S. Patent 6,285,999) explains, “a
document should be important (regardless of its content) if it is highly cited by
other documents. Not all citations, however, are necessarily of equal significance.

A citation from an important document is more important than a citation from a
relatively unimportant document… [Thus] the rank of a document is a function of the
ranks of the documents which cite it.” patents.google.com
                                                         42

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




In practice, the PageRank of a page A is defined recursively:

r(A)=1−dN + d∑i=1nr(Bi)L(Bi), r(A) = \frac{1 - d}{N} \;+\; d \sum_{i=1}^{n}
\frac{r(B_i)}{L(B_i)} ,r(A)=N1−d​+d∑i=1n​L(Bi​)r(Bi​)​,

where B1…BnB_1 \ldots B_nB1​…Bn​are pages linking to A, L(Bi)L(B_i)L(Bi​) is the number
of outgoing links from page BiB_iBi​, N is the total number of pages, and d is a damping
factor (usually set around 0.85): patentimages.storage.googleapis.com and
snap.stanford.edu.

In other words, “the ranks form a probability distribution over web pages, so that the
sum of all Web pages’ PageRanks will be one,” and the rank of a page can be
interpreted as “the probability that a random web surfer ends up at the page after
following a large number of forward inks.”: patentimages.storage.googleapis.com

Because a random surfer occasionally jumps to a random page with probability (1–d),
even pages with few links can get some baseline rank.

This elegant link analysis makes PageRank an objective measure of a page’s citation
importance.

As Brin and Page noted in their 1998 research paper, “PageRank…corresponds well
with people’s subjective idea of importance. Because of this correspondence,
PageRank is an excellent way to prioritize the results of web keyword searches.”:
snap.stanford.edu

How it was used: In Google’s early search engine, PageRank was a core ranking
signal used to “prioritize” or weight search results. Google even had Google Toolbar
Updates back in the day.

Pages with higher PageRank (i.e. more or better-quality backlinks) tended to rank
higher in the “10 blue links” results, all else being equal.

PageRank was computed offline by iteratively propagating link weights, and Google
updated these scores periodically.

By the early 2000s, Google even exposed a rough 0–10 PageRank score via the
browser Toolbar, underscoring how central it was to ranking.
                                                          43

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Importantly, even from the start Google recognized that PageRank was one signal
among many - it improves relevance when combined with content-based scoring.

Nonetheless, it became the foundation of Google’s ranking, embodying the principle
that “links…are votes of support” and that pages “endorsed by many high-quality
sites” should be ranked as more authoritative.

PageRank (Link-Based Authority) - What SEOs said at the time

   ●​ Key Internal Details (Google): Google’s original ranking algorithm PageRank
      assigns each page a numerical importance score based on backlinks. In Larry
      Page’s formulation, a page’s rank is calculated from the ranks of pages linking
      to it hobo-web.co.uk. PageRank is query-independent - it condenses the entire
      web’s link graph into a “global ranking of all Web pages, regardless of content,
      based solely on backlinks” patents.google.com. Early on, Google noted that
      even low-quality pages contribute a minimum PageRank, so creating many
      interlinked dummy pages could artificially inflate a target page’s score
      patents.google.compatents.google.com. Google addressed link spam with later
      patent tweaks (e.g. weighting links from sites with many pages)
      patents.google.com, but PageRank remained a core baseline in the ranking
      system gofishdigital.com.​

   ●​ Observations (Bill Slawski, Jim Boykin) : Some SEOs recognised PageRank’s
      central role and pitfalls. As early as 2007 on an article I commented on, Jim
      Boykin discussed “old BackRub techniques with some TrustRank thrown in,”
      acknowledging the link-vote model behind ranking
      internetmarketingninjas.com. Bill Slawski frequently analyzed Google’s link
      algorithms, noting the vulnerability of PageRank to spam farms and reciprocal
      “endorsement” loops patents.google.com. He explained that many low-value
      links can still boost a page since “every linking page is guaranteed to have a
      minimum PageRank… links from many such low quality pages can still inflate the
      PageRank score.” patents.google.com Slawski also highlighted Google’s
      attempts to dampen manipulation, like the “reasonable surfer” model giving
      different weight to links patents.google.com. At the time, we advised that
      PageRank is essentially a measure of link-derived authority - “rank assigned to

                                                         44

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      a document is calculated from the ranks of documents citing it”
      hobo-web.co.uk - a point that aligned exactly with Google’s own definition.​

   ●​ Notable Quotes/Metaphors: Bill often described backlinks as votes or peer
      reviews. He quoted Google’s description that PageRank uses “information
      external to webpages - their backlinks - which provide a kind of peer review.
      Backlinks from ‘important’ pages are considered more significant… by
      recursive definition.”patents.google.com This metaphor of link votes
      anticipated Google’s internal thinking. Myself, in practical guides, emphasized
      that “Google has long worked [by displaying] organic results based on
      KEYWORDS and LINKS” hobo-web.co.uk - effectively telling SEOs that link
      authority (PageRank) still underpins rankings.​

   ●​ Accuracy in Hindsight: While we didn't know for sure at the time, our
      perspectives on PageRank were highly accurate. PageRank indeed proved to
      be the foundational ranking factor Google used, and their advice to acquire
      quality backlinks was prescient. Slawski’s early warnings about link spam mirror
      the tactics Google fought internally patents.google.com. Over time Google
      integrated many other signals, but as late as the DOJ trial (2023) it was
      confirmed that PageRank (or its derivatives) remains in use.

TrustRank: Incorporating Link Trust & Spam Protection (2004–2000s)

As the web grew, link spam (artificial link networks or “link farms”) began to
undermine PageRank’s reliability. In response, researchers (including some later
Googlers) developed TrustRank, an evolution of PageRank that emphasizes link
trustworthiness over raw link popularity.

A Google patent on link-spam detection defines TrustRank as “a link analysis
technique related to PageRank” and “a method for separating reputable, good pages
on the Web from web spam.” It works on the presumption that good (non-spam)
websites seldom link to spam sites patents.google.com.

TrustRank involves two steps: first, human experts identify a small seed set of highly
trustworthy pages; second, a score propagation algorithm spreads a “trust score”
outwards through the link graph.
                                                         45

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




As the patent explains, “TrustRank involves two steps, one of seed selection and
another of score propagation. [Thus] the TrustRank of a document is a measure of the
likelihood that the document is a reputable (i.e., non-spam) document.”
patents.google.com

Google implemented this concept internally to downweight webspam and promote
authoritative content.

Rather than counting all backlinks equally, links from a trusted seed page confer more
value.

In effect, this is like running a biased PageRank that starts from trusted nodes.

A later Google patent describes “select[ing] a few ‘trusted’ pages (also referred to as
seed pages) and [finding] other pages likely to be good by following the links from the
trusted pages.” patents.google.com

By crawling outward from a set of “high-quality seed pages” and measuring link
distance (hops or weighted path length) to other pages, Google can compute a trust
score for each page based on proximity to trusted sites.

Pages closely linked to the trusted seeds receive higher trust scores, while those deep
in the link graph or mainly linked from untrusted sources are deemed less reliable.

This distance ranking approach was patented by Google and reduces the influence
of spam farms: “good documents on the Web seldom link to spam” and thus spam
pages end up many link-hops away from the reputable core patents.google.com.

In practice, Google could use TrustRank to demote or filter pages with high PageRank
but low trust.

One Google filing notes that the system may compute a “discrepancy between the
link-based popularity (e.g. PageRank) and the trustworthiness (e.g. TrustRank) of a
given web document” to catch artificially boosted pages patents.google.com.

In essence, a page with many inbound links might still rank poorly if those links come
from low-trust sources.

                                                          46

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By the late 2000s, Google’s ranking algorithm quietly incorporated such link quality
assessments to complement raw link count, reinforcing the mantra that not all links
are equal.

Usage: TrustRank (and related “link distance” signals) are used internally as part of
Google’s ranking and anti-spam systems. Though Google did not publicly call it
“TrustRank” by name, Google engineers have affirmed the concept. For example, a
Google patent by the company’s researchers explicitly describes using “a seed set of
reputable documents” with trust values, then propagating those trust values to linked
pages.

This helps assign each page a trust score that can modify its ranking. In summary,
TrustRank evolved PageRank by adding a notion of link reliability, ensuring that a
page’s rank reflects not just quantity of links, but the quality and trustworthiness of
those link sources.

   ●​ Observations (Bill Slawski): Bill Slawski closely followed Google’s moves on
      trust. He noted that “Google Trustrank is very different from Yahoo TrustRank…
      Yahoo’s TrustRank [identifies] spam, whereas Google developed a system for
      reordering rankings of web pages” based on trust signals
      seonorth.caseonorth.ca. Years before Google’s trial revelations, Slawski
      discussed patents on using trusted seed sites to influence rank. He cited one
      Google patent wherein “the system…assigns lengths to links…computes the
      shortest distances from seed pages to each page…[and] determines a ranking
      score for each page based on the computed shortest distances.”
      gofishdigital.com In plainer terms, Bill explained that pages closer (in link hops)
      to authoritative sites would rank higher, capturing the essence of “TrustRank”
      as “distance from authority sites”.
   ●​ Slawski explicitly connected Google’s trust metrics to the “distance
      between documents.” He highlighted that Yahoo’s TrustRank “diminishes with
      increased distance between documents”, requiring carefully chosen seed sets
      seonorth.caseonorth.ca - a concept Google mirrored. In a 2019 analysis, Bill
      wrote that Google had a patent for ranking based on how “close or distant
      [pages] might be to a set of trusted seed sites” gofishdigital.com. This “seed
      set distance” metaphor was essentially Bill translating Google’s internal method
      into SEO-friendly terms. I often spoke of “authority” in a similar vein - often
                                                         47

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      referencing Bill (and Jim Boykin) - and recommending getting links from .gov or
      .edu sites and communities’ hubs because those confer trust (a notion very
      aligned with TrustRank).​

   ●​ Bill Slawski (RIP) effectively reverse-engineered Google’s thinking through
      patents, identifying that Google sought a “trust score” to combat low-quality
      results. This was confirmed when Google’s Pandu Nayak later revealed the
      addition of an explicit quality/trust metric around 2011 to address content farm
      issues hobo-web.co.uk. My own long-standing emphasis on site credibility,
      authoritative backlinks, and user trust anticipated Google’s E-A-T (Expertise,
      Authoritativeness, Trustworthiness) philosophy. In hindsight, this guidance
      some of us shared at the time to “be closer to trusted authorities” (both
      literally in link graphs and figuratively in reputation) was accurate enough. We
      (along with many others like Rand Fishkin at the time accurately
      predicted/documented that Google was integrating trust evaluations into
      ranking - something the DOJ trial exhibits and Google patents have since made
      evident.

PageRank’s Role Today

Even as Google’s algorithm has become vastly more complex, it still uses PageRank
internally in 2025 - but as one factor among hundreds, and usually mediated through
higher-level scores like Q*.

Google’s own public documents affirm this.

In a 2019 white paper on combating disinformation, Google noted that “the best
known of these signals is PageRank, which uses links on the web to understand
authoritativeness.” searchengineland.com

In other words, link-based authority (PageRank) remains a fundamental signal for
evaluating a page’s trust and expertise.

Google’s search engineers continue to value the “distance from a known good
source” that PageRank-style algorithms provide justice.gov.


                                                         48

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




However, they no longer rely on raw PageRank alone. As Google’s John Mueller
explained, modern ranking is “not just PageRank of course…there are lots of different
topics in there and PageRank is more or less a side comment.” searchengineland.com

PageRank has effectively been subsumed into composite metrics like quality score
and into specific applications (e.g. identifying authoritative seed sites, boosting
trusted domains, etc.).

In summary, PageRank’s evolution over two decades reflects Google’s shifting focus
from quantity of links to quality of content and trust.

The original PageRank algorithm (circa 1998) introduced the paradigm of ranking by
link popularity (with damping factor ~0.85 to model random surfing)
snap.stanford.edu.

TrustRank and related link-distance algorithms (mid-2000s) built on this by
prioritizing links from vetted “trusted” pages and demoting spam, under the principle
that “good pages seldom link to bad ones.”patents.google.com

And in the 2010s, “QRank” or page quality scores further blended PageRank with
numerous other signals to measure a page’s true authority and reliability, addressing
content quality issues beyond links.

Today, Google’s ranking uses a sophisticated mix of these factors:

PageRank is still there under the hood, informing the algorithm about the link-based
authority of pages justice.gov, but it operates in concert with semantic relevance
models, machine learning systems (like RankBrain and BERT-based RankEmbed
stradiji.com), user feedback metrics, and domain-level quality evaluations.

As a result, Google Search can surface results that are not only popular in the link
graph, but also trusted, expert, and satisfying - fulfilling the original goal of
PageRank (“bringing order to the web” by leveraging links snap.stanford.edu) while
adapting to the modern web’s challenges.




                                                         49

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Key Take-aways

PageRank & Link-Based Authority: One of the oldest authority signals is Google’s
famous PageRank algorithm, which treats links as “votes” of confidence.

Kim described PageRank as “a single signal relating to distance from a known good
source” - essentially measuring how far removed a webpage is from trusted,
reputable sites on the web justice.gov.

In the trial, he confirmed that Google “uses [PageRank] as an input to the Quality
score.” justice.gov In practice, a page linked by many high-authority sites will inherit
some authority itself.

This link-based authority is one component of the overall page quality/Q★
score. (For example, a university or government site linking to a page conveys a level
of trustworthiness to that page.) By feeding PageRank into the quality metric, Google
combines traditional link popularity with other quality assessments to rank
authoritative content higher.

“I asked Gary (Illyes from Google) about E-A-T. He said it’s largely based on links
and mentions on authoritative sites. i.e. if the Washington post mentions you, that’s
good. He recommended reading the sections in the QRG on E-A-T as it outlines things
well.” Marie Haynes, Pubcon 2018




                                                         50

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Panda and Google’s Site-Level Quality Score (QScore)

One of the revelations from the U.S. Department of Justice antitrust trial against
Google (2023) was confirmation of an internal metric often called “QScore” or “Q”* -
essentially the continuation of Panda’s site quality score concept.

In trial exhibits, a Google search engineer described Google’s ranking signals at a high
level, explicitly highlighting a Quality signal (Q) that is “generally static across
multiple queries and not connected to a specific query” justice.gov.

This quality score incorporates various factors to gauge a site’s trustworthiness and
authority.

The engineer stressed that “Q … is incredibly important”* and that competitors would
love to decode it justice.gov. In fact, he noted “Quality score is hugely important even
today. Page quality is something people complain about the most.”justice.gov -
underscoring that Google invests heavily in getting this right because low-quality
content undermines user trust in search results.

Crucially, the testimony linked QScore’s origin to the Panda era: “HJ [Hyung-Jin]
started the page quality team 17 years ago… around the time when the issue with
content farms appeared… Google had a huge problem with that. That’s why Google
started the team to figure out the authoritative source.”justice.gov. This places the
genesis of the quality score around 2008 or so, leading up to Panda’s launch in 2011
to fight content farms. It confirms that Panda was essentially the first implementation
of Google’s site-wide quality scoring. The QScore (often denoted internally as Q or
Q*) is the modern incarnation of Panda’s score, now deeply integrated into ranking.

Another insight from the trial is that the quality score can be “easily reverse
engineered” if one had access to enough data, because it is “largely static and
largely related to the site rather than the query.” justice.gov. This static nature is what
makes it powerful - it acts as a constant site reputation metric. But it also means if an
outsider could figure out what Google’s quality score for each site is (e.g., by
analyzing large amounts of search results across queries), they might infer
which sites Google algorithmically trusts the most.

Google guards this closely.
                                                          51

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The quality score (Q) isn’t just Panda’s content analysis now; it appears to be a
composite metric. The trial doc mentions that “PageRank… is used as an input to the
Quality score.”justice.gov. It also alludes to a “popularity signal that uses Chrome
data” justice.gov likely feeding into quality. In other words, QScore today probably
combines: content quality signals (Panda), link-based authority (PageRank), user
engagement in serps or traffic signals (Chrome/Android data, etc.), and perhaps other
trust signals (brand recognition, factual accuracy measures, etc.). Think of QScore as
Google’s internal measure of a site’s overall value to users. Panda was the
foundation of that score, focusing on content quality. Over time, Google has layered
on more inputs. But when SEOs talk about “Panda” or “site-wide quality,” they are
essentially talking about this QScore system.

It’s notable that even with modern AI advancements in search, Google still relies on a
concept of site quality.

The engineer in 2025 said, “Nowadays, people still complain about [content] quality
and AI makes it worse.” justice.gov - indicating Google’s quality algorithms
(Panda/QScore) are continually being challenged by new waves of low-effort content
(like AI-generated spam).

Yet the core principle remains: trustworthy, authoritative sites are algorithmically
scored higher; deceptive or low-value sites get scored lower.

Google’s ranking system then uses this as a significant factor. In the same exhibit, the
importance of authority is emphasized: if competitors learned Google’s quality scores,
“they have a notion of ‘authority’ for a given site” justice.gov, implying that QScore
correlates with a site’s perceived authority/trust.

So, Panda’s legacy is that “authority” is not just about links anymore, but about
content quality and user trust at the site level.

Impact and Legacy of Panda

The Panda update had immediate and far-reaching effects on the web.

Many well-known “content farm” style sites saw dramatic drops in visibility overnight
in 2011. For example, Demand Media’s eHow (which had tens of thousands of short
                                                         52

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




how-to articles) was reportedly hit, as were sites like Suite101 and Mahalo - to the
point that “the web [was] still buzzing about its implications” weeks after, noted Wired
in March 2011 wired.com.

Conversely, Panda benefited “established sites known for high-quality information”
wired.com - e.g., mainstream news outlets, government sites, medical journals, etc.,
saw their rankings improve relative to lower-quality competitors.

One anecdote shared by Cutts: Before Panda, someone searching about a medical
condition found “content farms were ranking above government sites” for that query.
After Panda, “the government sites are ranking higher,” which was a desired outcome
wired.com.

This highlights how Panda shifted the balance towards authority and reliability.

There were some unintended casualties as well. Some sites with mostly good content
but a few weak spots got caught in the dragnet.

For instance, affiliate websites with thin product pages, forums or Q&A sites with
lots of user-generated content (some of which might be low quality), or small
businesses with mostly great pages but a few duplicate pages - some of these felt
Panda’s sting.

Google’s advice to them was consistent: improve the overall quality of your site (or
remove the bad parts) and you can gradually recover as the algorithm reassesses you
developers.google.com.

Over time, many such sites did recover by following quality best practices.

From an industry perspective, Panda was a wake-up call. It put an end to the era of
spammy SEO tricks like mass-producing keyword-stuffed pages or scraping content
from other sites to get easy traffic.

It pushed publishers to focus on content excellence and user experience. It also
spawned the concept of “Panda-proof” content strategies, emphasizing depth,
originality, and user trust.


                                                         53

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




By integrating Panda into the core ranking system, Google essentially made quality a
permanent, always-on ranking factor.

Today, whenever Google rolls out a “core update” (as it does several times a year),
sites that see gains or losses often are feeling the effect of tweaks to these quality
evaluations (among other things).

Google itself has said core updates “may cause some sites to notice drops or gains”
and that “there’s nothing wrong with pages that may now perform less well… Instead,
it’s that changes to our systems are benefiting pages that were previously
under-rewarded” - often referring to quality improvements in the algo.

This is very much in line with Panda’s original mission.

In summary, Panda’s legacy is the notion that “overall site quality” matters
tremendously for SEO, not just the relevance of a single page or the number of links.

It ushered in an era where Google is far better at weeding out thin content and
boosting authoritative sources, and it laid the groundwork for future improvements in
evaluating content quality at scale.


Google’s Panda Algorithm - Site-Level Quality Scoring
Google’s Panda algorithm was a major search ranking system introduced in early 2011
with the goal of dramatically improving search result quality.

It was launched to reduce rankings for “low-quality sites” - pages that are
“low-value add for users, copy content from other websites or… just not very useful” -
while rewarding high-quality sites with original, in-depth content
googleblog.blogspot.com.

The initial Panda update impacted nearly 12% of all Google queries
googleblog.blogspot.com, making it one of the most significant algorithmic changes
in Google’s history.




                                                          54

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Internally, Google engineers actually nicknamed the project “Panda” after one of the
key engineers (Navneet Panda) who developed a breakthrough technique for
evaluating site quality wired.com.

This was brought to all our attention at the time, again by Bill Slawski.

Panda fundamentally changed how Google assesses website content quality and
introduced a new site-wide quality score into the ranking process, complementing
traditional signals like PageRank.

Origins and Purpose of Panda

By 2010, Google’s search team was facing widespread criticism that “content farms” -
sites churning out large volumes of shallow, low-value content - were dominating
search results at the expense of higher-quality sites wired.com.

Google’s own Amit Singhal (then head of Search Quality) described how after the
2009 Caffeine indexing update, Google’s fresher and bigger index began to surface a
new class of problem: “The problem had shifted from random gibberish, which the
spam team had… taken care of, into… written prose, but the content was shallow”
wired.com.

As Google’s spam chief Matt Cutts put it, content farms were essentially looking for
“what’s the bare minimum that I can do that’s not quite spam?”, slipping through the
cracks of earlier spam filters wired.com.

These sites weren’t outright violating old rules, but produced thin content that
frustrated users.

In early 2011, Google assembled a team (led by Singhal and Cutts) to tackle this gap in
quality.

     “We’ve been tackling these issues for more than a year… working on this
     specific change for the past few months.” googleblog.blogspot.com

Google’s solution was the Panda algorithm update (initially nicknamed “Farmer”
externally, until Google revealed the internal name Panda).

                                                          55

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Panda’s original purpose was to algorithmically assess website quality and
down-rank sites with thin or low-quality content, especially content farms -
googleblog.blogspot.com and wired.com.

“This update is designed to reduce rankings for low-quality sites… At the same time, it
will provide better rankings for high-quality sites - sites with original content and
information such as research, in-depth reports, thoughtful analysis and so on.”
googleblog.blogspot.com explained Google’s official blog when Panda first launched.

In other words, Panda introduced a site-level quality classifier into Google’s ranking
algorithms - something very different from earlier ranking systems that had mostly
focused on individual page relevance and link-based authority signals.

What Panda introduced was new: Prior to Panda, Google’s ranking relied heavily on
signals like PageRank (link popularity), topical relevance to the query, and a variety of
spam filters for blatant abuses.

There was no robust mechanism to judge the overall quality of content on a site.

Panda changed that by introducing a sort of “content quality score” at the site level.

This meant that if a site had a lot of low-quality pages, the entire site’s rankings
could be demoted - a sharp departure from the earlier page-by-page focus.

Google explicitly acknowledged this shift: “Our site quality algorithms are aimed at…
reducing the rankings of low-quality content.

The recent ‘Panda’ change tackles the difficult task of algorithmically assessing
website quality.” developers.google.com and developers.google.com.

In a Q&A, Matt Cutts confirmed that Panda was developed to catch what earlier
algorithms missed: “It sort of fell between our respective groups [the search quality
team and the spam team]. And then we decided, okay, we’ve got to come together
and figure out how to address this.” wired.com.

Notably, Google has said Panda was initially aimed squarely at content farms and
similar low-quality sites.

                                                          56

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




In a DOJ antitrust trial exhibit, a Google engineer (HJ Kim) reflected that Panda’s
beginnings coincided with “the time when the issue with content farms appeared.
Content farms paid students 50 cents per article… Google had a huge problem with
that. That’s why Google started the [page] quality team to figure out the authoritative
source.” justice.gov and justice.gov.

Panda was the result of that effort. Over time, its scope expanded beyond just
“content farms” to any site with poor-quality content.

But its core purpose remained: ensure that useful, trustworthy, and authoritative
websites rank above those with thin or unsatisfying content.




                                                         57

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Assessing Site Quality

Panda works by assigning a quality score to an entire site (or a large section of a
site), and using that score as a ranking factor.

Unlike keyword relevance or link-based metrics, this is a broader measure of how
beneficial and trustworthy a site’s content is to users.

According to a Google patent (co-invented by Navneet Panda) on “predicting site
quality,” Google’s system can “determine a score for a site… that represents a
measure of quality for the site”, and this “site quality score for a site can be used as
a signal to rank… search results… found on one site relative to… another site.”
patents.google.com and patents.google.com.

In other words, Panda’s output is essentially a site-wide quality score (like QScore or
*Q internally) that can boost or dampen all pages from that site in search rankings.

Training the Quality Classifier: To build Panda, Google took a very data-driven,
“scientific” approach. Amit Singhal explained that Google first defined what “high
quality” vs “low quality” means by using human quality raters. “We used our standard
evaluation system… we sent out documents to outside testers (raters). Then we asked
the raters questions like: ‘Would you be comfortable giving this site your credit card?
Would you be comfortable giving medicine prescribed by this site to your kids?’”
wired.com. Google’s engineers compiled a rigorous list of questions to probe a site’s
credibility and value.

According to Matt Cutts, “There was an engineer who came up with a rigorous set of
questions, everything from ‘Do you consider this site to be authoritative? Would it be
okay if this was in a magazine? Does this site have excessive ads?’” wired.com. These
and similar questions (which Google later shared publicly as guidance) cover things
like: Is the content written by an expert? Is it original, insightful, and more than just
superficial? Does the site have duplicate or overlapping pages on the same topics? Is
the content free of stylistic or factual errors? Would you trust this site for your money
or your life? Would you expect to see this in a reputable publication? Are there too
many ads? Is the content short or lacking in substance? developers.google.com and


                                                          58

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




developers.google.com. By collecting many such ratings, Google essentially built a
dataset of websites labeled “high quality” or “low quality” based on human judgment.

Next, machine learning was applied to this data. As Cutts described, “we actually
came up with a classifier to say, okay, IRS or Wikipedia or New York Times is over on
this side, and the low-quality sites are over on this side” wired.com. In simple terms,
Google extracted a variety of measurable features from websites (page content,
patterns of word usage, duplication, user engagement signals, etc.) and trained a
classifier that could predict a site’s quality rating.

Singhal gave a metaphor: “You can imagine in a hyperspace a bunch of points, some
points are red [low quality], some points are green [high quality]… Your job is to find a
plane which says that most things on this side… are red, and most… on that side… are
green.” wired.com. This is essentially how the Panda algorithm works internally - it
uses a machine-learned model to separate good vs. bad, based on many input
signals.

One specific approach revealed in Google’s patent is a phrase-based site quality
model.

The patent describes generating a “phrase model” that looks at the relative frequency
of various n-grams (word sequences) on a site patents.google.com and
patents.google.com.

Certain phrases or patterns of phrasing tend to correlate with higher or lower quality
content. (For example, one could imagine “how to make money fast” appearing
frequently on low-quality sites, whereas “references” or “methodology” might
correlate with higher-quality, research-oriented sites - this is a hypothetical
illustration.)

The system uses a large set of “previously scored” sites (from the human ratings) to
learn the phrase frequency characteristics of good vs bad sites patents.google.com
and patents.google.com.

Then for any new site, Google can compute a predicted quality score by analyzing its
content in terms of these phrase-based features patents.google.com and
patents.google.com.
                                                          59

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Importantly, this process is fully automated: “Site quality scores representing a
measure of quality for sites… can be computed fully automatically”
patents.google.com and then used by Google’s ranking engine as an input
patents.google.com.

Panda does not require manual intervention once the model is in place; it continuously
evaluates sites as Google crawls and indexes their content.

What signals does Panda specifically use? Google has never published the exact
formula (to prevent gaming the system), but the guiding questions and patents give
strong clues.

Content depth and originality are clearly important - sites with “shallow” or
“short, unsubstantial” content are flagged as low quality developers.google.com.

Duplication or mass-produced content is a negative signal - e.g. “duplicate,
overlapping, or redundant articles on the same topics with slightly different keywords”
hurts quality developers.google.com.

Trust and authority signals matter - if experts or authoritative sources write the
content, that’s positive developers.google.com.

If the site is recognised as a go-to authority in its field (or would be cited in print),
that’s a plus developers.google.com.

User experience factors like excessive advertising, poor layout, or lots of distracting
elements can indicate low quality developers.google.com.

Basic writing quality - correct grammar, no blatant factual errors - also feeds into
perceived quality developers.google.com.

Panda likely also considers engagement metrics indirectly (Google has hinted that it
did not directly use Chrome toolbar or Analytics bounce rates for Panda, but it’s
plausible that sites users tend to block or avoid correlate with Panda scores - indeed
Google found an 84% overlap between sites that users most frequently manually
blocked via a Chrome extension and the sites Panda flagged as low quality
wired.com).

                                                          60

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Crucially, Panda’s quality score is applied site-wide (or section-wide). This means if
a significant portion of your site’s pages are low quality, the entire site can be
demoted in Google results.

Google warned that “low-quality content on some parts of a website can impact the
whole site’s rankings” developers.google.com.

In practice, Panda acts as a sort of penalty (or dampener) on an entire domain if the
overall quality is judged to be poor.

Conversely, high quality sites get a boost across all their pages. This site-level
approach was new - earlier algorithms mostly evaluated pages individually.

A Google engineer in the antitrust trial described this quality signal (internally called
QScore or Q): “Q (page quality, i.e. the notion of trustworthiness) is incredibly
important… Q is largely static and largely related to the site rather than the query.”*
justice.gov and justice.gov. “Static” here means the quality score doesn’t change
based on each query; it’s an overall property of the site. So if Panda deems a site
low-quality, that site will tend to rank lower on all queries, no matter the topic, until the
quality improves. This was a significant change that incentivized webmasters to
improve the entirety of their site’s content, not just individual pages.

It’s worth noting that Google’s PageRank (link popularity) was even folded into this
quality scoring mechanism. The trial documents reveal that “PageRank… is used as an
input to the Quality score.”justice.gov In other words, Google’s site quality classifier
doesn’t ignore links - a site widely cited on the web (high PR) likely gets some benefit
in the quality score as well, perhaps as a proxy for authority. And Google likely uses
many other signals (possibly user satisfaction metrics, brand mentions, etc.) in the
quality score beyond just the content analysis that Panda started with. Panda was the
pioneering system for this kind of site-level evaluation, and over time Google has
continued to refine it into a broader “quality” framework.




                                                          61

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Evolution of the Panda Algorithm

After its initial launch in February 2011 (sometimes referred to as Panda 1.0), the
Panda algorithm went through numerous iterations and improvements over the years.

In the beginning, Panda updates were run periodically as “data refreshes” or new
versions that Google would announce every so often (monthly or bi-monthly in
2011-2012).

Notable milestones in Panda’s evolution include:

   ●​ Panda 2.0 (April 2011) - This update extended Panda’s impact beyond the U.S.
      and also started incorporating new signals, including user feedback signals.
      Google said at the time: “We’ve rolled out this improvement globally to all
      English-language Google users, and we’ve also incorporated new user
      feedback signals… In some high-confidence situations, we are beginning to
      incorporate data about the sites that users block into our algorithms.”
      developers.google.comdevelopers.google.com. This showed that Google was
      fine-tuning Panda by using real user behavior (perhaps like the Chrome
      blocklist data) to validate and adjust the algorithm. Panda 2.0 also “goes
      deeper into the ‘long tail’ of low-quality websites” to catch poorer results that
      the first version might have missed developers.google.com. The impact of
      these tweaks was smaller (~2% of queries affected, vs ~12% for the original)
      developers.google.com.​

   ●​ Ongoing Panda Updates (2011–2013) - Google continued to release Panda
      updates, numbered sequentially (Panda 3.0, 3.1, … etc.), improving the classifier
      and refreshing the data. Many of these were minor adjustments. Google
      sometimes quietly rolled them out; webmasters would notice ranking
      turbulence, and Google would later confirm a Panda update happened. The
      goal remained the same: refine the quality signals to more precisely demote
      only the truly “low-value” sites and let genuine quality sites rise. For example, a
      Panda update in 2012 targeted scraper sites (sites that plagiarize content)
      more effectively. By 2013, there were over two dozen Panda iterations.​



                                                         62

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




●​ Major Panda 4.0 (May 2014) - This was a significant update to Panda’s
   algorithm. Google’s Pierre Far described it as “a new Panda update” that
   incorporated some new signals and was supposed to be gentler, allowing some
   previously penalized sites to escape if they had improved. He mentioned it
   “added a few more signals to help Panda identify low-quality content more
   precisely” sitecenter.com. Panda 4.0 impacted roughly ~7.5% of English queries
   (per Search Engine Land reports) - still a big change. Notably, some large sites
   were hit hard or saw gains. For instance, eBay famously lost rankings in this
   timeframe (likely due to thin content on many eBay pages), while sites with
   robust content saw improvements sitecenter.comsitecenter.com.​

●​ Panda 4.2 (July 2015) - Google announced what turned out to be one of the
   last discrete Panda updates. Uniquely, Panda 4.2 was a very slow, gradual
   rollout, taking months to fully propagate. It affected an estimated 2–3% of
   queries sitecenter.com. Google hinted that this slow rollout was to minimise
   shock and perhaps to integrate Panda more deeply into the “core” ranking
   system.​

●​ Integration into Core Algorithm (January 2016) - At the start of 2016,
   Google confirmed that Panda had been incorporated into Google’s core
   ranking algorithm. This means Panda was no longer a separate filter run
   occasionally; it became part of the main ranking pipeline, evaluating sites
   continuously. Practically, this implied that Panda’s quality scoring would be
   updated in real-time (or near real-time) as Google crawls the web, rather than
   in big waves. “In January 2016, Google integrated Panda updates into its ‘core’
   algorithm, signaling that changes in the way they prevent poor-quality websites
   from ranking would now happen on an incremental, ongoing basis,” rather than
   sudden large updates sitecenter.com.​
   ​
   However, “core integration” did not mean Panda started to act instantly on
   every new piece of content. Gary Illyes of Google clarified that Panda in core
   still isn’t purely real-time in the way something like indexing is. It is more that
   Panda’s data gets refreshed more continuously, but there may still be some
   delay as the system accumulates enough data about a site. Still, from 2016
   onward, Google stopped announcing Panda hits or recoveries - it’s always
                                                      63

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   running in the background.​

●​ Post-2016 and Modern Updates - After Panda became part of the core
   algorithm, Google shifted to talking about broader “core updates” which can
   encompass multiple factors (including quality). Panda as a standalone name
   faded from public discussion, but its concept lives on strongly in Google’s
   approach to search. In fact, internal testimony in 2023–2024 (DOJ v. Google
   trial) makes clear that site quality scoring is still a crucial part of Google’s
   ranking formula. One Google search engineer noted in 2023 that “Quality score
   is hugely important even today. Page quality is something people complain
   about the most.” justice.gov and that Google continuously works on it
   (especially as new problems like AI-generated spam arise justice.gov). In the
   years since Panda’s integration, Google also introduced other quality-related
   algorithms - for example, the “Medic” update (August 2018) which seemed
   to emphasize E-A-T (Expertise, Authority, Trustworthiness) on “Your Money or
   Your Life” sites, and the Helpful Content Update (2022) which targets
   unhelpful, low-value content. These can be seen as spiritual successors to
   Panda, targeting content quality issues in more modern contexts. But it’s likely
   that much of the original Panda philosophy (and perhaps even code) underpins
   these systems, all contributing to that overall quality score (QScore) for sites.​

●​ Continuous Improvements - Google has repeatedly stated that it keeps
   refining its quality algorithms. “We will continue testing and refining the
   change… as we have more to share,” wrote Singhal during Panda’s rollout
   developers.google.com. This includes adjusting the weighting of the quality
   score, tuning what features the classifier pays attention to, and making it
   harder to game. Google also uses core updates to address edge cases or false
   positives. For instance, some sites that were unfairly hit by Panda (because
   they had a few thin pages dragging down an otherwise decent site) might
   recover in subsequent updates as the algorithm improved. By integrating Panda
   into core, Google essentially made quality assessment a permanent,
   ever-evolving part of search ranking.​




                                                      64

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




One important aspect of Panda’s evolution is how Google handles manual
exceptions or overrides. Google has been adamant that Panda (and its successors)
are purely algorithmic.

In the early Panda days, Google allowed webmasters to submit a reconsideration
request if they thought they were hit unfairly, but Google would use that feedback
only to improve the algorithm, not to manually boost individual sites. “We aren’t
making any manual exceptions [for Panda], we will consider [feedback] as we
continue to refine our algorithms,” Google said developers.google.com.

This has largely remained true - recovery from Panda comes from fixing your site, not
from appealing to Google.




                                                         65

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Deconstructing Site Quality via Exploit
Mark Williams Cook, of Candour did some exceptional work in this area and I was lucky
enough to chat with him about it at the time - “The endpoint exploit we found
literally had a metric called "site_quality" which at minimum determined if you
got some kind of rich results”.

Insights from a fascinating talk on "Conceptual Models of SEO" reveal a deeper, more
nuanced layer to how Google evaluates websites, moving far beyond traditional
metrics like keyword density or backlink counts.

Based on data allegedly retrieved from a Google exploit, the speaker outlines a
compelling case for a master metric: a "Site Quality Score."

This score appears to function as a foundational assessment of a site's authority,
directly impacting its ranking potential and eligibility for prominent search features.

A Foundational Ranking Gate

The core of the discovery is a "Site Quality Score" that Google allegedly
calculates for every single website, scored on a scale from 0 to 1 at the
subdomain level.

This isn't just another data point; it acts as a critical qualifier.

The speaker revealed a specific threshold: sites with a quality score below 0.4 were
found to be ineligible for Rich Results like Featured Snippets or "People Also Ask"
boxes.

This implies that no amount of on-page optimization for these features will succeed if
a site hasn't first passed this fundamental quality check.

It's a heat race you must qualify for before you can even compete.




                                                          66

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Measuring Trust: How Site Quality is Calculated

So, what constitutes this all-important score? According to Mark, who likes to
reference Google patents, like myself, the calculation depends on whether Google has
sufficient user data for a site.

   1.​ For Established Sites: The score is calculated based on signals that measure a
       site's real-world brand authority. Google looks at how many times users
       specifically search for your brand or domain name, how often they select
       your site in the search results even when it isn't ranked number one, and
       how often your brand name appears in anchor text across the web. In
       essence, Google is measuring your reputation and the trust users place in you.
   2.​ For New or Obscure Sites: When user data is scarce, Google uses a predictive
       model. It analyzes the content on your pages to create a "phrase model"—a
       numerical representation or "shape" of your website. It then compares this
       profile to the profiles of sites for which it already has established quality
       scores. It predicts how good your site is likely to be based on its resemblance
       to known, trusted entities.

The AI Dilemma and the Helpful Content Correction

This predictive model had a significant vulnerability, which the speaker argues led to a
massive influx of low-quality, AI-generated content ranking highly in 2022.

Because Large Language Models (LLMs) are trained on vast amounts of high-quality
text, the content they produce naturally mimics the "numerical shape" of a good site,
effectively tricking the predictive site quality model. Brand new sites could publish
thousands of AI-generated pages and be initially judged as high-quality, leading to a
surge in traffic.

Google's fix, according to this model, was the Helpful Content Update.

This update was a system-wide correction designed to close the loophole.

It heavily penalised sites that exhibited high traditional authority metrics (like a large
backlink profile) but had very low brand authority signals.

                                                          67

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The update was a clear signal that Google was doubling down on genuine, established
authority, making it immensely difficult for unknown sites to rank for competitive
topics, regardless of their content's superficial quality.

Mark reinforces this with a quote from former Google CEO Eric Schmidt I've used in
many SEO books now…: "Brands are the solution, not the problem. Brands are how
you sort out the cesspool."

Other Key Concepts from Mark’s presentation:

   ●​ Query Intent Classifiers: Google attaches labels to queries. The talk revealed
      classifiers like isDebunkingQuery (e.g., "is the earth flat"),
      medicalClassifier, and newsScore. The type of query dictates the type of
      results Google wants to show.
   ●​ The Eight Semantic Classes: The talk unveiled eight "Refined Query Semantic
      Classes" that seem to cover almost all queries, with the largest being "short
      fact or bool" (a question with a yes/no or simple factual answer). This is a
      critical insight, Mark predicts these are the queries most likely to be lost to AI
      Overviews - and I agree 100%.
   ●​ Content and Consensus: Google was found to generate a "consensus score"
      by counting the number of passages in content that agree with, contradict, or
      are neutral to the prevailing view. For a debunking query, only high-consensus
      content will rank. For a political query, Google may intentionally seek a mix of
      consensus and non-consensus results to provide balance. This means your
      content might be perfect, but if it doesn't fit the specific "recipe" of results
      Google wants for that query type, it won't rank.




                                                         68

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Conclusion: Brand is the New Bottom Line

The ultimate takeaway from the video is that in the modern SEO landscape, "site
quality" is largely synonymous with "brand authority."

The days of outranking authoritative sites with clever technical SEO alone are
dwindling.

The provided example of a rehabilitation-focused client outranking the NHS and other
established entities - despite having a fraction of the backlinks - illustrates this
perfectly.

Their success was attributed to a higher site quality score, earned through signals
that proved their authority and trustworthiness in a critical "Your Money Your Life"
(YMYL) category.

Ultimately, building a high-quality site in Google's eyes means building a real brand
that users seek out, trust, and mention.

This conceptual model suggests that long-term SEO success is now intrinsically linked
to genuine brand-building efforts that resonate with real people, not just algorithms.

For myself Mark’s work brings a long journey to a satisfying conclusion in this area.




                                                         69

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Practical Steps for Webmasters to Improve Site Quality
QUOTE: “One piece of advice I tend to give people is to aim for a niche within your
niche where you can be the best by a long stretch. Find something where people
explicitly seek YOU out, not just “cheap X” (where even if you rank, chances are
they’ll click around to other sites anyway).” John Mueller, Google 2018

Google’s quality scoring can seem intimidating, but its principles actually align with
common-sense best practices for building a great website.

Google has given extensive guidance - both in 2011 and in recent years - on how
webmasters can ensure their sites are seen as high-quality.

Here are practical, evidence-based steps to take:

   1.​ Audit Your Content and Remove or Improve Low-Quality Pages: Start by
      identifying pages on your site that are “thin”, redundant, or not useful to
      users. This includes very short articles with little info, copied content, duplicate
      or near-duplicate pages (e.g. same content targeting different keywords),
      auto-generated content, or pages that simply aggregate info from elsewhere
      without adding value. Google warns that “low-quality content on some parts of
      a website can impact the whole site’s rankings”developers.google.com. To
      avoid that site-wide Panda drag, either remove these pages, merge or expand
      them into more substantial resources, or “noindex” them if they must exist (so
      Google doesn’t count them against your site). Be careful: In the past, some site
      owners deleted hundreds of pages and saw no immediate improvement -
      Google’s Gary Illyes has noted that simply removing content isn’t a guaranteed
      fix seroundtable.com and seroundtable.com. If the content has any value, it
      may be better to “thicken” it - “Thin content: make it better, make it… thick,
      and add more high [quality] stuff,” Illyes advised twitter.com. In short, prune the
      truly junk content, but for borderline pages, beef them up rather than tossing
      them out. Over time, having a cleaner, richer content profile will lift your site’s
      quality score.​
      ​



                                                          70

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




2.​ Focus on E-A-T - Expertise, Authoritativeness, Trustworthiness: Panda is
   effectively a machine approximation of these traits. Ensure that qualified
   experts or enthusiasts write your content, and demonstrate their credentials.
   For example, include author bios that highlight expertise for YMYL (Your Money
   or Your Life) topics like health or finance. Make sure your content is factually
   accurate and cite trustworthy sources. Eliminate obvious errors - Google’s
   guidelines ask “Does this article have spelling, stylistic, or factual errors?”
   developers.google.com. Build authority by covering topics in-depth and
   becoming a go-to source in your niche. If your site is a recognized brand or
   cited by others, that boosts quality signals. As Google’s questions ask: “Is the
   site a recognized authority on its topic? Would you trust this site for a medical
   query? Would you be comfortable giving your credit card here?”
   developers.google.com and developers.google.com. Strive to earn “yes”
   answers to those questions. This might involve publishing original research,
   getting expert reviews, or simply demonstrating deep knowledge and
   transparency. In practice, improving E-A-T might mean adding author bylines
   and credentials, listing your physical business address and customer service
   info (for trust), getting mentions or links from other authoritative sites, and so
   on. These all contribute to how the algorithm perceives your site’s
   trustworthiness.​

3.​ Provide Substantial, Valuable Content (Avoid “Shallow” Text): Every page
   on your site should have a clear purpose and deliver value that stands out from
   the competition. Before publishing a page, ask yourself: “Does this page
   provide substantial value compared to other pages in search results?”
   developers.google.com and developers.google.com. If it’s basically a rehash of
   a Wikipedia article or a generic template with a few keywords swapped, it’s at
   risk. Panda loves **“original content, original research, original analysis”*
   developers.google.com. So put in the effort to make your content unique. This
   could mean including real-case studies, insightful commentary, data
   visualization, images or videos you created, etc. Longer content is not
   automatically better, but in many cases you should aim for comprehensiveness.
   A 300-word stub article on a broad topic is likely “thin”; a 2000-word
   well-structured article with facts, examples, and insights is more likely to be
   seen as high quality (assuming it’s well-written). Remember, one of Panda’s key
                                                      71

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   questions: “Does this article provide a complete or comprehensive description
   of the topic? Is this the sort of page you’d want to bookmark, share with a
   friend, or recommend?” developers.google.com and developers.google.com. If
   you can honestly answer yes, you’re on the right track.​

4.​ Improve User Experience - Usability, Ads, and Engagement: Panda doesn’t
   just look at what you publish, but also how you present it. A site overloaded
   with ads, especially above the fold, or with pop-ups that frustrate users, sends
   a negative quality signal (Google had a related “Top Heavy” algorithm for too
   many top-of-page ads, which aligns with Panda). Ask: “Does this site have an
   excessive amount of ads that distract from or interfere with the main content?”
   developers.google.com. Make sure the answer is no. Keep your page layout
   clean and reader-friendly - well below 30% ad density, in my opinion - based
   on Better Initial Ads Standards. Ensure fast page load times. Organize content
   with clear headings and avoid intrusive interstitials. Another subtle factor:
   navigation and site architecture. High-quality sites make it easy for users to
   find what they need; low-quality sites might be a maze of “content farm” links.
   Also, engagement metrics (though not explicitly confirmed as Panda signals)
   often correlate with quality. If users frequently bounce back to Google
   quickly from your page, that’s a sign something isn’t satisfying. While
   Google says Panda didn’t directly use “bounce rate”, it did use human
   judgments that correlate with it. So aim to keep users engaged: use images,
   break up text, make content scannable, and answer the query intent
   thoroughly. All these improvements not only please Panda but also your human
   visitors (which is the ultimate goal).​

5.​ Avoid “Gaming” the System - Don’t Chase Algorithm Loopholes: Panda
   (and its successors) are explicitly designed to catch sites that try to cheat their
   way to higher rankings without genuinely earning it. If you find a tactic that
   seems to boost rankings but doesn’t actually improve user experience, be wary
   - it might work short-term, but Panda will likely catch up. As Google’s Gary
   Illyes warned, if Google figures out that a site is successfully “gaming our
   systems” they will “push the site back just to make sure that it’s not working
   anymore.” sitecenter.com. In practice, this means don’t spam keywords, don’t
   auto-generate a thousand doorway pages, don’t copy content from elsewhere,
                                                      72

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   and don’t publish content solely for search engines instead of users. Google’s
   algorithms (and quality raters) are getting increasingly sophisticated at
   identifying these tactics. It’s far better to invest that energy in legitimately
   better content. In Google’s own words, “rather than focusing on specific
   algorithmic tweaks, we encourage you to focus on delivering the best possible
   experience for users” developers.google.com. If you do that, Panda shouldn’t
   be a problem.​

6.​ Monitor Your Site’s Quality Continuously: Quality is not a one-time fix. It’s an
   ongoing commitment. Regularly review your site: as it grows, ensure new pages
   meet your quality standards. Update or prune outdated content (keeping
   content fresh can be seen as a sign of care and quality). Watch metrics like
   what pages have high exit rates or very low time-on-page - these might be
   content to improve. Solicit user feedback - if users complain about certain
   pages, take that seriously. Google’s own core update advice is to “focus on
   content” and provides questions similar to Panda’s to self-assess your site.
   Using Google’s Search Quality Rater Guidelines (a document Google
   published that mirrors many Panda concepts) can be insightful - it describes in
   detail what Google considers a high “Page Quality” vs a low one (for example,
   YMYL pages with no author info and poor expertise are rated lowest). Aligning
   your site with those guidelines is effectively optimizing for Panda.​

7.​ Be Patient and Consistent: If your site was hit by quality algorithms (i.e., a
   broad drop in rankings coinciding with a known quality update or core update),
   making fixes will not yield an immediate rebound. Google’s quality scoring
   (especially when it was run periodically) can take weeks or months to recognize
   improvements. Even now, with continuous updates, it might take a major core
   update for your recovery to fully materialize. Google has said “as sites change,
   our algorithmic rankings will update to reflect that” developers.google.com -
   implying that recovery will come if you truly improve, but you must consistently
   maintain quality and wait for Google to re-crawl and re-evaluate enough of
   your site. Don’t be discouraged by lack of instant results; focus on making your
   site objectively better. Many webmasters have reported successful Panda
   recoveries after a few months by following the above steps diligently.​

                                                      73

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




   8.​ Use Google’s Tools and Feedback Channels: Make sure to leverage Google
      Search Console for any technical issues that could affect quality (crawl errors,
      mobile usability, Core Web Vitals, etc.). Sometimes what appears to be a Panda
      issue could be compounded by technical SEO problems (for instance, a
      misconfiguration causing duplicate pages). Fix those too - a well-run site is
      part of quality. Additionally, Google’s Webmaster Forum can be a place to get
      advice if you’re not sure why your site is perceived as low quality. While Google
      won’t tell you exactly what to do, you may get useful insights from product
      experts or see if a Google representative has given any specific guidance. In
      the Panda launch period, Google even invited affected site owners to provide
      feedback for the engineers developers.google.com. While they don’t do that
      publicly now, the Search Liaison on Twitter and Google’s blog posts often
      address common issues - keep an eye on those to understand Google’s
      expectations.​


By following these steps, you’re essentially aligning your site with what quality scoring
is designed to reward.

As Google’s Amit Singhal summed up: “Focus on delivering the best possible user
experience on your websites and not on what you think are Google’s current ranking
algorithms or signals.” developers.google.com

If you do that, the rest (rankings) will eventually follow. Panda taught the SEO world
that quality is king - a lesson that is even more true today.

References: Google’s official announcements and blog posts, patents, and internal
documents have all consistently pointed to the above principles.

The 2011 Google blog post “More guidance on building high-quality sites”
developers.google.com and developers.google.com is essentially a checklist that
foreshadows today’s best practices. Google’s patent on site quality scoring confirms
the technical underpinning of Panda’s site-wide classifier
patents.google.compatents.google.com.

                                                          74

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The DOJ trial exhibits from 2023 reaffirm the lasting importance of Panda’s site quality
score (QScore) in modern ranking justice.gov and justice.gov.

And quotes from Googlers like Amit Singhal and Matt Cutts give a transparent look at
Panda’s intent and mechanics, straight from the source wired.com and wired.com.

Following this evidence-based advice will not only help you avoid quality related
penalties but also improve your site’s overall SEO performance and user satisfaction in
the long run.

Google’s communications about Quality Score
Prominent spokespeople like John Mueller and Gary Illyes have explicitly stated on
multiple occasions that Google does not use an "overall domain authority" or "website
authority score" but they have all but also told us… they do.

On the surface this discrepancy between the internal reality of the Q* metric and the
public-facing narrative represents one of the most significant disconnects uncovered
by the trial.

On further examination of these statements, with full context, we get a more nuanced
answer from John, where he even explicitly mentions “Quality Score”.

When John Mueller was asked "could it be that when old pages were published, we
had a higher website authority or something that Google memorised and we don't
have anymore or for new pages, Google is applying more strict rules than for old
content?"

He answered:




                                                         75

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




“In general, when we have something that's kind of like a sitewide score, then the
current sitewide score applies to everything for that website. So from my point of
view, we don't have anything like a website authority score. But if we did have
something like that or if we have like when we're looking at, for example, like
quality signals that are more sitewide, then that's something that applies
across the whole website in the state that it's at now. So it's not the case that we
would say, oh, five years ago, you had this score for your website, therefore your
content will be rated like this forever. Rather we look at your website overall now
and we apply the current score to all of your pages on the website. So that's kind
of what we do when it comes to sitewide signals…. It's always based on what has
happened in the past. That's definitely something that kind of gets collected over
time, but that's the current score based on our understanding of the current website,
which of course, that understanding is based on things that have happened in the
past. So it's not so much that the score, uh, that we had maybe I don't know, last year
is applied to different parts of the website, but rather as we understand the website
now, the current score based on that is what we apply across the website now.

John continues: “In general, I recommend folding things together and putting things
on one strong website because that's something where you can kind of, um,
concentrate all of the information that we have about your website onto one
one domain or one website, which makes a lot easier for you to maintain and also
makes it easier for us to understand like overall, this is a really strong website.”

John reiterated: “There's always history there because we can't recrawl the whole
website now. So we can't recalculate the current quality score now.”

As you can see, the context around John’s answer is illuminating, even though he
confirms the don’t really have a “website authority score” he clarifies that some
signals “like quality signals that are more sitewide” and “quality score” - yes, he
even uses the term in the same conversation - is based on historic metrics and that
quality calculations like those mentioned in Google Panda, that are always a “rolling”
calculation.

Your “quality score”, we are told, is dynamic. It can go up, or down.

​

                                                         76

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By this point in time Googler spokespeople a lot higher up than John were on record
confirming this idea of a site-wide quality score:

"We made a series of changes ...that reduced the quality scores of certain types
of websites...We didn’t want users clicking on crummy sites." Eric Shmidt, Google
2014

Notice that while John Mueller of Google was criticised for denying a “website
authority score”, he did confirm, in the same discussion seconds later, the
nature of a website's “quality score”.

Folk overlook lots of context when criticising Googlers and their statements around
these issues. One can often easily overlook the context of the audience they are
talking to too.

​




                                                         77

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Role of Machine Learning - RankBrain, DeepRank, and the AI
Layer
While the trial emphasized the foundational role of human-engineered systems, it also
provided critical context for how Google deploys its formidable machine learning (ML)
capabilities.

The evidence suggests that Google's use of AI is not as an all-encompassing,
autonomous "ranking brain," but rather as a set of highly specialized tools applied to
solve specific, complex problems that are intractable for hand-crafted rules.

This approach reflects a pragmatic, risk-averse engineering culture focused on
integrating powerful ML models within a controllable and understandable framework.

RankBrain: The Query Interpreter
Testimony from Google engineers Eric Lehman and Pandu Nayak clarified the primary
and specific function of RankBrain, one of Google's earliest and most famous ML
systems.

Its core purpose is not to rank documents directly, but to interpret search
queries, particularly those that are novel, ambiguous, or part of the long tail of
search - the vast number of unique queries that Google has never seen before.

When simple keyword matching is insufficient to determine a user's intent, RankBrain
helps the search engine understand the underlying meaning and concepts within the
query, allowing it to retrieve a more relevant set of results.

A crucial detail for the antitrust proceedings emerged from Eric Lehman's testimony:
RankBrain is trained on historical search data, not on live, real-time user data.

While the sheer scale of this historical data is itself a product of Google's market
dominance, this distinction subtly mitigates the argument that the system relies on a
moment-to-moment data advantage that competitors lack.

Although its relative importance within the full suite of ranking signals may have
evolved since it was first introduced and famously described as the "third most
important signal," RankBrain's role as a sophisticated query interpretation model
                                                         78

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




remains a cornerstone of Google's ML layer.

DeepRank and the Pursuit of Transparency
The trial also shed light on other deep learning models like DeepRank, which are used
in the ranking process.

More importantly, the evidence revealed a concerted engineering effort at Google to
maintain transparency and control even over these complex systems.

One trial exhibit noted that BERT-based DeepRank signals could be "decomposed
into signals that resembled the traditional signals".

Another document explained that a system called "eDeepRank" attempts to "take
LLM-based signals and break them down into components to make them more
transparent".

This effort to deconstruct the outputs of ML models is a profound indicator of
Google's engineering philosophy.

It shows that Google engineers are not simply deploying black-box models and
trusting their outputs.

Instead, they are actively building parallel systems to understand why an ML model
made a particular ranking decision.

This aligns perfectly with the rationale for hand-crafting foundational signals like T*
and Q*: the institutional imperative to maintain human understanding, control, and the
ability to debug the system's behavior.




                                                         79

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Human-ML Symbiosis
The complete picture that emerges from the trial is not one of machine learning
replacing human engineers, but of a deeply symbiotic relationship. The architecture is
layered to leverage the strengths of each approach:
 1.​ Human-Engineered Foundation: Systems like T* (Topicality) and Q* (Quality)
     provides a stable, predictable, and understandable foundation for relevance and
     trust.
 2.​ Data-Driven Refinement: The Navboost system acts as a powerful refinement
     layer, using the scaled intelligence of past user behavior to improve upon the
     foundational scores.
 3.​ Specialized ML Tools: ML models like RankBrain and DeepRank are then applied
     to solve specific, high-complexity problems like query understanding and
     nuanced ranking adjustments that are difficult to address with hand-crafted
     rules.21

This symbiosis is also evident in the challenges Google faces. One engineer's
testimony acknowledged that the rise of AI-generated content is making the
problem of search quality worse, not better.

This highlights the ongoing "cat and mouse game" where human oversight and
continuous engineering are essential to develop new signals and systems to combat
the abuse of new technologies that aim to manipulate search rankings.

This reality runs counter to the simplistic notion that Google can simply deploy more
AI to solve the problem of AI-generated spam.

Google's approach to machine learning is thus revealed to be highly pragmatic.

Rather than ceding control to a single, end-to-end ML model, it integrates specialized
models as components within a broader, more traditional software engineering
architecture.

The strategic decision to invest in making these models' outputs more transparent
and understandable demonstrates a mature, risk-averse culture.

This suggests that Google's durable competitive advantage lies not just in possessing
                                                         80

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




superior ML models, but in having the engineering discipline and robust infrastructure
to deploy them safely and effectively in a high-stakes, global production environment
without sacrificing control or accountability.#


Overview of Google's RankBrain and BERT-based RankEmbed
Bill Slawski’s quote from 2019 succinctly debunked misconceptions: “Semantic search
at Google is not powered by LSI… You cannot optimize pages for…RankBrain or BERT.”
hobo-web.co.uk - using it to bust SEO myths and illustrate that Google’s NLP is far
beyond old techniques.

RankBrain

Launch: Officially confirmed by Google in 2015.

What RankBrain Does

RankBrain is Google's first significant implementation of machine learning to
understand search queries. It specifically addresses queries Google hasn't seen
before, helping interpret user intent and meaning.

     Google (Greg Corrado, Senior Research Scientist):​
     "`RankBrain' uses artificial intelligence to filter results. RankBrain has
     become the third-most important signal contributing to the result of a
     search query." (Bloomberg, Oct 2015)

Technical Mechanism (Neural Embeddings)

RankBrain converts words into mathematical vectors (embeddings), allowing Google
to understand semantic relationships between terms.

     Google Patent US9245078B1 ("Word embedding and phrase
     embedding generation"):​
      "Words are represented as embeddings in continuous vector spaces.
     These embeddings encode semantic information by capturing
     relationships between words based on context. Query terms and phrases
     are represented as embeddings, enabling semantic matching."
                                                         81

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




RankBrain evaluates ambiguous or complex queries by identifying similar past
searches and user behaviors to interpret intent. The system learns continuously from
user interaction (clicks, dwell time).

Integration with Ranking Signals

RankBrain doesn't replace PageRank or topicality signals but integrates as a
complementary signal.

     Google (Gary Illyes, Webmaster Trends Analyst):​
     "RankBrain will understand better what results work for queries. It’ll
     understand that certain stop words should not be dropped. Sometimes the
     word “with” is dropped from a query, but RankBrain will understand that we
     need to keep it." (Q&A, 2016)

Current Usage

As of 2025, RankBrain remains actively used by Google for query interpretation and
semantic relevance.




                                                         82

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




BERT-based RankEmbed

Launch: BERT officially integrated into Google Search in October 2019.

What BERT (RankEmbed) Does

BERT (Bidirectional Encoder Representations from Transformers) significantly
advances Google's understanding of language context. RankEmbed refers specifically
to Google's implementation of BERT in ranking algorithms.

     Google Blog (Pandu Nayak, VP Search):​
     "BERT models can therefore consider the full context of a word by looking
     at the words that come before and after it - particularly useful for
     understanding the intent behind search queries"​
     (Google Official Blog, 2019)

Technical Mechanism (Transformer-based Neural Network)

BERT is a deep-learning neural network architecture designed around the
Transformer model. Unlike previous models, BERT is bidirectional, meaning it
considers the full context of a word by looking at words before and after it.

     Google Paper ("BERT: Pre-training of Deep Bidirectional Transformers
     for Language Understanding"):​
      "BERT uses a multi-layer bidirectional Transformer encoder. The model is
     pre-trained on large text corpora, using masked language modeling (MLM)
     and next sentence prediction (NSP) tasks. It effectively captures
     context-dependent meanings of words."​
      (Devlin et al., Google AI, 2018)

Google further describes BERT as being especially useful for understanding complex
queries that depend on subtle nuances.

     Google Blog (Pandu Nayak, VP Search):​
     "BERT will help Search better understand one in 10 searches in the U.S. in
     English”​
     (Google Official Blog, 2019)
                                                         83

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




RankEmbed and Semantic Matching

RankEmbed specifically refers to embedding-based methods using BERT-style neural
embeddings to match user queries with relevant documents.

This patent explicitly describes the use of neural embeddings, like those from BERT, in
ranking systems:

   ●​ Queries and documents are converted into embeddings.
   ●​ Semantic similarity scores (cosine similarity, dot product) determine ranking
      positions.

Integration with Ranking Signals

BERT-based RankEmbed works alongside traditional signals (PageRank, topicality,
freshness) to refine Google's understanding of user queries and document content.

     Google (Danny Sullivan):​
     "A key signal, RankEmbed, is a “dual encoder model” that embeds queries and
     documents into an “embedding space.” This space considers semantic
     properties and other signals. Retrieval and ranking are based on a “dot
     product” or “distance measure in the embedding space. 2019




                                                         84

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Current Usage
As of 2025, BERT-based RankEmbed remains integral to Google's ranking algorithms. It is
particularly important for handling long-tail queries, questions, and conversational searches.




 Factor                                  RankBrain                                 BERT-based RankEmbed


 Launch Date                             2015                                      2019


 Core Mechanism                          Neural embeddings, ML                     Bidirectional Transformer
                                                                                   embeddings


 Function                                Query intent & semantic                   Contextual
                                         interpretation                            query-document matching


 Integration                             Complements traditional                   Complements traditional
                                         signals                                   signals


 Main Use                                Ambiguous or rare queries                 Complex, conversational,
                                                                                   long-tail queries


 Patents / Papers                        US20200364850A1 (neural                   Devlin et al. (BERT)
                                         ranking)


 Current Status                          Active                                    Active




                                                          85

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Key Takeaway

RankBrain and BERT-based RankEmbed have revolutionized Google's ability to
understand and process user queries.

While RankBrain introduced machine learning into query processing for ambiguous
searches, BERT-based RankEmbed provided a significant leap forward in contextual
semantic matching, especially for conversational searches.

Both systems function integratively, enhancing Google's traditional signals (topicality,
authority, PageRank) to provide more accurate, contextually relevant search results.




                                                         86

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Reconciling the New Model of Google Search
The collective evidence presented in the U.S. v. Google trial necessitates a
fundamental revision of the public understanding of Google's search engine.

The testimonies and internal documents provide the constituent parts of a new, more
granular architectural model.

This model reveals a logical pipeline that leverages human engineering, massive user
data, and specialized machine learning in distinct stages.

Scrutinising these components not only clarifies how Google Search works but also
exposes the deep chasm between this internal reality and the carefully curated public
narrative the company has maintained for over a decade.




                                                         87

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Unified Ranking Pipeline
Based on the trial evidence, the process of generating a search result can be modeled
as a multi-stage pipeline:
 1.​ Query Interpretation: When a user enters a search, the query is first processed
     to discern its underlying intent. For common queries, this may be straightforward.
     For novel, ambiguous, or long-tail queries, specialized machine learning systems
     like RankBrain are employed to interpret the user's need beyond simple keyword
     matching.
 2.​ Initial Retrieval & Foundational Scoring: The system retrieves a broad set of
     potentially relevant documents from its massive index, which contained an
     estimated 400 billion documents as of 2020. Each of these documents is then
     given a set of base scores by the foundational, hand-crafted systems. The​
     T* (Topicality) system provides a score for query-dependent relevance based on
     the ABC signals (Anchors, Body, Clicks). Concurrently, the​
     Q* (Quality) system provides a largely static, query-independent score for the
     overall trustworthiness and authority of the document's source domain, with
     PageRank as a key input.
 3.​ User-Behavior Refinement: This is arguably the most critical and competitively
     significant stage. The Navboost system takes the list of documents scored by T*
     and Q* and subjects it to a powerful re-ranking and filtering process. Drawing on
     13 months of aggregated user click data, Navboost dramatically promotes results
     that have historically satisfied users for similar queries or in similar contexts (e.g.,
     same location, same device type) and demotes those with poor engagement
     signals. This step reduces the candidate set from tens of thousands to a few
     hundred elite contenders.
 4.​ Final Ranking & SERP Construction: The final, refined list of documents may
     undergo further scoring adjustments by more computationally expensive deep
     learning systems like DeepRank. Simultaneously, the Glue system analyzes
     real-time interaction signals to determine the optimal layout of the Search Engine
     Results Page, deciding which SERP features (maps, images, knowledge panels) to
     display and where to place them. The final, assembled page is then delivered to
     the user.



                                                          88

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




The Data Feedback Loop as a Competitive Moat
This detailed architectural model provides a clear mechanical explanation for the
"data feedback loop" that was at the heart of the DOJ's antitrust case.
The trial evidence validates the theory that Google's dominance is a self-reinforcing
cycle.
The process is as follows:
 1.​ Google's exclusive default placement deals with companies like Apple and
       Samsung guarantee it a dominant market share (over 90%) and an unparalleled
       volume of search queries.4
 2.​   This massive query volume generates an equally massive and proprietary stream
       of user interaction data (clicks, dwell time, etc.).
 3.​   This data is the exclusive fuel for the Navboost system.
 4.​   Navboost uses this data to significantly improve the quality and relevance of
       Google's search results in a way no competitor, starved of similar data, can
       replicate.
 5.​   This superior search quality is then used by Google as the primary business
       justification for its partners to continue signing exclusive default deals.

This cycle, it is claimed by those pursuing Google, creates an insurmountable barrier
to entry.

The claim is that a potential competitor cannot develop a search engine of
comparable quality without access to large-scale user data, but it cannot acquire that
user data without first achieving a scale that Google's exclusionary contracts are
designed to prevent.




                                                            89

          © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Chasm Between Public Narrative and Internal Reality
The most profound implication of the trial is the claims of the exposure of a significant
and deliberate gap between Google's public communications about its search engine
and the operational reality revealed under oath.

The discrepancies are not minor clarifications but sometimes contradictions on core
aspects of the ranking system.

This has somewhat damaged the credibility of Google's public-facing representatives
and will reshape the relationship between the company and the technical community
that analyzes its products.

It is worth pointing out from my experience, as a professional SEO investigating
Google’s statements for almost 20 years, Google spokespeople almost all but told us
how the search giant worked — this is undeniable.

Almost every aspect of search was shared by Google over the years.

It's clear why they aimed to focus webmasters' attention all these years on user
experience (like Core Web Vitals)and not Clicks or Links.




                                                          90

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




 Topic                                    Google's Historical Public                Revelation from U.S. v.
                                          Stance                                    Google Testimony/Exhibits


 Use of Click Data for                    Publicly evasive or dismissive.           Clicks are a core, foundational
 Ranking                                  Often described clicks as a               signal. Signal 'C' (user dwell
                                          "noisy" signal used for                   time) is a component of the T*
                                          evaluation and                            (Topicality) score. The
                                          experimentation, not direct               powerful Navboost system is a
                                          ranking.                                  major ranking refiner based
                                                                                    entirely on 13 months of
                                                                                    aggregated user click data.


 Site-Level Authority Score               Explicitly and repeatedly                 Google uses Q*, an internal,
                                          avoided. Statements include:              largely static, domain-level
                                          "we don't really have 'overall            score to measure
                                          domain authority'" and "we                trustworthiness and quality.
                                          don't have anything like a                PageRank, a measure of link
                                          website authority score."                 authority, is a key input to this
                                          would be semantically correct             score. This functions as a site
                                          but technically, not.                     authority metric.


 Nature of Ranking                        Often portrayed as                        The foundational systems (T*,
 Algorithm                                increasingly driven by                    Q*) are "hand-crafted" by
                                          inscrutable, end-to-end                   engineers for control and
                                          Machine Learning and AI, with             debuggability. ML is used for
                                          systems like RankBrain taking             specific, targeted tasks like
                                          center stage.                             query interpretation, not as a
                                                                                    monolithic ranking brain.




This evidence demonstrates that the simplified, and at times misleading, model of
search presented to the public, SEOs, and webmasters was a strategic choice.

The long-standing paradigm of "trust but verify" in relation to Google's public
guidance has been negatively impacted.
                                                           91

         © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




For the sophisticated technical community, the trial record - comprising sworn
testimony and internal documents - now stands as the canonical source of truth.

Future public statements from the company will inevitably be viewed through a lens of
some scepticism and cross-referenced against the legally compelled facts.

The dynamic has shifted permanently from one of interpreting guidance to one of
reverse-engineering a blueprint.

The "black box" of Google Search, while not fully transparent, is now more illuminated
than ever before, and the insights gained will redefine the field of search analysis for
years to come.

In my opinion, when a SEO or webmaster (or anyone for that matter for the former 2
are always listening) it is akin to a bank robber asking a bank teller if the bank has
a safe where it is, what its called and what is the combination. The bank teller,
to be transparent and helpful, will answer those questions with varying degrees
of opacity. Something, I think, was always to be expected from any private
company.

Again in my opinion if Google didn’t control their SERPS, then SEOs, Blackhats
spammers and mainly hackers would dominate every SERP. Small publishers would
have an equally hard time, if not worse, is what I am saying.

The current situation though, and again this is just my opinion, is that this path kind of
ensures the “house always wins” when small publishers do not.




                                                          92

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Unlocked Warehouse
While the DOJ trial revealed the existence of Navboost, the Content Warehouse leak
gave us an unprecedented look at its mechanics, including metrics like goodClicks and
lastLongestClicks.

What Google's Accidental Leak Tells Us About Search, Secrecy, and
Strategy
In the spring of 2024, the digital world was simmering. A tension had been building for
months between Google and the global community of search engine optimisation
(SEO) professionals, marketers, and independent publishers who depend on its traffic
for their livelihoods - especially after the impact of September 2023 HCU Update.

It was in this climate of uncertainty that a simple, automated mistake became the
spark that ignited a firestorm of revelation.

This was not a dramatic, cloak-and-dagger operation.

There was no shadowy whistleblower or sophisticated cyberattack. Instead, on March
13, 2024, an automated software bot named yoshi-code-bot made a routine update to
a public GitHub repository.

In doing so, it inadvertently published thousands of pages of Google's highly
sensitive, internal API documentation.

For weeks, these documents sat in plain sight, largely unnoticed. On May 5, Erfan
Azimi discovered the repository and shared it: Rand Fishkin, founder SparkToro, and
Michael King, of iPullRank.

After weeks of verification, they unleashed their findings on May 27, and the digital
marketing world was irrevocably changed.

What was exposed was not the algorithm's source code - the complex, proprietary
recipe for ranking web pages.

Rather, it was something arguably more valuable for strategic analysis: the internal
documentation for Google's "Content Warehouse API".

This was the blueprint of the system, a detailed inventory of the ingredients Google
uses.
                                                          93

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




It outlined over 14,000 attributes across nearly 2,600 modules, revealing the specific
types of data Google collects, the metrics it measures, and the systems it employs to
make sense of the entire internet.

While it didn't reveal the precise weighting of each factor, it provided an
unprecedented look at the menu of options available to Google's engineers.

The leak's true significance lies in the potential chasm it exposed between what
Google has publicly told the world for over a decade and what its own internal
documentation revealed.

For years, SEO professionals had operated on a combination of official guidance,
experimentation, and hard-won intuition.

Many of their core beliefs - that a website's overall authority matters, that user clicks
influence rankings, that new sites face a probationary period - were consistently and
publicly dismissed by Google's representatives.

The leak served as a stunning vindication for this community, confirming that their
instincts, honed through years of observation, were largely correct.

For Google, it triggered a crisis of credibility.

The ultimate value of this accidental revelation is not a simple checklist of technical
tricks to climb the search rankings. It is the profound strategic realignment it
demands.

The unlocked warehouse confirms that sustainable success in Google's ecosystem is
less about manipulating an opaque algorithm and more about building a genuine,
authoritative brand that users actively seek, trust, and engage with.

It proves that the focus must shift from attempting to please a secretive machine to
demonstrably satisfying a now-quantifiable human user.

This chapter will dissect the anatomy of the leak, explore its most significant
contradictions, and lay out the new strategic playbook for any business that wishes to
thrive in a post-leak world.




                                                          94

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Anatomy of a Leak
This library confirmed that the "Google algorithm" is not a monolithic entity but a
complex, multi-layered ecosystem of specialized systems working in concert.

Subsection 1.1: What Was Actually Leaked?
The story of the leak begins with a timeline.

On March 13, 2024 (some reports cite March 29), an automated bot, yoshi-code-bot,
appears to have accidentally published a copy of Google's internal Content
Warehouse API documentation to a public GitHub repository. This repository remained
public until it was removed on May 7, 2024. During this window, the information was
indexed and circulated, eventually finding its way to SEO professional Erfan Azimi, who
then shared it with industry veterans Rand Fishkin and Michael King. It was their
coordinated analysis and publication on May 27 that brought the leak to global
attention.

The source of the leak is crucial; it came directly from Google's own infrastructure.

The documentation was for the internal version of what appears to be its
Content Warehouse API, a system for storing and managing the vast amounts of
data Google collects from the web.

The files contained links to private Google repositories and internal corporate pages,
and multiple former Google employees who reviewed the documents confirmed their
legitimacy, stating they had "all the hallmarks of an internal Google API".

The sheer technical density of the material - filled with definitions for protocol buffers
(protobufs) and thousands of module attributes - further cemented its authenticity.

It was not a curated "false flag" designed to mislead, but a messy, genuine, and
accidental glimpse into Google's engineering world.

The scale of the leak was immense.

The documentation spanned over 2,500 pages, detailing 14,014 distinct attributes, or
"features," organized into 2,596 modules.

These attributes represent the specific types of data that Google's systems are
designed to collect and consider, covering everything from search and YouTube to

                                                          95

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




local services and news.

Google's official response was swift but cautious.

In a statement to media outlets, a Google spokesperson confirmed the
documents were authentic but urged the public to avoid making "inaccurate
assumptions about Search based on out-of-context, outdated, or incomplete
information".

This was widely interpreted by the SEO community as a standard non-denial denial, an
attempt to downplay the significance of the revelations without explicitly refuting
them.

A Glimpse Inside the Machine: Core Systems and "Twiddlers"
Perhaps the most fundamental insight from the leak is that the popular conception of
a single, monolithic "Google Algorithm" is a fiction.

The documentation confirms a far more complex reality: a layered ecosystem of
interconnected microservices, each with a specialized function, working together in a
processing pipeline.

This structure means there isn't one thing to "optimise for"; rather, a successful
strategy must address signals relevant to each stage of the process.

The journey of a web page through Google's systems can be understood through
several core components named in the leak:

  ●​ Crawling: The process begins with systems like Trawler, which are responsible
     for discovering and fetching content from across the web.
  ●​ Indexing: Once content is fetched, it is processed and stored by a suite of
     indexing systems. Alexandria and TeraGoogle appear to be the primary and
     long-term storage systems, respectively. Critically, a system named SegIndexer
     is responsible for placing documents into different tiers within the index. This
     confirms the long-held theory that Google maintains different levels of its index,
     with links from documents in higher-quality tiers carrying more weight.
  ●​ Ranking: The initial scoring and ranking of documents is handled by a primary
     system called Mustang. This system performs the first pass, creating a
     provisional set of results based on a multitude of signals.

However, the process does not end with Mustang. The leak sheds significant light on a

                                                         96

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




subsequent and powerful layer of the system known as "Twiddlers."

This concept is critical for any business leader to understand, as it represents
Google's final editorial control over its search results.

Twiddlers are re-ranking functions that adjust the order of search results after
the main Mustang system has completed its initial ranking.

They act as a fine-tuning mechanism, applying boosts or demotions based on
specific, often real-time, criteria. Unlike the primary ranking system, which evaluates
documents in isolation, Twiddlers operate on the entire ranked sequence of
results, making strategic adjustments before the final list is presented to the
user.

The leaked documents reference several types of these re-ranking functions,
illustrating their power and versatility.

Examples include FreshnessTwiddler, which boosts newer content; QualityBoost,
which enhances quality signals; and RealTimeBoost, which likely adjusts rankings
based on current events or trends.

The most frequently mentioned and strategically significant of these systems is
NavBoost, a powerful Twiddler that re-ranks results based on user click behavior,
which will be explored in detail in the next section.

The existence of this multi-stage architecture - crawling, tiered indexing, initial
ranking, and multiple layers of re-ranking - proves that Google's process is far more
nuanced and dynamic than a simple mathematical formula.




                                                          97

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Four Great Vindications - Where SEO Theory Met
Google's Reality
The core impact of the Google Warehouse leak was not in revealing entirely new
concepts, but in confirming, with documentary evidence, what many experienced SEO
professionals had long suspected to be true.

For years, a significant gap existed between the public guidance offered by Google's
representatives and the real-world results observed by practitioners.

The leak bridged this gap, vindicating long-held theories and exposing a pattern of
strategic obfuscation from the search giant.

This section deconstructs the four most significant areas where Google's public
narrative crumbled in the face of external interpretation of its own internal
documentation.

The Ghost in the Machine: siteAuthority is Real
For years, the concept of "domain authority" - the idea that Google assigns an overall
quality or trust score to an entire website - was a central point of contention.

Google's public-facing representatives, most notably John Mueller, repeatedly and
explicitly denied its existence.

They framed it as a metric invented by third-party tool providers like Moz and Ahrefs,
stating unequivocally that Google does not use "Domain Authority at all" for ranking
purposes.

While occasionally hinting at "site-wide level metrics," the official line was a firm denial
of a holistic authority score akin to Domain Authority - or at least - called that…
(semantics).

The leaked documentation obliterated this narrative.

Contained within a module for "Compressed Quality Signals" - a set of fundamental
quality scores stored for every document Google crawls - is an attribute explicitly
named siteAuthority.

This discovery was a watershed moment for the SEO community, providing concrete
proof that Google does, in fact, calculate a site-wide authority metric that influences
                                                          98

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




the ranking potential of every page on a domain.

This confirmation fundamentally changes the strategic calculus. It means that
optimizing individual pages in isolation is an incomplete strategy.

The overall reputation, trustworthiness, and authority of the entire domain are critical,
acting as a foundational element upon which individual page performance is built.

The leak suggests this siteAuthority score is likely a composite metric, influenced by a
variety of signals.

These almost certainly include the PageRank of the site's homepage (which the
documents state is considered for every single document on the site), brand-related
signals, and, crucially, the user interaction data collected by systems like NavBoost.

A site is not just a collection of pages; it is an entity with a reputation, and Google is
measuring it.

The All-Seeing Eye: Clicks, Chrome, and NavBoost
No topic has been more contentiously debated than the role of user behavior in
Google's rankings.

The official narrative from Google has been consistent: user engagement signals like
click-through rate (CTR) are too "noisy" and easily manipulated to be used as a direct
ranking factor.

Furthermore, Google representatives have explicitly denied using data from the
company's own Chrome browser to inform search rankings.

The leak reveals this to be, at best, a semantic misdirection.

The documentation is saturated with references to a system named NavBoost,
described as a powerful "re-ranking system based on click logs of user behavior".

This system, also referred to as "Glue," was first mentioned in testimony during the
U.S. Department of Justice antitrust trial but was detailed extensively in the leak.
NavBoost analyzes a sophisticated array of click metrics to gauge user satisfaction,
including:




                                                          99

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




  ●​ goodClicks and badClicks: To differentiate between successful and unsuccessful
     user interactions.
  ●​ lastLongestClicks: A critical metric that identifies the search result a user dwells
     on before ending their search session, serving as a powerful proxy for
     satisfaction.
  ●​ unsquashedClicks: A normalized click metric, likely designed to "squash" or
     discount inorganic patterns and prevent manipulation from click-bots.

The fuel for this massive data-processing engine is the Google Chrome browser.

The leak exposed a module named ChromeInTotal and attributes like
uniqueChromeViews and chrome_trans_clicks, providing undeniable evidence that
Google leverages clickstream data from its billions of browser users to power its
ranking systems.

This revelation represents a true paradigm shift.

User behavior is not a passive outcome of good rankings; it is a primary, active input
into the ranking algorithm.

Every click is a vote. When a user clicks a result and stays on that page (a "long click"),
they are sending a positive signal to Google that their query was satisfied.

When they click a result and immediately return to the search page to choose another
option ("pogo-sticking"), they are sending a powerful negative signal. This was a
signal brought to my attention by A.J. Kohn in 2008.

This elevates the strategic importance of on-site user experience (UX), headline and
meta description optimization, and brand recognition to the level of core ranking
imperatives.




                                                         100

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Walled Garden: The "Sandbox" and hostAge
For as long as SEO has existed, practitioners have observed a phenomenon known as
the "Google Sandbox," a supposed probationary period during which new websites
struggle to gain visibility, regardless of their content quality.

Officially, Google has always denied the existence of such a mechanism.

The leak provided (to some) clear validation for this long-standing theory. Within the
PerDocData module, which contains information about individual documents, is an
attribute named hostAge.

The documentation's description is unambiguous: this attribute is used to "sandbox
fresh spam in serving time".

This confirms that a website's age is a factor Google considers and that new domains
are treated with algorithmic suspicion until they have had time to establish a track
record of credibility and trustworthiness.

This has profound strategic implications, particularly for startups, new product
launches, and any business entering the digital space for the first time.

It means that expecting immediate SEO success is unrealistic.

The "sandbox" effect necessitates a long-term approach.

Building trust and authority through high-quality content, positive user signals,
and legitimate backlinks from day one is not merely good practice; it is an
essential requirement to eventually escape this initial period of algorithmic
probation and compete on a level playing field.




                                                         101

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Unspoken Hierarchy - Whitelists and Special Treatment
Google's public image is that of an objective, impartial organizer of information, with
its algorithm serving as a neutral arbiter of relevance and quality.

The leak, however, reveals that for the most sensitive and critical topics, the playing
field is anything but neutral.

The documents detail the existence of potential whitelists, which are essentially
lists of domains that are manually elevated for certain types of queries.

Attributes such as isElectionAuthority and isCovidLocalAuthority demonstrate that for
high-stakes "Your Money or Your Life" (YMYL) topics like elections and public health,
Google actively intervenes to promote sources it deems authoritative and suppress
potential misinformation.

This is a logical and arguably necessary measure for public safety, but it
confirms that for these queries, authority is not algorithmically earned but editorially
assigned by Google.

Furthermore, the system appears to apply special classifications to different types of
websites.

The documentation includes flags for smallPersonalSite and identifies sites based on
their business model, such as blogs, e-commerce stores, or travel sites.

This suggests that Google may apply different ranking adjustments or even limit the
number of sites of a certain type that can appear in a single search result, enforcing a
kind of algorithmic diversity.

The existence of these systems - siteAuthority, NavBoost, hostAge, and whitelists -
are not independent phenomena.

They are deeply interconnected, forming a self-reinforcing system that creates a
virtuous cycle for established, trusted brands.

A new site begins its life in the hostAge sandbox. To escape, it must build trust, which
is measured by its overall siteAuthority.

A significant component of this authority score is derived from positive user
engagement signals captured by NavBoost - real users, tracked via Chrome, clicking
on and trusting the site.
                                                          102

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The path to authority is paved with positive user interactions.
This creates a powerful feedback loop: a strong brand drives user searches and clicks,
which boosts NavBoost scores, which in turn increases siteAuthority, which helps the
site rank for even more terms, further reinforcing the brand's visibility and dominance.
This system inherently favors those who have already won the battle for user trust.

This also provides context for Google's history of public denials.

It is unlikely that these statements were born of simple malice, and often, the context
of who Google is talking to is often missed.

Rather, they can be interpreted as a form of strategic obfuscation intended to
protect the integrity of the search results.

If Google were to publicly announce, "We use clicks to rank websites," it would
instantly spawn a massive, illicit market for click-bots, rendering the signal useless.

By denying it, they discouraged this manipulative behavior.

Similarly, by stating, "We don't use Domain Authority," they were being semantically
precise - they do not use the specific metric from the company Moz - while
conveniently obscuring the existence of their own possible internal equivalent,
siteAuthority.

For business strategists, the lesson is clear: Google's public communications are a
component of its algorithmic defense system and must be treated with a healthy
degree of interpretation - and always remember who Google is usually talking to
about this kind of stuff - remember about the bank teller analogy from earlier in this
book.




                                                          103

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Area of Contention          Google's Public                 Key Leaked Attributes The New Reality
                            Narrative (Pre-Leak)            & Systems             (Post-Leak Insight)

Site-Wide Authority         "We don't have a 'domain siteAuthority,                        Google calculates a
                            authority' score. It's not authorityPromotion,                 holistic, site-wide
                            something we use at all." Homepage PageRank                    authority metric that
                                                                                           influences rankings. A
                                                                                           site's overall reputation
                                                                                           matters immensely.

User Click Signals          "Clicks are a 'noisy'     NavBoost, goodClicks,                User click behavior is a
                            signal; we don't use them badClicks,                           primary input for a
                            directly for ranking."    lastLongestClicks                    powerful re-ranking
                                                                                           system (NavBoost). User
                                                                                           satisfaction is a direct
                                                                                           ranking factor.

Chrome Browser Data         "I don't think we use           ChromeInTotal,                 Data from billions of
                            anything from Google            uniqueChromeViews,             Chrome users is
                            Chrome for ranking."            chrome_trans_clicks            collected and used to
                                                                                           power ranking systems
                                                                                           like NavBoost and
                                                                                           evaluate site popularity.

New Site "Sandbox"          "There is no 'sandbox' for hostAge, PerDocData                 A sandboxing mechanism
                            new websites."             module                              exists to temporarily limit
                                                                                           the visibility of new
                                                                                           domains until they
                                                                                           establish
                                                                                           trustworthiness.



​




                                                         104

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Strategic Imperatives in a Post-Leak World
The revelations from the Google Warehouse leak are not merely academic.

They demand a fundamental re-evaluation of digital strategy. The era of focusing on
narrow, technical SEO tactics, or any SEO tactics, in a vacuum is over.

The new playbook requires a holistic, integrated approach where brand marketing,
user experience design, and content strategy are no longer separate disciplines but
core components of a successful search presence.

The leak provides the blueprint for four key strategic imperatives.

Strategy 1: Brand is the Ultimate Ranking Factor
The single most important strategic takeaway from the leak is that a strong brand is
the most durable competitive advantage in search.

The documentation connects the dots between the abstract concept of brand equity
and the concrete metrics Google uses to rank websites.

The existence of siteAuthority confirms that Google measures a domain's overall
reputation.

The dominance of NavBoost proves that Google rewards what users click on and
trust.

The ultimate expression of this trust is when a user bypasses generic queries
and searches directly for a brand name.

This action is the strongest possible signal of authority and relevance. The leak
confirms what many have long argued: search is, and always has been, a branding
channel.




                                                        105

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




This understanding necessitates a shift in marketing priorities and resource allocation.

  ●​ Actionable Recommendations:
       ○​ Organizations must rebalance their marketing investments, moving beyond
          a narrow focus on technical SEO and link acquisition to embrace broader
          brand-building initiatives. This includes public relations, advertising that
          drives brand recall, community engagement, and establishing genuine
          thought leadership in their industry.
       ○​ Success measurement must evolve. While non-branded keyword rankings
          remain important, a key performance indicator (KPI) should be the growth
          of branded search volume. An increase in users searching directly for a
          company's name is a direct input into the NavBoost system and a clear
          indicator of growing siteAuthority.
       ○​ The strategy should also account for unlinked "mentions." The leak
          contains numerous features that reference entity mentions - the
          appearance of a brand name across the web, even without a hyperlink.
          This suggests that Google tracks these mentions as a signal of prominence
          and authority, similar to how it treats links. I go into Mentions in my article
          on Hobo Web - Mentions: The New Economy in the Age of AI Overviews?

Strategy 2: User Experience is Undeniably the New Core SEO
The leak effectively dissolves the wall between user experience (UX) design and
search engine optimization.

The NavBoost system, with its detailed click metrics, makes the user the ultimate
arbiter of a page's quality.

A poor user experience, which causes a user to quickly click the "back" button and
return to the search results - a behavior known as "pogo-sticking" - is a direct,
negative ranking signal registered as a badClick.

Conversely, an experience that is so helpful and satisfying that the user ends their
search journey on that page generates a lastLongestClick, one of the most powerful
positive signals.




                                                        106

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Therefore, optimizing for the user is optimizing for Google's most important systems.

  ●​ Actionable Recommendations:
       ○​ Invest significantly in foundational UX elements. This includes fast page
          load speeds, intuitive site navigation, and flawless mobile usability. The
          primary goal is to remove all friction that might prevent a user from
          accomplishing their task and send them back to the search results.
       ○​ Frame all content creation around the principle of profound user
          satisfaction. The objective is no longer just to rank for a keyword, but to be
          the definitive answer that ends the user's search.
       ○​ Integrate Conversion Rate Optimization (CRO) directly into the SEO
          workflow. Activities like crafting compelling page titles and meta
          descriptions to maximise click-through rates are no longer just about
          driving traffic; they are a direct ranking activity that feeds the NavBoost
          system. A higher CTR signals to Google that a result is more relevant and
          appealing to users.

Strategy 3 - Build Topical Authority, Not Just Keyword-Optimised Pages
The leak provides a technical look into how Google understands expertise.

It reveals features like siteFocusScore, pageEmbeddings, and siteRadius, which use
sophisticated mathematical representations (vector embeddings) to measure a
website's topical focus.

The siteFocusScore gauges how concentrated a site's content is on a core set of
topics, while siteRadius measures how much a single page deviates from that central
theme.

A page that is topically aligned with its parent site is seen as more authoritative.

This confirms that a scattergun approach to content - chasing disparate keywords
without a coherent strategy - is detrimental.

I have been a proponent for topical authority for many years, with many other SEO
including Koray Tuğberk GÜBÜR.




                                                          107

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




  ●​ Actionable Recommendations:
       ○​ Develop deep and comprehensive "content hubs" or "topic clusters" that
          thoroughly cover a business's core areas of expertise. This demonstrates a
          clear topical focus to Google's systems and establishes the site as an
          authority in its niche.
       ○​ Conduct ruthless and regular content audits. Low-quality, outdated, or
          off-topic pages should be pruned or significantly updated. These pages
          dilute a site's siteFocusScore, drag down its overall siteAuthority, and send
          negative signals to Google.
       ○​ Emphasize and showcase authorship. The leak confirms that Google stores
          author information and may even calculate an authorReputationScore.
          Associating content with named, credible experts who have a
          demonstrated history in a topic strengthens the site's E-E-A-T (Experience,
          Expertise, Authoritativeness, Trust) signals and reinforces topical authority.

Strategy 4: Adopt a Scientist's Mindset - Test, Don't Just Trust
The most significant casualty of the Google leak is trust.

The chasm between Google's public statements and its internal systems has created a
deep and likely permanent credibility deficit in some circles.

For business leaders and marketers, the era of passively accepting Google's guidance
as gospel is perhaps over.

The path forward requires a more empirical and skeptical approach.

The leak provides the technical underpinnings for the strategic advice Google has
been promoting around "people-first" and "helpful content."

The recent, often painful, "Helpful Content Updates" (HCU) can now be understood as
the public-facing application of these internal systems.

The HCU may penalise unhelpful content by measuring NavBoost signals like
badClicks and short dwell times.

It might demote sites that lack focus, which is measured by siteFocusScore. It elevates
sites with strong siteAuthority.


                                                          108

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The leak reveals the enforcement mechanisms behind the advice.

  ●​ Actionable Recommendations:
       ○​ Cultivate a culture of experimentation. The leak's revelations should not be
          treated as immutable laws but as a powerful set of hypotheses. Businesses
          must test these concepts against their own data to see what drives results
          in their specific market.
       ○​ Diversify information sources. Rely on a combination of independent
          research, peer-reviewed case studies from the SEO community, and
          internal data analytics rather than solely on Google's official blogs and
          spokespeople.
       ○​ Invest in sophisticated analytics that move beyond simple rank tracking.
          The focus should be on measuring what Google is now known to measure:
          user engagement metrics, branded search growth, and performance
          across entire topic clusters.

What the Leak Truly Tells Us
The accidental unlocking of Google's Content Warehouse did not provide a simple
cheat sheet for gaming the search rankings.

Instead, it offered something far more valuable: a moment of education.

It forced a fundamental strategic reset, sweeping away years of myth and misdirection
and replacing them with a new, evidence-based understanding of what it takes to
succeed in the world's most important digital marketplace.

The leak precipitated several critical shifts in strategic thinking.

The focus must move:

  ●​ From optimizing individual pages to building domain-level siteAuthority.
  ●​ From targeting isolated keywords to establishing deep topical authority.
  ●​ From pursuing technical tricks to building a durable brand that users know and
     trust.
  ●​ From placing blind trust in Google's public relations to engaging in empirical
     testing and healthy skepticism.




                                                         109

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Ultimately, this event dissolves the artificial silos that have long existed between SEO,
brand marketing, user experience design, and content strategy.

   ●​ To improve siteAuthority, one needs a strong brand.
   ●​ To improve NavBoost scores, one needs a flawless user experience.
   ●​ To improve siteFocusScore, one needs a coherent content strategy.

A successful digital leader can no longer treat these as separate functions; the leak
proves they are all inputs into a single, interconnected system of evaluation.

The ultimate lesson from the unlocked warehouse is that the path to sustainable
success on Google is no longer about trying to reverse-engineer a secret algorithm.

It is about building a business so genuinely valuable, authoritative, and user-focused
that the algorithm, in all its complex, multi-layered glory, has no choice but to
recognize and reward it.

The leak revealed that Google's trillions of calculations are all designed to answer a
few simple but profound questions about every business online:

  1.​ Is it an authority? (Measured by siteAuthority and siteFocusScore)
  2.​ Do its users agree? (Measured by NavBoost and Chrome data)
  3.​ Is it trustworthy and established? (Measured by hostAge and whitelists)

The game has not fundamentally changed. The rules have simply, and finally, been
revealed.

The leak doesn’t confirm everything for me. It cannot, without official Google
confirmation.

But it is a good starting point for SEO geeks.




                                                          110

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Trust in Google’s E-E-A-T, the Helpful Content Update, and the
Disconnected Entity Hypothesis
The evidence is now undeniable. The sworn testimonies from the U.S. v. Google trial
and the internal blueprints from the Content Warehouse leak have provided an
unprecedented, canonical model of Google's ranking architecture.

We know that foundational systems like Topicality (T*) and Quality (Q*) are
deliberately engineered.

We know that a powerful user-behavior engine, Navboost, refines rankings based on
13 months of click data.

And we know that a site-level authority score, long denied, is a reality. But how do
these pieces fit together?

How can a site with strong traditional signals still fail?

Trust is the Lever

Trust has emerged as a central pillar of SEO quality in Google’s guidelines and
algorithms.

In late 2022, Google expanded its well-known E-A-T (Expertise, Authoritativeness,
Trustworthiness) framework to E-E-A-T (adding Experience) and explicitly elevated
Trust as “the most important member of the E-E-A-T family”.

In parallel, Google’s Helpful Content Update (HCU) - now integrated into core
ranking systems - focuses on promoting content that is genuinely helpful, reliable, and
people-first - see hobo-web.co.uk and hobo-web.co.uk.

Based on these observations, I have developed what I call the Disconnected
Entity Hypothesis to explain these site-level quality issues and to explain why
sites lacking clear identity and trust signals have been disproportionately affected by
recent updates - here hobo-web.co.uk - but that is just part of the story for HCU
victims.


                                                          111

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Tom Capper has a nice article on Moz crystalising some of this too which might be
very pertinent, if indeed the vast majority of your traffic is non-branded traffic (which
means, it is at risk).

‘Trustworthiness’ in Google’s Quality Rater Guidelines (E-E-A-T and
Section 2.5.2)

Google’s Search Quality Rater Guidelines (QRG) make it clear that
trustworthiness is the critical factor in determining page quality.

As Google puts it: “Trust is the most important member of the E-E-A-T family because
untrustworthy pages have low E-E-A-T no matter how Experienced, Expert, or
Authoritative they may seem.” - a sentiment made clear in Paul Harr’s SMX
presentation in 2016.

In other words, no amount of expertise or authority can make up for a lack of
trust. But what exactly does “trust” mean in this context?

According to the QRG and Google’s documentation, raters evaluate trust by asking
whether a page is “accurate, honest, safe and reliable.”.

For example, YMYL (Your Money or Your Life) pages (like medical or financial
advice) must be factually accurate and reliable to be considered trustworthy,
e-commerce sites should have secure payment systems and customer service info,
and product reviews should be honest and not merely promotional - see
vertical-leap.uk.

A key instruction in the QRG is that raters should determine who is responsible for a
website and its content.

In fact, Section 2.5.2 of the guidelines - titled “Finding Who is Responsible for the
Website and Who Created the Content on the Page” - explicitly directs raters to look
for information about site ownership and content creators - see hobo-web.co.uk.

This section is essentially about identity transparency.



                                                          112

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google wants raters to find pages like “About Us” or author bios that clearly state who
owns the site, who operates it, and who authored the content.

If such information is missing when it ought to be present, the page (or site) is likely to
be judged untrustworthy.

I emphasised that this seemingly simple requirement is “quietly powerful” - it sits “at
the core of identity transparency” for a site and underpins many other trust-related
criteria.

Also that “a lack of this information when you should have it is at the core of the
Disconnected Entity Hypothesis.” hobo-web.co.uk




                                                          113

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




What “Trust” Signals Do Raters Look For?

Google’s guidelines outline several trust signals that quality raters (and by extension,
Google’s algorithms) consider when evaluating a page or site’s trustworthiness:

   ●​ Clear Website and Content Creator Identity: Raters are instructed to check
      what the website or content creators say about themselves - for instance, an
      About Us page or author profile. They ask: Is it clear who owns the site and who
      wrote the content? developers.google.com If a website is hiding its owners or
      authors without a good reason, that’s a red flag. Google has even said
      “Something that helps people intuitively understand the E-E-A-T of content is
      when it’s clear who created it… We strongly encourage adding accurate
      authorship information.” hobo-web.co.uk. In practice, this means pages should
      display author bylines (where appropriate) that link to a bio, and sites should
      have an accessible About page disclosing the organization or individuals
      behind the site. Hobo Web underlines this point: to comply with section 2.5.2,
      “your site should include: An About Page with legal and editorial ownership, a
      Contact Page with email, phone, and office hours, [and] a declared Site Editor
      or company representative” hobo-web.co.uk. In short - make it immediately
      obvious who is behind the content and website.​

   ●​ Background & Reputation (What Others Say - MOST IMPORTANT): Raters
      also look for independent information about the website or author - such as
      news articles, reviews, references, or forum discussions about them
      vertical-leap.uk. Positive mentions or reviews from credible sources can bolster
      trust, whereas scandals or numerous complaints will hurt. The QRG explicitly
      asks raters to consider “what others say about the website or content creators”
      and whether there is “independent, reliable evidence” of trustworthiness (or
      evidence of untrustworthiness). This means your site’s reputation on the wider
      web (e.g., customer reviews, expert recommendations, BBB listings, etc.)
      matters for trust. Hobo Web’s philosophy aligns with this: I advised webmasters
      to monitor and manage their online reputation, as it feeds into how Google
      assesses E-E-A-T. He notes that brand signals - like quality inbound links or
      mentions on authoritative sites can positively influence how Google treats a
      site, though they cannot compensate for a fundamentally weak or

                                                         114

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   untrusted entity - see hobo-web.co.uk andhobo-web.co.uk.​

●​ Content Accuracy and Transparency: Raters examine the content itself for
   signs of trustworthiness. Is the content factually correct and well-sourced?
   Does it cite evidence or sources for claims? Does it read as honest and
   objective, or deceptive and spammy? One of Google’s self-assessment
   questions for creators asks: “Does the content present information in a way
   that makes you want to trust it, such as clear sourcing, evidence of the
   expertise involved, [and] background about the author or the site…?”
   developers.google.com. Including references, citations, or evidence in your
   content can help establish trust, especially for YMYL topics. Additionally,
   Google encourages transparency about how content was created. In its official
   docs, Google suggests sharing “how a piece of content was produced”, for
   example, if you write a product review, you might disclose how many products
   you tested and how you tested them, possibly with photos as proof
   developers.google.com and developers.google.com. If content is AI-generated
   or heavily automated, Google advises being upfront about it when a user
   might wonder (the recent “Who, How, and Why” guidance)
   developers.google.com, since “lack of authorship transparency is considered
   unhelpful.” hobo-web.co.uk In essence, honesty about your content’s origins
   and your credentials fosters trust. Tip Add AI disclaimers to content that
   requires it.​

●​ User Safety & Site Security: Trust also encompasses user safety, which
   includes having a secure browsing environment. For e-commerce or financial
   sites, this means proper use of HTTPS, secure checkout processes, and clear
   customer service/contact information. Google’s guidelines note, for example,
   that “Online stores need secure online payment systems and reliable customer
   service” as part of trust vertical-leap.uk. If a site asks users to input sensitive
   data, raters will check for things like SSL certificates and other signs the site is
   legitimate and safe. Webmasters should ensure their site is technically secure
   (no malware, HTTPS in place) and provide channels for customer support or
   user inquiries - these are critical trust signals especially on sites that handle
   transactions.​

                                                      115

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




In summary, Google’s conception of “trust” centers on transparency, accuracy,
safety, and reliability.

Make it clear who you are, what your credentials or qualifications are, why users
should trust your content, and how you operate in an honest, user-centric manner.

These qualities are baked into the QRG and influence how Google’s algorithms assess
site quality.




                                                         116

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Domino Effect of Section 2.5.2: Why Identity
Transparency Matters
Section 2.5.2 of the QRG might sound simple, but meeting its criteria creates a
domino effect of compliance across many trust and quality factors. Hobo Web
argues that “getting 2.5.2 right sets the stage for passing nearly every E-E-A-T and
YMYL trust requirement in the guidelines.” hobo-web.co.uk

By ensuring your site clearly discloses ownership and content creators, you inherently
start to satisfy related guidelines about having sufficient contact information, content
creator info, and avoiding the “Lowest Page Quality” triggers that stem from
anonymity or lack of accountability.

Consider the multiple sections of the QRG that are tied into this basic transparency:
there are sections on “About us” and contact info (2.5.3), website reputation
(3.3.1), creator reputation (3.3.4), the definition of High E-E-A-T (Section 7.3)
versus Lowest E-E-A-T (Section 4.5.2), and specific flags for “Inadequate
information about the website or content creator” (4.5.1) and “Deceptive or
misleading website information” (4.5.3).

All of these can be directly or indirectly addressed by doing a thorough job of
disclosing who you are and taking accountability for your content

It’s no surprise that a site lacking an About page, with no author names on articles,
and no way to contact the owner would likely be labeled “Lowest E-E-A-T” in rater
evaluations hobo-web.co.uk.




                                                         117

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




And while rater evaluations don’t directly determine rankings, Google’s engineers
use that data to train the algorithms on what low-quality sites look like
hobo-web.co.uk and hobo-web.co.uk.

     Hobo Web insight: “If you’re serious about trust, rankings, or
     reconsideration requests, start with 2.5.2. It’s not just a box to check - it’s
     the foundation for the rest of your site’s credibility.” In other words,
     establishing trust via clear identity and responsibility is Step 1 for any site
     looking to perform well in Google. Hobo Web notes that complying with
     these transparency guidelines often aligns with legal requirements (e.g.
     publishing a business’s legal name, address, and terms of use) - so a side
     benefit is that you’ll likely be following the law as well. And if you aren’t
     doing these things, your site might not just be missing trust signals in
     Google’s eyes; it might be operating in a gray area legally, which is
     certainly not the kind of site Google wants to send users to
     hobo-web.co.uk.

In short, Google wants 1. trustworthy sites that 2. don’t produce search engine
first content, and a hallmark of a trustworthy site is being forthcoming about who
runs it.

This is the philosophy behind section 2.5.2.

Webmasters should ensure that every page clearly indicates who’s responsible (the
site itself can be considered the “author” if it’s a brand, but even then the brand’s
legal entity should be clea rhobo-web.co.uk).

Common best practices include an About Us page with company details, author
pages or footers with bios on content pages, easily findable contact information
(email, phone, physical address if applicable), and even schema markup
(Organization, Person) to tie the site to known entities in a machine-readable way
hobo-web.co.uk.

These steps are fundamental to establishing trust.




                                                         118

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Trust as a Ranking Factor - From Quality Guidelines to Core
Updates
Google frequently reminds us that E-E-A-T (and thus trust) is not a direct ranking
factor in itself - there’s no “Trust score” number that gets added to your ranking
calculation.

Instead, Google uses a “mix of factors” as proxies for E-E-A-T developers.google.com.
However, those factors absolutely feed into Google’s ranking algorithms. In particular,
trust-related signals have become increasingly important in core algorithm
updates.

Google’s Helpful Content Update in 2022-2023 underscored this by introducing a
site-wide classifier to identify “unhelpful content”. Initially a separate algorithm, the
Helpful Content system was “rolled into the core ranking systems” in 2024
hobo-web.co.uk.

Google’s Search Liaison Danny Sullivan described the evolution, noting that the
helpful content classifier continually evaluates sites and that “It is now part of a core
ranking system that’s assessing helpfulness on all types of aspects.” hobo-web.co.uk.

In essence, helpfulness (which strongly correlates with trustworthiness and quality) is
baked into core updates. Sullivan explained that starting in 2022, Google tuned its
ranking systems to “reduce unhelpful, unoriginal content” and has “brought what we
learned from that work into the March 2024 core update.” hobo-web.co.uk.

So how does trust play into “helpful content”? Google’s public communications
and QRG make it clear that content cannot be “helpful” if it’s untrustworthy.

A truly helpful piece of content should satisfy the user’s query in a reliable way -
which implies that the content comes from a trustworthy source and is accurate.

Danny Sullivan has emphasized that Google’s algorithms aim to reward content with
strong quality signals (like high E-E-A-T) and that “quality signals - like helpfulness -
matter more” in determining what ranks hobo-web.co.uk.


                                                          119

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Indeed, Google even directs site owners to the Quality Rater Guidelines as a “key”
reference for understanding core update impacts: “If you want a better idea of what
we consider great content, read our raters guidelines.” hobo-web.co.uk

This suggests that many of the things raters look for - especially trust indicators - are
reflected in what the core algorithm rewards or penalizes.

One important point that emerged from the HCU era is that “trust” problems can
outweigh “content” quality.

Many site owners were puzzled when the September 2023 Helpful Content Update
and subsequent spam updates hit their sites, even though their content was
well-written and user-focused.

Google personnel hinted at the answer: often the issue wasn’t the content itself, but
the site’s overall trust signals.

As John Mueller explained in one context, “with the core updates we don’t focus so
much on just individual issues, but rather the relevance of the website overall…
the way you’re making it clear to users what’s behind the content… all of those
things… play in.” hobo-web.co.uk.

In other words, core updates look at the site holistically, including whether the site
is transparent and trustworthy, not just whether a particular blog post reads well.

I observed the same pattern in sites impacted by HCU: “Content isn’t even the primary
problem,” I noted, pointing out that many affected sites had perfectly fine content
hobo-web.co.uk.

Instead, a common thread was weak trust/identity signals - although naturally, we
can’t see any user satisfaction data on this front.

Google’s Danny Sullivan himself acknowledged that some niche sites with great
content were nonetheless not performing well post-HCU, and that Google needed to
“do a better job for [creators] where their hearts are in the right place”
hobo-web.co.uk and hobo-web.co.uk - implying that the current systems might be
unintentionally penalizing some good-content sites because they lack other markers
of establishment or trust.
                                                          120

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Sullivan contrasted “niche blogs built with genuine passion/expertise” with those
“purely for search rankings”, stressing that Google “doesn’t want to reward” the latter
hobo-web.co.uk and hobo-web.co.uk.

One could interpret this as: sites that look “thin” on trust - possibly newer, not
well-established brands, minimal transparency - might get algorithmically
lumped in with low-quality sites, even if their content is decent.

Google’s challenge (and intention) is to separate the truly valuable independent sites
from the fly-by-night spam. But until they perfect that, being clearly trustworthy is a
small publisher’s best defense.

The “Helpful = Trustworthy” Principle

I explain the relationship between “helpful content” and trust: “You cannot have
‘helpful content’ if you fail [the] quality signal - helpfulness - which is on the whole
based on TRUST - no matter how good your content really is, or even how good your
links are.”.

In this analysis, a site that lacks clarity about its authorship, ownership, intent or
value will “completely violate Google’s helpfulness standard,” even if the actual
on-page content is well-written hobo-web.co.uk.

Put plainly, content cannot be deemed helpful if it comes from an untrusted
source. Imagine a website offering medical advice with great detail and correctness,
but nowhere does it state who the doctors or publishers are behind the site - from
Google’s perspective, that site fails the trust test, so the content cannot be fully
valued. Hobo Web gives a vivid example: “Imagine even a site with 100 real doctors on
it but no information about the creator or entity responsible for the site itself. Now in
Google’s world, the doctors aren’t real doctors, as far as Google is concerned… It
doesn’t matter who the authors are... or the quality of their content [if the site doesn’t
establish trust].” This drives home the point that without an identifiable,
accountable entity, your excellent content may as well be anonymous chatter.

From Google’s side, this makes sense.



                                                          121

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google’s fundamental mission is to satisfy users - and part of that is ensuring the
results they deliver won’t mislead or harm those users.

If Google can’t figure out “who is behind this site if something goes wrong?”, it’s less
likely to trust the site.

Why would Google send users to a site “where responsibility for the domain is not
clearly defined or [users can’t] easily interact with [the responsible entity]?”
hobo-web.co.uk.

If something were to go awry- say, bad financial advice leading to loss, or a scam
transaction - Google’s own reputation is at stake for having ranked that site.

Thus, trust and helpfulness are inextricably linked: a site that isn’t demonstrably
trustworthy cannot truly be deemed helpful to users, no matter the intrinsic quality of
its pages.




                                                          122

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Disconnected Entity Hypothesis
The Disconnected Entity Hypothesis (DEH) is my attempt to explain a particular
pattern seen with Google’s Helpful Content and Spam updates, and what I see is a
logical conclusion for sites that fail to meet section 2.5.2 of the Quality Rater
Guidelines..

In essence, the hypothesis posits that Google is classifying some sites as
“unhealthy” or “disconnected” entities when they lack sufficient transparency
and trust signals connecting them to a credible entity hobo-web.co.uk and
hobo-web.co.uk.

These sites then experience ranking declines regardless of content quality, as if
Google has flipped a switch that limits their visibility.

They are “disconnected” in that Google cannot connect the site to a known,
trustworthy entity (or any entity at all), which triggers distrust in the algorithm.

According to Hobo Web’s DEH analysis, if your site was hit by the late 2023 HCU/Spam
updates, “you are at best (and hopefully) a Disconnected Entity and at worst an
Unknown or Spam Entity.”hobo-web.co.uk In other words, Google’s systems might be
treating your domain as an entity with no trust backing or as an outright spam
producer.




                                                          123

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




A “Weak or Disconnected Entity,” is a domain that either:

   ●​ has become disconnected from the entity that originally built its reputation
      (e.g. a domain that expired or was sold - the new owner is essentially a new
      entity with no inherited trust), and/or​

   ●​ has insufficient transparency about the entity responsible for it (e.g. an
      independent publisher site that never clearly identifies its owner or authors),
      and/or​

   ●​ has an entity behind it that is not clearly defined and managed in a way that
      users (and Google) can easily verify hobo-web.co.uk and hobo-web.co.uk.​


Most small independent websites fail in the second or third way: they might build a
nice brand name or have good content, but they neglect to make the actual entity
(person or company) explicitly known and accessible: hobo-web.co.uk.

They focus on building a brand (which used to be enough in SEO), but not a trusted
entity. “Old SEO = brand. New SEO = brand + healthy entity status,”.

A healthy entity status means Google recognizes who you are and has some level of
confidence in you.

One striking claim from my own research is that Google’s ranking systems might
effectively be gating sites based on “Entity Health” before even considering
traditional signals like backlinks or content relevance.

Perhaps a hierarchy: “Google’s ranking systems: Entity Health Status > Links >
Relevance.” hobo-web.co.uk

If your domain is flagged as an “Unhealthy, Unknown or SPAM entity,” then “it
does not matter how good your links or content is” - the site will struggle to rank for
YMYL queries. This aligns with anecdotes of HCU-hit sites where even publishing new
high-quality content or earning new links doesn’t move the needle; the site seems to
have an invisible penalty. In Hobo’s words, “eventually = SEO will just turn OFF” for
disconnected entities until they fix their trust issues.
                                                         124

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The “disconnected” terminology also ties into being “unverified” in Google’s eyes.

Google’s constant endeavor with its Knowledge Graph and entity-based indexing is to
know what entity (organization, person, etc.) is associated with a website and content.

When a site clearly establishes that (via about pages, schema, external profiles, etc.),
it’s “connected” to an entity with some history or reputation.

When a site is vague or anonymous, Google might treat it as an island - a
disconnected entity - and thus risky or low-priority.

It’s worth noting that Hobo Web differentiates a “Disconnected Entity” from a
full-blown “Spam Entity.” A disconnected entity can be rehabilitated (it’s not
inherently bad; it’s just not well integrated into Google’s web of entity information). In
fact, Google may be giving some sites time to sort these issues out.

Sites that are genuinely spam or malicious likely get hit hardest and may never
recover. Disconnected but otherwise legitimate sites often see a slow decline rather
than an overnight crash - possibly as Google’s algorithms give them months or even
years of declining traffic as a grace period to improve trust signals before
“terminating” them completely hobo-web.co.uk.

This could explain why some sites see gradual drops across multiple core updates. It’s
a hypothesis, but one seemingly backed by the patterns I’ve observed.

So how do you “reconnect” a disconnected entity? Essentially by providing the
signals that Google found lacking - which takes us right back to Section 2.5.2
compliance and E-E-A-T principles. Hobo Web’s case studies showed that applying
“E-E-A-T principles” can revive a disconnected entity’s fortunes.

That means making the site’s real-world identity and trust factors crystal clear:
who runs it, who writes it, why it’s credible, etc. Anderson asserts, “when you take a
Disconnected Entity and reconnect it using E-E-A-T principles, it works!” - although -
that may not be the case if publishing search engine first content. In fact, in my
conclusion I state robust entity health - which consists of clear identity,
transparency, and verified expertise - is now crucial for sustainable SEO
success.

                                                          125

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google is increasingly prioritizing “entity trustworthiness,” making “clear and
verifiable entity definitions essential” if you want to rank well long-term
hobo-web.co.uk.

Let’s tie this explicitly to trust: The Disconnected Entity Hypothesis basically
highlights lack of trust (in the form of unclear entity information) as a primary reason
for sites being algorithmically downgraded. Anderson says it plainly: “A Disconnected
Entity by definition, is Unhelpful Content, and ... since Trust is the most important
factor of E-E-A-T, then it looks to me as if a lack of trust is the biggest lever (at
least one of them) to decimate anyone’s rankings.” hobo-web.co.uk.

In HCU and spam update aftermath, many affected sites were those with weak
“About” sections, no author info, or operating under brand names with no public face.

These sites got swept up as collateral damage in Google’s fight against low-quality
content. DEH suggests Google needs websites to prove their trustworthiness as
entities to avoid being misclassified.

From Google’s perspective, this isn’t punitive - it’s protective. Google is trying to
safeguard users (and its own reputation) by not surfacing content from entities it
deems untrustworthy or anonymous.




                                                          126

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Trust Signals vs. “Disconnected” Sites

It’s illuminating to map how Google’s trust signals correspond with the DEH
framework:

   ●​ Ownership & Author Transparency: Google: “Who is responsible” (Section
      2.5.2), clear author bylines, about pages hobo-web.co.uk and
      developers.google.com. Hobo: make ownership and editorial control explicit
      (full company name, real individuals, site editor, etc.) hobo-web.co.uk. DEH says
      lack of this is the definition of a disconnected (untrusted) entity
      hobo-web.co.uk. Action: Add detailed About Us, author bios, and don’t hide
      who’s behind the site.​

   ●​ Contact and Accountability: Google: Sites should have contact info and
      customer support info where appropriate (especially for YMYL and commerce)
      vertical-leap.uk. Lack of contact info is cited in QRG as a sign of low trust.
      Hobo: include Contact pages with email, phone, address, and even business
      registrations if applicable hobo-web.co.uk and hobo-web.co.uk. Action: Provide
      a means for users (and Google) to reach the responsible entity - it signals you
      stand behind your content.​

   ●​ Content Quality & Accuracy: Google: Quality raters check if content is
      accurate, well-sourced, and satisfies user needs. Trust is damaged by factual
      errors or deceptive/misleading information. Hobo: while content quality alone
      isn’t enough if trust fundamentals are broken, you still need to ensure your
      content is actually helpful and correct. Part of building trust is reviewing your
      content for accuracy and updating it. Hobo Web often stresses aligning content
      with E-E-A-T - e.g., demonstrating experience or expertise where you have it
      (don’t write outside your knowledge area just for traffic)
      developers.google.com and developers.google.com. Action: Audit your content
      for any trust killers - remove or fix poorly sourced material, cite evidence for
      claims, and stick to topics where you can provide real value.​




                                                         127

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




●​ External Reputation (“What others say”): Google: Raters search for outside
   reviews or mentions; a trustworthy site usually has some positive footprint
   (unless very new). Hobo: suggests that brand building and links help, but only
   insofar as they indicate a real presence - a brand without an identified entity is
   hollow: hobo-web.co.uk. Action: Encourage satisfied users to leave reviews
   (e.g., on Google Business Profile for local, or industry sites), get mentioned by
   other reputable sites (press, collaborations), and address negative feedback to
   improve your online sentiment. This isn’t quick SEO magic, but over time it
   contributes to Google’s perception of your trustworthiness.​

●​ Technical Trust Signals: Google: Secure your site (HTTPS), avoid excessive or
   disruptive ads, and generally provide a good user experience - these indirectly
   relate to trust (a safe, professional site vs. a shady, ad-ridden one). While not
   explicitly our focus, these are table stakes in site quality. Hobo: in his checklists
   and audits, he also covers these basics. Action: Use HTTPS, have a clean site
   free of malware, and be transparent if you use cookies/trackers (privacy
   policies) - these things show professionalism and care for users.​

●​ Honesty about Content Creation: Google: If you use AI or automate content,
   say so when appropriate; don’t publish ghost-written or AI-written content
   under a fake persona - that’s a trust breaker (and against Google’s spam
   policies if done to mislead): developers.google.com and
   developers.google.com. Hobo: strongly criticizes “faking E-E-A-T” with fake
   author profiles or fake credentials: hobo-web.co.uk. He calls that a “grey…
   Black even!” tactic: hobo-web.co.uk - it may temporarily fool some, but it’s
   unethical and could backfire. Action: Be authentic. If you must use a pen name
   or brand name as author, make sure there’s still an accountable entity behind it
   (and explain the reasoning to users, e.g. “Editorial Team” authorship with a
   team page). Don’t invent a fake expert; either use real experts or at least real,
   transparent identities for your content creators.​




                                                      128

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By comparing these perspectives, it’s clear that Google’s official guidance and
Hobo Web’s SEO philosophy are closely aligned on trust signals. Hobo Web’s DEH
basically says “Do everything Google tells raters a trustworthy site should do - or risk
being filtered out.”

Both emphasise genuine transparency and an investment in credibility.




                                                         129

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Section 2: The Strategic Playbook

Entity SEO: How to Get Your Business Recognised as an Entity by
Google
Over the last 20 years I’ve watched Google shift from merely matching keywords to
truly understanding who and what is behind the content.

This evolution - often summed up by Google’s move to “things, not strings”- means
that optimizing for entities is now essential, especially for small businesses.

What Are “Entities” in Google’s Eyes?

In Google’s terminology, an entity is essentially a uniquely identifiable “thing” or
concept - a person, place, organisation, idea, etc., as understood in Google’s
Knowledge Graph similarweb.com.

Google’s Knowledge Graph is a massive knowledge base of facts about the world,
which helps Google connect the dots between words and the real-world “things” they
represent.

As Google Fellow Amit Singhal described when introducing the Knowledge Graph, it
enables you to search for things (people, places, landmarks, businesses, etc.) that
Google knows about, and get relevant information instantly.

In plain terms, Google wants to know exactly who or what it’s dealing with when
ranking content.

From an SEO perspective, an entity isn’t just a keyword or phrase; it’s an actual
concept with a unique identity in a database.

Dixon Jones puts it nicely: “An entity is a concept in a database with an ID number…
something like the Eiffel Tower can be called ‘Eiffel Tower’ or ‘that big metal thing in
Paris’ in many languages, but they all refer to the same well-recognized entity”
20i.com.

                                                          130

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




In practice, this means Google can recognize your business (or any topic) as a distinct
entity if it finds enough information to confidently identify it in its Knowledge Graph or
other structured data sources.

Why Entity SEO Matters (Especially for Small Businesses)

Entity SEO is about making your business “known” to Google as a trustworthy,
authoritative entity.

This is crucial for earning Google’s trust.

If Google can’t determine that your website is backed by a real, reputable entity, you
risk being overlooked or filtered out.

In fact, I’ve proposed the “Disconnected Entity Hypothesis” to describe what happens
when a site lacks clear entity signals.


                                                          131

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




In short, sites without a robust, verifiable tie to a trusted real-world presence tend to
get shoved to the gutter of search results.

Google won’t outright call it that, but the pattern is clear: websites with “no face” - no
identifiable author, no company info, no external authority signals - struggle to rank.

Google is “hunting down entities it can’t vouch for”, as I have mentioned in my DEH.

On the flip side, if you establish your business as a known entity, Google is more
confident serving your content.

Google spokespersons have repeatedly emphasized the importance of expertise,
authority, and trust (E-E-A-T).

But those signals can only fully apply when Google understands who is providing the
content.

As Jason Barnard explains: “If Google understands who you are, then it can apply
[E-E-A-T] signals fully. If it has to guess who you are, then it can only apply them in a
dampened manner… So being in Google’s Knowledge Graph - meaning Google has
fully understood who you are - is crucial. Entity understanding and your identity in
Google’s mind is the single most important thing in SEO”.

In other words, your great content and links won’t count for nearly as much if Google
can’t tie them to a credible entity.

This is especially true after recent algorithm updates (e.g. Google’s Helpful Content
and spam updates) which appear to favor “entities [Google] can bank on - healthy
ones with a trail of trust” hobo-web.co.uk.

For small businesses, entity SEO can be a competitive equalizer. You might not
outspend big brands on ads or have the highest Domain Authority, but you can carve
out a clear identity in your niche.

By doing so, you signal to Google: “We are a real business with expertise in our area,
and here’s the proof.”

Let’s find out exactly how to do that.
                                                          132

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Practical Steps to Implement Entity SEO for Your Business

Entity SEO may sound abstract, but it comes down to concrete steps and best
practices.

Below I outline how a small business can build its entity presence, with guidance from
Google and experienced SEOs.

Secure Your Business’s Presence in Authoritative Databases

To get recognized as an entity, your business needs to exist in the reference sources
that Google trusts. An expert tip is to go to the primary sources of information for
Google.

Google’s Knowledge Graph draws information from many places - including Wikidata,
Wikipedia, official databases, and high-authority websites searchenginejournal.com.

In a 2018 AMA, Google’s John Mueller confirmed “we use the Knowledge Graph
(which comes from various places, including Wikipedia) to try to understand entities
on a page.” searchenginejournal.com

This tells us that having a Wikipedia page greatly boosts entity recognition (though
Wikipedia has strict notability rules, so not every small business can get one).

If your business is notable enough, earning a Wikipedia entry or at least a mention on
Wikipedia will directly feed Google reliable info about your entity.

However, Wikipedia isn’t the only game in town. Wikidata (the structured database
behind Wikipedia) is another key source; a Wikidata item for your business can
sometimes lead to Knowledge Graph recognition. Other databases matter too. For
example, many companies (especially startups and local businesses) get listed on
Crunchbase, which provides structured profiles of organizations. As one guide notes,
registered businesses usually have a Crunchbase entity… linking to this in your
schema markup can help disambiguate your company inlinks.com and inlinks.com.
Even if Crunchbase isn’t as “powerful” as Wikipedia, it’s a verified source that can
reinforce your business details.


                                                         133

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                     Strategic SEO 2025




Google Business Profile is another must for any local business. If you haven’t
already, claim your Google Business Profile (formerly Google My Business) and fill it
out completely.

This effectively creates an entity entry for your business in Google’s local Knowledge
Graph.

As Dixon Jones emphasizes, “make sure you’re on Google My Business, [it’s]
especially important if you’re a local or regional business.”20i.com

A Google Business Profile listing ties your entity to a physical location, category,
hours, reviews, and more - all signals that you’re a legitimate “somebody” in Google’s
eyes, not a faceless website.

Other authoritative listings: Depending on your niche, seek inclusion in industry
databases, government registries, or respected directories.

For instance, a healthcare clinic might get listed on Healthgrades, or a software
company on G2 Crowd.

The goal is to create a footprint of facts about your business across the web: same
name, address, phone, description, and URL on all these sources.

Consistency is key - you want Google’s crawlers to find the same core information
everywhere, reinforcing the existence of one unambiguous entity (your business).

Designate an “Entity Home” on Your Website and Use Schema Markup

Your own website should clearly represent your business as an entity. Think of your
site (especially your homepage or an about page) as the Entity Home - the definitive
source about who you are.

Jason Barnard advises creating a dedicated page (often the About Us page) that
explicitly describes your organization or yourself, and then corroborating that
across the web - see thebrandserpguy.com.

I agree.


                                                             134

           © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




In practice, on that page you should include factual details: founding date, founders
or key people, location, what you do, awards, etc.

Then, you’ll want to mark it up with structured data.

Use schema.org markup (JSON-LD format) to define your entity for search engines.

For a business, you’d typically use the Organization schema (or LocalBusiness
for local companies) with properties like name, logo, address, founding date, founder,
etc. A crucial property to add is sameAs - here you list URLs that represent the same
entity.

These can include your social media profiles, your Crunchbase page, your Wikipedia
page (if exists), Wikidata entry, Google Business Profile, or any other profile that
Google can reference.

By providing sameAs links, you’re basically telling Google “All these pages refer to the
same entity - me.”

This can greatly help Google reconcile your entity’s identity across its Knowledge
Graph and the web.

One thing to avoid: Don’t use Google’s internal Knowledge Graph IDs (the /g/ or /m/
codes) in your schema.

John Mueller has noted that while it’s technically possible to put a Knowledge Graph
ID as a sameAs URL, it’s not recommended seroundtable.com.

Stick to public URLs that are recognized as authoritative profiles.

After adding structured data, test it with Google’s Rich Results Test or Schema
Markup Validator to ensure it’s error-free.

While schema markup alone doesn’t guarantee a Knowledge Panel or ranking boost,
it’s an important puzzle piece. It provides machine-readable context about your entity
that can validate what Google finds elsewhere.



                                                          135

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Build a Cohesive Topical Content Structure (Site-Wide Entity Optimisation)

Beyond official profiles and schema, your site’s content and structure need to
communicate your topic authority to Google.

Entity SEO isn’t just about metadata; it’s also about how you organize and interlink
your content so Google can see the breadth of knowledge you offer around your
niche.

Start with your site architecture. A logical, hierarchical site structure helps Google
understand the relationship between topics on your site.

In fact, Google’s John Mueller has implied that a well-structured site (with clear URL
hierarchy, breadcrumbs, and internal links) helps Google grasp how things are
connected on your site – see similarweb.com.

For example, if you run a travel blog, a structure like:

bash

CopyEdit

example.com/travel/                                    (general topic page)

example.com/travel/destinations/                                (subcategory)

example.com/travel/destinations/paris                                     (specific entity page)

example.com/travel/destinations/milan                                     (specific entity page)



tells Google that Paris and Milan are entities under the sub-topic “destinations,”
which falls under the broader topic “travel.”

A pyramid-like site architecture (homepage -> categories -> sub-categories -> pages)
isn’t just good for UX, it’s an SEO signal that your content is systematically covering a
topic area. By structuring content into silos or clusters, you essentially map an entity
hierarchy that mirrors real-world relationships.
                                                          136

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Use internal linking generously and meaningfully to reinforce these relationships.

If you have a cornerstone page about “Home Security Systems,” and several blog
posts about related components (cameras, sensors, alarms), link them to the
cornerstone and to each other where relevant.

This network of contextual links essentially says to Google: “All of these pages are
about closely related entities/topics.” The easier you make it for Google to crawl your
whole content network and see the connections, the better it can assign you authority
for the overall subject matter.

Pro tip: Analyze how top competitors structure their sites in your vertical.

If the leading sites in your niche all have sections for certain subtopics, that’s a clue to
what Google expects to see as part of the topic cluster.

You don’t need to copy their navigation exactly, but knowing the common content
segments can guide your own site structure decisions - see.

Optimise On-Page Content with Entities in Mind (Semantic SEO)

Entity SEO also transforms how you approach on-page optimization.

Instead of obsessing over one keyword per page, think in terms of covering a topic
comprehensively and semantically (as I advised in my 2018 ebook).

Here are practical on-page tactics for entity optimization:

   ●​ Use Semantic HTML Structure: Organize your headings (H1, H2, H3…) to
       reflect a logical breakdown of the topic. Your H1 should clearly state the page’s
       primary topic (the main entity or concept). Then use H2s for major subtopics or
       aspects, and H3s for details under those subtopics. This not only helps readers
       but also signals to Google the hierarchy of concepts on the page
       similarweb.com and similarweb.com. For example, if your page is about
       “Healthy Eating Habits” (primary entity), an H2 might be “Foundation of a
       Balanced Diet,” with H3s under it like “Role of Macronutrients” and “Essential
       Micronutrients”similarweb.com. This kind of semantic structuring reinforces the
       context of each section. Pro Tip - Optimise your Headings 2-6 for the USER ON
                                                          137

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   THE PAGE not necessary for Search Engines.​

●​ Cover Related Entities and Contextual Terms: Google’s understanding of a
   page comes from the network of entities mentioned in the text. Ask yourself:
   what related people/places/things would naturally be mentioned when
   discussing this topic? If you have a page about “Bruce Lee,” it’s logical to
   mention “Jeet Kune Do” (the martial art he created) and maybe related figures
   like “Ip Man” or “Chuck Norris.” In fact, if none of those appear, your content
   might be seen as oddly shallow or off-topic. Google’s analysis (with NLP
   algorithms) will look for these semantically connected terms. A quick trick is the
   “People Also Ask” or related searches: these often hint at entities tied to your
   topic. Another is using Google’s Natural Language API demo on a high-ranking
   article in your topic to see what entities Google extracts - this can reveal which
   terms Google strongly associates with the topic.​

●​ Answer User Intents Directly: Because Google’s stated goal is to satisfy
   search intent, make sure your content addresses common questions head-on.
   Often, this means providing a concise answer or definition at the top of your
   page (which can even land you a featured snippet). From an entity standpoint,
   directly defining or explaining the entity early on gives Google a clear signal of
   relevance. For instance, if the page is “What is Entity SEO?”, start by literally
   defining it. This aligns with semantic search principles - you’re matching the
   meaning of the query, not just repeating the query words. It’s good for users
   and for search engines’ understanding. Think DISTANCE “from”.. Distance from
   the TITLE to THE TEXT or from the TITLE to a TABLE.​

●​ Avoid “Keyword Stuffing”; Aim for Meaningful Coverage: In the old days,
   one might just repeat a keyword to death. Now, it’s about coverage of
   concepts. If you write naturally and thoroughly on a subject, you’ll include many
   related entities and phrases without forcing it. One clever exercise (credited to
   Laurent Bourelly) is the “mystery word game”: write a paragraph about your
   topic without naming it explicitly - if it’s still obvious what you’re talking about,
   you’ve included enough relevant context words. This helps ensure you’re not
   relying on one keyword, but truly describing the entity and its connections.​

                                                      138

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




In short, think of each page as building Google’s confidence in your knowledge of the
topic.

The more clearly Google can see which entities your content is about and how they
relate, the more likely it is to deem your page a good answer for relevant queries.

Highlight Authors and Build E-E-A-T Signals Around Your Entity

Google’s focus on entities isn’t just about companies or products - it also extends to
people (like the authors who write your content or the professionals at your
business).

For a small business, this means you should feature real, credentialed people and link
their identities to your brand.

Make sure articles and blog posts on your site have author names (and ideally brief
bios) attached.

If Jane Doe writes an article, have an author page for Jane that describes her
credentials (and mark it up with Person schema).

Google has what’s called an “author entity” for prominent writers and a “publisher
entity” for websites jasonbarnard.com.

You want to feed the right information to those. The more Google can identify “Oh,
this article on example.com was written by Dr. Jane Doe, who also is listed as a
medical expert on HealthSite.org,” the more it can trust the content.

While as a small business you might not have famous authors, you can still establish
your team’s expertise.

Link to their LinkedIn profiles, mention awards or certifications - anything that would
exist in the Knowledge Graph or at least signal REAL credibility.

Likewise, be transparent about your business on your site.

Have a robust About page (as discussed) and show trust factors: physical address,
photos of your team or premises, membership in professional associations, etc.
                                                         139

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




All these contribute to your entity’s trustworthiness.

Google’s own quality rater guidelines (used internally to train algorithms) place huge
importance on E-E-A-T for content creators and websites.

By demonstrating Experience, Expertise, Authority, and Trust, you are essentially
saying “We are a legitimate, experienced entity in our field.”

Remember, as Jason states “Google is a child that wants to learn, and we need to
educate it” about who we are.

So don’t be shy - spell it out. If you’re a licensed contractor with 10 years in business,
say so.

If your CEO has a PhD in the topic you’re covering, make that known.

Think of every detail as another reference point for Google’s entity understanding.

Maintain Consistency and Corroborate Information Across the Web

Finally, Entity SEO isn’t a set-and-forget deal. You need to maintain consistency of
your business information and continually seek corroboration.

Ensure that whenever your business is mentioned elsewhere (news articles,
directories, guest posts, etc.), the facts match what’s on your site. Inconsistent NAP
(Name, Address, Phone) data, for instance, can confuse Google’s local entity
resolution.

Use a service or manually keep track of all your listings and correct any discrepancies.

It also pays to monitor your presence in Google’s Knowledge Graph. You can use
Google’s Knowledge Graph API to query if your entity is recognized (it will return an ID
and some info if found).

Set up Google Alerts for your brand name or key people to catch new mentions -
those might be opportunities to build more entity signals (for example, if a local
newspaper mentions your business, make sure they included a link or at least the
correct name - if not, you might reach out to fix any errors).

                                                         140

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




If you start to see a Knowledge Panel appear for your business on Google searches
(that box with facts on the right side), that’s a great sign - it means Google has
assembled an entity profile for you.

At that point, claim the knowledge panel (there’s a “Claim this knowledge panel”
option) by verifying through your official accounts.

Once claimed, you can suggest edits to it and further ensure accuracy.

Leverage Entity-Oriented SEO Tools (Optional, Advanced)

As a bonus step, consider using tools that specialize in entity SEO to refine your
strategy. For example, InLinks (which Dixon Jones co-founded) can analyze your
content and suggest internal links and schema based on entity detection.

There are also tools to visualize knowledge graphs of your site or to find entity gaps in
your content.

While you can absolutely do Entity SEO manually, these tools can provide
guidance - think of them as training wheels that use NLP APIs and knowledge bases
to make recommendations (like telling you that top-ranking pages about “DIY home
security” all mention a certain concept that you omitted).

Whether or not you use specialized software, the fundamental strategy remains: treat
your website not just as a collection of pages, but as an interconnected web of
known entities and topics.

You’re effectively teaching Google how all the pieces fit together, and why your
business is a trusted authority in your domain.




                                                          141

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Building Trust and Recovering from Trust Deficits
Bringing it all together, here is a checklist of practical steps:

   1.​ Clearly Identify Your Website’s Owner and Purpose: Create a
       comprehensive “About Us” page that states who runs the site (individual
       person, team, or company). Include the business name and any legal entity
       information if applicable (e.g. LLC name, registration number for companies).
       Explain your site’s mission or purpose so users (and Google) understand why it
       exists. This directly addresses QRG Section 2.5.2 by showing who is
       responsible: hobo-web.co.uk. Action: For example, list your company’s full
       name and address, or if you’re a solo blogger, share a bit about yourself and
       your credentials. Hobo Web advises that publishing your “full company name,
       legal registration info, and editorial oversight” is ideal hobo-web.co.uk.​

   2.​ Provide Author By-lines and Bios: Every piece of content that has a
       discernible author should list one. Add by-lines to articles or posts, and link
       each by-line to a dedicated author page or bio section. On the author page,
       include the author’s background, qualifications or experience in the topic, and
       other publications if relevant. Google explicitly “strongly encourages adding
       accurate authorship information” on contenthobo-web.co.uk and
       developers.google.com. This helps satisfy the “Who created the content”
       aspect of E-E-A-T. Action: Even if the “author” is the site name (e.g. a news
       site’s staff), you can state content is produced by the “So-and-so Editorial
       Team” and describe that team. The key is no content should appear
       anonymous - lack of authorship is a trust killer (“Lack of authorship
       transparency is considered unhelpful” in Google’s own words hobo-web.co.uk).​




                                                          142

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




3.​ Show Contact and Customer Support Info: Make it easy for users to contact
    you or get help. At minimum, have a Contact page with a contact form or email
    address. If you are a business, provide a phone number and physical address. If
    you offer products or services, list customer service channels or hours.
    Google’s guidelines associate robust contact info with higher trust (and lack
    thereof with lowest quality) hobo-web.co.uk. Particularly for online stores or
    financial services, users expect to see real-world contact details - it signals
    that there are real people behind the site who will be accountable. Action:
    Ensure your site’s footer or menu prominently links to contact information.
    Respond promptly and helpfully to inquiries - while this is a user experience
    issue, positive interactions can lead to good reviews, bolstering your reputation.​

4.​ Implement Structured Data for Organization/Person: Help Google connect
    the dots about your entity by using schema markup. For example, use
    Organization schema on your About page to provide your company’s details
    (name, logo, address, founders), or Person schema for individual authors
    (name, title, sameAs links to social profiles or Wikipedia, etc.). Shaun Anderson
    notes that structured data “ties it all together” for section 2.5.2 compliance.
    While schema markup itself isn’t a ranking factor, it can feed Google’s
    Knowledge Graph and improve how your entity is recognized. Action: Add
   <script type="application/ld+json"> JSON-LD markup on pages to
   define the organization and author entities. Also link out to official profiles
   (LinkedIn, official social media, professional associations) - these sameAs links
   can reinforce that your entity is real and has external corroboration.​

5.​ Demonstrate Experience and Expertise in Content: This addresses the
    “Experience” and “Expertise” parts of E-E-A-T which support trust. Ensure
    your content showcases first-hand experience where appropriate. For instance,
    if you have a product review, mention that you tested the product and include
    original photos or data - Google says “it can build trust with readers” when
    they see evidence of hands-on experience developers.google.com. If you
    discuss a YMYL topic, consider having an expert review the content (and
    mention that fact). Always strive for factual accuracy and cite sources for
                                                      143

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   important facts or statistics. Not only will this satisfy users, it aligns with
   Google’s helpful content criteria (e.g., “Does your content clearly demonstrate
   first-hand expertise…?”developers.google.com). Action: Add a “References”
   or “Sources” list if applicable, or inline citations for studies and authoritative
   references. Regularly update content that could go out-of-date, especially on
   YMYL topics, to maintain accuracy. Speak in 1st person for Experience and 2nd
   person when speaking to the user (another recommendation from Google’s
   style guide - internal documents).​

6.​ Boost Your External Reputation (Earn Mentions and Reviews): While you
    can’t directly control what others say, you can encourage positive buzz. If you
    are a business, claim your profiles on review platforms (Google, Yelp,
    industry-specific sites) and encourage happy customers to leave reviews. If you
    produce great content, do outreach for backlinks or mentions from respected
    sites in your niche - not for “link juice” per se, but to get your name out there
    as a trusted source. Google’s raters will check for external signals
    vertical-leap.uk, and likely the algorithm does something similar (evaluating
    mentions or links from authoritative sources as a sign of credibility). Action:
    Feature testimonials or case studies on your site; these might get picked up by
    others. Engage in your community (forums, Q&A sites, social media) under your
    real name or brand to build a positive presence. Also, watch for any negative
    content about your site - if it’s legitimate criticism (e.g. unresolved customer
    complaints), address it proactively. A pattern of unresolved complaints can tank
    trust.​

7.​ Ensure Technical Trust Factors are in Place: This includes having a valid SSL
    certificate (HTTPS) - an outright requirement nowadays, as users (and
    browsers) will flag non-HTTPS sites as insecure. Also, display trust seals or
    certifications if relevant (for example, if you have a medical site and you follow
    the HONcode, or if you’re e-commerce and have PCI compliance - let users
    know). Make sure your site doesn’t trigger security warnings (no malware, no
    deceptive download prompts). Use clear design and avoid aggressive ads or
    pop-ups, especially on YMYL pages, as these can be seen as signs of low
    quality or even scaminess hobo-web.co.uk. Action: Perform a security audit. If
    you handle user data, have a privacy policy and terms of service accessible -
                                                      144

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                              Strategic SEO 2025




   beyond legality, it shows transparency about user rights. If you have any
   third-party endorsements or are part of a trusted program (like Google’s News
   Publisher, or a medical accreditation), mention it.​

8.​ Align with Google’s “People-First” Content Guidelines: Ultimately, build
    your site for users, not for Google. Google’s helpful content criteria asks: “Will
    someone reading your content leave feeling like they’ve had a satisfying
    experience?”developers.google.com. If your site is currently filled with
    SEO-driven filler or articles on every trending topic (regardless of your
    expertise), you may have a trust problem. Focus your content scope on what
    you (or your team) truly know and can provide value in. Google is increasingly
    adept at distinguishing content created just to game search from content
    created to genuinely help people. As Danny Sullivan put it, they’re looking to
    reward sites “built on genuine expertise, passion, and trust” and filter out those
    “built purely for flipping and monetization” and hobo-web.co.uk. Action: Do a
    content audit: If you find pages you created solely for traffic with no real value,
    consider pruning or improving them. Stick to a “why” that is user-centric - e.g.
    publishing because you have something unique to share or a problem to solve
    for users. This will naturally enforce a level of authenticity that machines and
    people interpret as trust.​

9.​ If Hit by HCU or a Core Update, Fix Trust Issues First: If you’ve experienced
    a sharp drop in rankings around a known core update or helpful content
    update, and you suspect your site might be seen as a “disconnected” or
    low-trust entity, prioritize the above trust-building steps. Technical SEO or
    minor tweaks won’t bring back traffic if the core issue is Google’s lack of trust
    in your site. John Mueller has advised that sites can “regain traffic by improving
    quality,” but it’s not a simple fix and often requires broad changes that align
    with core updates’ timing hobo-web.co.uk - in other words, you might only
    see recovery when Google reruns its core algorithm, and only if you’ve
    significantly improved what was lacking. Hobo Web’s guidance is even more
    direct: “Any recovery would start with [Section 2.5.2] basic compliance if that is
    what is lacking, because Trust is the overarching signal from E-E-A-T, and
    you can’t have trust without compliance in this area.” hobo-web.co.uk. So,
    start by fixing the trust fundamentals (transparency, E-E-A-T signals) and
                                                      145

    © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




      be patient for Google to reassess. During this time, continue adding genuinely
      helpful content and demonstrate to Google that your site is run by responsible,
      real people.​

   10.​Avoid Black-Hat or Deceptive “E–E-A-T” Hacks: Finally, a caution: Do not
       attempt to fake trust signals. Google and users are getting better at sniffing out
       inauthenticity. For instance, don’t create a bogus persona with a stock photo
       and pretend they are the author of all your articles - that can backfire terribly if
       uncovered (and it’s ethically dubious). Don’t buy fake reviews or engage in link
       schemes claiming endorsements. Such tactics violate Google’s guidelines and
       can lead to manual or algorithmic penalties. As I wryly recounted on the Hobo
       SEO Blog, some SEO advice out there encourages people to “fake E-E-A-T with
       fake profiles,” which he calls “extremely grey… Black even!” hobo-web.co.uk.
       It’s not worth risking your site’s integrity. Instead, invest that effort into real
       improvements and genuinely showcasing the expertise you and your team
       possess.​


By following the steps above, you will be aligning your site with Google’s definition of
a high-trust, high-quality site. This not only helps protect you from the negative
impacts of core updates and HCU changes, but it also provides a better experience to
your users (which is the whole point - “helpful content” is content that serves users
well). In many ways, what’s good for users is now clearly good for SEO.

“Trust” is the linchpin

In Google’s modern search landscape, “Trust” is the linchpin of site quality.

The Search Quality Rater Guidelines place trust at the heart of E-E-A-T, and Google’s
core algorithms - reinforced by updates like the Helpful Content Update - increasingly
filter out sites that lack trust signals. “Trust” from Google’s perspective comes from
transparency, credibility, and a positive reputation. Section 2.5.2 of the QRG, asking
“Who is responsible?”, might be the simplest distillation of what webmasters need to
answer on every site they run.



                                                         146

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




If Google (or users) can’t readily see who stands behind a website and why that site
should be believed, rankings will suffer - as we saw dramatically with many sites in late
2023.

The Disconnected Entity Hypothesis provides a framework to understand these
outcomes: it essentially says trust = connectivity.

A site well-connected” to a known entity (through clear disclosures and associations)
can be trusted; a site that is “disconnected” (operating in the shadows of anonymity
or unclear ownership) is treated as untrustworthy by default, no matter how good its
content might be.

This hypothesis, while not officially confirmed by Google, aligns with everything
Google has been communicating. Google doesn’t want to send users to what it calls
“unhealthy” or questionable web entities. Therefore, our job as webmasters and
SEOs is to present our sites as healthy, trustworthy entities - to prove our
credibility at every opportunity.

To recap, ensuring strong E-E-A-T with an emphasis on Trust involves: clearly
stating who you are (Experience/Expertise are embodied in real people or
organizations), demonstrating authority through real-world credentials or
endorsements, and above all not giving the user or Google any reason to doubt the
honesty, safety, or reliability of your content. I made the Hobo EEAT Tool to help with
this aspect.

It’s an ongoing effort - trust can’t be built overnight, but it can be steadily earned. By
following the principles discussed - drawn from both Google’s guidelines and expert
insights like Hobo Web’s - webmasters can fortify their sites against algorithm
changes and create a better web experience for users. In the end, that is Google’s
goal as well: “to surface great content for users”, which inherently means content
from sources users (and Google) can trust.

By making Trust your SEO north star, you not only please Google’s algorithms, you
build a site that users feel comfortable and confident using - and that is a recipe for
long-term success.



                                                          147

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Key Takeaway

Entity SEO is about aligning your online presence with Google’s understanding of the
world.

For small businesses, it’s one of the most powerful ways to punch above your weight.

By turning your business into a well-defined entity that Google can recognize -
through profiles on trusted platforms, structured data, authoritative content, and
consistent signals of credibility - you make it easy for Google to trust and surface your
site.

In my experience, this approach can protect you from algorithm volatility too.

When Google’s systems “know” you and the value you provide, you’re less likely to be
seen as just another generic site. Instead, you become the go-to entity for your niche
or locale.

And as Google’s algorithms continue to evolve, focusing more on semantic
understanding and less on crude keyword matching, investing in Entity SEO is not just
wise - it’s essential. As Dixon bluntly put it, “if you don’t start thinking around entities,
you’re going to be screwed in the end”.

The good news is that by following the steps in this chapter, you’re well on your way to
entity optimization mastery.

You’re helping Google connect the dots about your business - forging the very
connections that can catapult your small business to big visibility.

Build your entity, nurture it, and let Google see the real-world expertise behind your
website. In the long run, this is how you win in SEO: not by tricking an algorithm, but
by building an identity online that algorithms recognize and reward.




                                                          148

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Search Engine-First Content - The Impact on the Web and Small
Businesses
Recently, we’ve watched Google wage war on what it calls “search engine-first
content.” This reminds me of other wars - like Panda, Phantom or Penguin and
unnatural links.

In simple terms, this is content created primarily to rank high on Google rather than to
genuinely help readers.

We’ve all encountered it - those webpages stuffed with keywords and generic fluff
that leave you unsatisfied.

Google itself defines search engine-first content as material “created primarily to
rank high in search results rather than to benefit the reader” Google.

In contrast, people-first content is made with the audience’s needs in mind.

Why Google Targeted “Search Engine-First” Content

By 2022, frustration with low-value, SEO-driven content had reached a boiling point.

Users were complaining that search results often led to unoriginal pages designed just
to attract clicks, not inform.

A popular hack at the time was appending “reddit” to Google searches to find
authentic user discussions instead of thin blog posts - a clear sign of dissatisfaction.

Google took notice:

 “We know people don’t find content helpful if it seems like it was designed to attract
clicks rather than inform readers,” one Google announcement explained
theverge.com.

In August 2022, Google responded with a major algorithm change pointedly named
the Helpful Content Update.


                                                          149

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Its purpose was to “make sure that unoriginal, low-quality content doesn’t rank
highly in Search”, and instead reward content “made specifically by and for people”
theverge.com.

In other words, Google set out to demote the search engine-first pages clogging its
results - those listicles and aggregators adding no unique value - and promote
websites offering original, satisfying information.

     “Content written for the purpose of ranking in search engines - what you
     might call ‘search engine-first content’ - has been frequently written about
     lately and discussed across social media. In short, searchers are getting
     frustrated when they land on unhelpful webpages that rank well because
     they were designed to rank well.” - Barry Schwartz, Search Engine Land
     (summarizing Google’s view) searchengineland.com

Google’s own spokespeople have been very vocal in promoting this people-first
philosophy.

Danny Sullivan, Google’s Search Liaison, emphasized that Google wants to reward
helpful content above all. “Quality signals - like helpfulness - matter more,”
hobo-web.co.uk.

The message from Google is clear: pages created just to game SEO, without real
expertise or value, are increasingly likely to be filtered out.

Their automated systems now ask, “Who, How, and Why” for each piece of content -
who made it, how it was created, and why (was it to genuinely help users, or just to
rank?) developers.google.comdevelopers.google.com.

Content that fails these sniff tests is prone to be flagged as unhelpful.

In fact, Google explicitly warns creators to “avoid creating search engine-first
content” and provides self-assessment questions as red flags.

For example: “Is the content primarily made to attract visits from search
engines?”; “Are you using extensive automation to produce content on many
topics?”; “Are you mainly summarizing what others have said without adding much
value?” developers.google.comdevelopers.google.com.
                                                         150

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Answering yes to questions like these is a sign that a page was made for search
rankings rather than readers - precisely what Google’s new updates target.




                                                         151

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Helpful Content Update (HCU)




Google’s Helpful Content Update (HCU) launched in late August 2022 and has been
updated several times since. It marked one of the most significant search algorithm
changes in a decade searchengineland.com. To understand its impact, let’s look at the
key milestones of HCU and what each brought:

   ●​ August 2022 - Initial Launch: Rolled out for English searches globally. This
      update introduced a new site-wide ranking signal to identify and devalue
      websites with a high amount of unhelpful, “search engine-first”
      contentsearchengineland.com and searchengineland.com. Google’s goal was
      to “help searchers find high-quality content” by rewarding pages “written for
      humans” and downgrading those written solely to ranksearchengineland.com
      and searchengineland.com. Early communication from Google stressed that
      this was an “ongoing effort” to reduce low-quality, unoriginal content on the
      web searchengineland.com.​

   ●​ December 2022 - Global Expansion: The helpful content system was
      expanded beyond English to cover all languages globally hobo-web.co.uk.
      Google also improved its classifier’s ability to detect low-quality content.
      (Notably, this December HCU update took 4–5 weeks to fully roll out, indicating
      the scale of the change.) hobo-web.co.uk. By this point, webmasters in all
                                                         152

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




      regions needed to ensure their sites weren’t filled with SEO-first filler content.​

   ●​ September 2023 - Refinements: Google rolled out another HCU update with
      some important tweaks. It clarified that using AI or automation without
      oversight could be risky, and warned against posting content from third parties
      “without adding value” - reinforcing that simply republishing or aggregating
      content isn’t considered helpful hobo-web.co.uk. Google also provided more
      guidance on recovery, acknowledging site owners’ concerns. (Interestingly,
      around this time Google softened its stance on AI-generated content if it was
      helpful and met their guidelines, focusing less on the how and more on the
      quality of the result.)​

   ●​ March 2024 - Integration into Core Algorithm: A major turning point came
      when Google fully integrated the helpful content system into its core ranking
      algorithm hobo-web.co.uk and hobo-web.co.uk. HCU was no longer a one-off
      update but an always-on part of how Google ranks pages. Google’s search
      engineers reported that these efforts had already led to a significant purge of
      unhelpful material online - roughly a 40–45% reduction in low-quality content
      showing up in search results searchengineland.com and hobo-web.co.uk. In
      short, nearly half of the shallow “made for SEO” stuff had been swept aside
      (according to Google’s measurements) by early 2024. Google also introduced
      new spam-fighting policies in tandem, underscoring that trustworthiness and
      transparency are crucial for content to be deemed helpful hobo-web.co.uk and
      hobo-web.co.uk. Sullivan put it plainly: the helpful content system is now a
      “core ranking system that’s assessing helpfulness” across all types of
      searches hobo-web.co.uk - a permanent part of Google’s DNA moving forward.​


It’s worth noting that HCU was designed as a site-wide signal from the start
searchengineland.com.

If a large portion of your website is filled with unhelpful content made just for search
engines, your entire site can be held back in Google’s rankings.




                                                          153

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




This made the update especially scary for publishers - even your good pages could be
dragged down if Google determined that your site, overall, was guilty of producing too
much fluff: searchengineland.com and searchengineland.com.

Still, the exact mechanics remain opaque. What’s clear is that Google expects a
“people-first” focus site-wide, not just a few high-quality pages masking a pile of
SEO spam.)

Who was most likely to get hit? Google initially pointed out a few content areas that
“may be impacted the most”: online education, arts & entertainment, shopping, and
tech-related content searchengineland.com.

These were niches where, historically, creators churned out articles mainly to rank -
think of thin “how-to” posts, auto-generated lyrics or synopsis sites, product roundup
blogs written by affiliates, etc.

Sure enough, many affiliate marketing sites, generic how-to blogs, and content farms
saw their traffic plummet once HCU rolled out.

Meanwhile, genuine expert sites and community forums (even Reddit threads) began
to rise higher for certain queries, aligning with Google’s goal of surfacing unique
information you haven’t seen before: searchengineland.com and
searchengineland.com. In Google’s eyes, this was a net win for searchers. But for
many site owners, especially small businesses relying on content marketing, the real
story of HCU was just beginning - and it wasn’t a happy one.




                                                         154

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Impact on Small Businesses and Publishers




From my vantage point, with access to hundreds of HCU hit sites, the impact of the
Helpful Content updates on a particular type of site has been devastating.

Almost immediately after the first HCU in 2022, forum threads and social media lit up
with stories of traffic crashes.

Many of these were small or medium-sized sites that had invested heavily in content
creation as a marketing strategy.

                                                         155

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




A lot of affiliate bloggers and niche informational sites (the kind run by solo
entrepreneurs or small teams) were particularly hard hit.

Google effectively declared that much of this content wasn’t up to snuff in terms of
authenticity or value - and the algorithms dealt out harsh punishment in the rankings.

All across the world, small businesses and indie publishers are reeling from HCU’s
effects.

On SEO communities, you’ll find countless reports of sites that lost 20%, 50%, even
90% of their organic traffic after being deemed “unhelpful.” Some were thin-content
sites churning out articles on every trending topic - arguably deserving of a hit.

But others were heartfelt projects by subject enthusiasts or entrepreneurs trying to
share knowledge while earning a living.

This is alarming for web diversity.

Small businesses feel they’re being squeezed out, not necessarily because their
content is bad, but possibly because they lack the brand authority signals that Google
now heavily favors (as I discussed earlier).

Morgan, a creator from the site Charleston Crafted, described the disconnect felt by
many at the Google Creator Summit. She felt Google's representatives acknowledged
the quality of their work but then offered solutions that missed the mark. Morgan
testified, "You have good content. You are good creators…. But they really
wanted to go off on different tangents about setting up a profile or getting you
verified." She expressed her frustration, stating, "I feel they were going down the
wrong trail… a Google Profile, a bio…." This perfectly captures the feeling that
Google was focusing on surface-level identity signals while publishers felt their core
content was being ignored.

It is very worth noticing about that… Google profile a bio…. Comment.




                                                          156

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




A Hard Road to Recovery (Or None at All)
“Fix” is hard to say when it comes to relevancy, but I’d assume bigger changes would
be visible when the next CORE UPDATES happen.” John Mueller, Google

When your website gets dinged by the Helpful Content system, how easy is it to
recover?

By Google’s design, and from what I’ve witnessed, not very easy.

In fact it can signal the death knell for your site.

The collective sentiment of the affected community is powerfully captured in
questions posed in online forums.

One publisher asked, “Assuming a site hit by HCU in 2023 has fixed everything
that caused the sitewide classifier to be applied, what is the timeframe for the
site to start climbing again?” This was followed by the more desperate and widely
shared observation: “We are many that would really love a reply to why not a
single HCU-hit site have begun climbing again and why new articles won’t
rank.”

The “helpful content” classifier that labels a site as unhelpful can stay in effect for
months, or years even after you improve your pages.

Google intentionally made this signal persistent, in order to discourage
quick-and-dirty fixes.

In practice, many site owners hit by HCU have found themselves in a long game of
waiting - or a hopeless spiral of no recovery at all - as I warned a few of them in
November 2024 and in my February 202 article when I first looked into the HCU
impact on small publishers.

Google’s official line is that sites can recover if they significantly improve content
quality, but “returning to pre-update levels isn’t realistic” in many cases.

John Mueller, a Google Search advocate, bluntly stated that if a site was heavily
affected, simply fixing a few things won’t bounce it back to where it was.

                                                          157

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




According to Mueller, these aren’t quick technical penalties you can lift; rather, they
often require fundamental changes to a site’s approach. “These are not ‘recoveries’ in
the sense that someone fixes a technical issue and they’re back on track - they are
essentially changes in a business’s priorities,” Mueller explained hobo-web.co.uk.

In other words, a website that thrived on search engine first content might need to
completely rethink its strategy (and even its business model) to align with what
Google now wants.

That’s a very bitter pill to swallow for a small business that, say, built its audience over
years by writing lots of search-optimised articles.

Google is basically saying: “Your old approach won’t cut it anymore. It’s not just one
thing to fix - you might have to overhaul your whole content philosophy.”

This stance was highlighted in late 2024, when Google took the step of inviting some
affected publishers to a private “Web Creator Summit.”

The company ostensibly wanted to hear out those who had been “strongly impacted”
by HCU and discuss solutions.

Unfortunately, by many accounts, that meeting only heightened publishers’
frustrations.

Several attendees reported that Google’s representatives acknowledged the
publishers had good content, but “Google’s systems could not detect that” and
thus treated them as low-quality mariehaynes.com and mariehaynes.com. Perhaps
most infuriating was the lack of a hopeful timeline: Google offered “little hope… for
recovery any time soon”.

In fact, when one affected site owner asked Google’s Danny Sullivan what he would do
if he had a website hit by the helpful content update, Sullivan’s response was
astonishingly candid.

He said, “I’d do something else in the meantime.” In other words, don’t count on
Google search traffic returning - go work on other channels or projects.


                                                          158

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




According to another attendee, Google’s advice was even more blunt: “If you were hit
by HCU do not expect a recovery anytime soon. Move on.”See Marie Haynes
research here: mariehaynes.com.

You can imagine how demoralizing it is for a small business owner to be told by
Google to essentially give up on the organic search traffic they’d lost.

Telling a business to just forget about that lost traffic (which may have been their main
source of customers), no matter how pragmatic, felt a little tone-deaf.

The data on recoveries backs up Google’s pessimism.

By late 2023, only a minority of sites had seen any rebound. SEO analyst Glenn Gabe
tracked roughly 400 websites that were hit hard by the September 2023 HCU update.

He found that after a full year - which included multiple core updates and another
helpful content tweak - only 22% of those sites had managed even a modest 20% or
greater increase in organic traffic seroundtable.com.

“Some [sites] have shown full recoveries but most did not,”

Gabe reported, noting that “most [sites] saw no lifts at all.” Even among the
“recoveries,” many were partial and volatile (a surge during one Google update, then a
drop off in the next) seroundtable.com.

Full restoration to pre-2022 traffic levels was “an anomaly,” Gabe observed -
essentially a rare lucky case seroundtable.com. And tellingly, a portion of sites
continued to decline even further with subsequent updates seroundtable.com. This
paints a grim picture: once labeled unhelpful, a site often stays in the dog house.

Some webmasters have thrown in the towel, either pivoting their sites to entirely
different topics, or abandoning projects altogether because they “can’t afford to
continue… with no traffic” seroundtable.com.

From Google’s perspective, this slow or non-existent recovery is not a flaw - it’s by
design.



                                                          159

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




They want to permanently discourage the kind of content strategies that HCU
targeted.

But from the small business perspective, it feels like being hit by a truck.

There’s a growing sentiment that Google is abandoning smaller sites.

As one attendee at the summit summed it up, Google basically told them “there’s
nothing wrong with your sites, it’s us” - meaning the search algorithm had changed in
ways that collateral damage was inevitable - and that they (Google) didn’t really know
how to fix it for those edge cases.

That admission, “we can’t give you any guarantees”, coupled with “that is not going to
change”, was a little chilling.

It means that even good content can fall through the cracks of Google’s system, and
Google’s best advice is to keep improving or find other avenues, because the
dominance of “people-first” signals is here to stay.

Highlighting the grim reality of HCU's impact, creator Mike Hardaker stated bluntly,
“We’re now 13 months since the HCU update rolled out, I don’t believe a single
website has recovered.”

This sentiment was echoed by another publisher, who, according to your research,
was told by Google that his "content was not the issue."

This creates a powerful contradiction: if the content isn't the problem and yet no one
is recovering, it points directly to a deeper, more fundamental issue.




                                                         160

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The E-E-A-T Factor
Google also tied the helpful content effort to its broader quality guidelines known as
E-E-A-T (Experience, Expertise, Authoritativeness, Trustworthiness). In Google’s view,
trustworthiness is the foundation of content quality hobo-web.co.uk.

That means who is creating the content, and why, matters a great deal. Pages that
don’t clearly indicate authorship, or sites that hide who is responsible for the content,
inherently lack trust in Google’s eyes and are unlikely to be deemed “helpful”.

This is one reason many small affiliate sites may have gotten hit - they often were
made to look generic, with no real author bios or brand identity, just a factory of SEO
articles.

Google now strongly encourages adding accurate authorship information to establish
trust - for both the site (which can be the Primary Author too) and the creator.

If a website fails to demonstrate transparency about its creators and purpose, “it
completely violates Google’s helpfulness standard - full stop,” as I’ve written
before on hobo-web.co.uk.

In fact, Google’s algorithms can sometimes judge a site as untrustworthy before even
analyzing the specific page content, simply because the site doesn’t meet basic
E-E-A-T markers (for instance, an “About Us” page, author credentials, etc.).

This focus on trust and expertise is something many supporters of HCU applaud - it
raises the bar for professionalism in content.

Small businesses that treated their site as a serious publication, showcasing real
expertise and credentials, tend to fare better than those that cranked out
cookie-cutter blog posts anonymously.

Even some of the initially skeptical SEOs have come around to the idea that Google’s
approach forces better content creation practices.

The era of tricking your way to the top with SEO gimmicks is fading. As an industry,
SEO is (painfully) being reminded that serving the user should be priority number one.

                                                          161

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




There’s a renewed emphasis on first-hand experience in content - something Google
explicitly rewards.

For example, Google hinted that for travel queries, they’d prefer content by people
who have actually visited the places they write about hobo-web.co.uk and
hobo-web.co.uk.

If you’re a small travel business that actually goes on tours and shares unique photos
and stories, you - theoretically - have a better chance now against a faceless travel
affiliate site.

That’s encouraging for genuine small operators, but it's a seismic shift in who is
expected to create content in 2025.

A rule of thumb for me is who created a type of content today, might not be who is
creating that type of content tomorrow.

Furthermore, some observers note that user satisfaction with Google results has
improved after these updates.

Google itself claims that thanks to HCU (and related changes), searchers are 40%
less likely to land on unhelpful pages now searchengineland.com.

And remember that “reddit hack” for getting better results?

Google’s tweaks in late 2022 were seen as “particularly helpful to anyone using the
append ‘reddit’ trick” theverge.com - meaning Google started surfacing more real
user forum content and authentic reviews on its own.

In fact, queries for many products began to automatically show a “Discussions”
section or even Reddit links, indicating Google’s algorithms were actively boosting
community content where appropriate.

We can only trust it has little to do with any financial agreements Google has with
Reddit (although that in itself is a big concern for many).




                                                          162

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Ongoing Debate: The Future of Content and Small Businesses
The advent of the Helpful Content system has undoubtedly reshaped the SEO
landscape.

On the one hand, search engine-first content and non-transparent entities are being
purged from Google SERPs for the betterment of user experience - and that’s hard to
argue against.

Small businesses that have true expertise and that publish high-quality, authentic
content can still thrive; in fact, they have a better shot at standing out now if they do
things right.

On the other hand, many legitimate small publishers have been caught in the
crossfire.

Their plight has been voiced by some of the most vocal critics of HCU.

Content creators at the summit outright told Google that “there was nothing wrong
with our sites” - the content was good, but Google’s algorithms simply didn’t surface
it.

Google’s response has been unsatisfying to say the least, essentially: we’re focused
on what users want, and we’re not turning back.

For small business owners who built their success on Google Search, that feels like
the rules of the game changed suddenly and the referee won’t hear an appeal.

Some critics have gone as far as suggesting ulterior motives.

They point out that Google makes the vast majority of its revenue from advertising.

If organic (free) visibility becomes harder for small businesses, where will those
businesses turn?

Likely to Google Ads, spending money to regain exposure. It’s a cynical take, but not
unheard of.


                                                          163

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Small businesses need to adapt. The age of easy SEO traffic by pumping out lots of
mediocre articles is over.

To succeed now, you truly have to put users first in your content strategy - just as
Google has been preaching.

That means focusing on your niche expertise, demonstrating your experience, being
transparent about who you are, and genuinely answering the questions your audience
has (better than anyone else).

It also means diversifying - relying solely on Google traffic is risky if an algorithm can
wipe you out overnight.

We’ve seen creators like myself start exploring alternatives like social media,
newsletters, or even other search platforms.

Interestingly, as Google squeezes some types of content, users are exploring things
like AI chatbots (ChatGPT) or community sites for answers.

The competition for how people find information is heating up, and Google’s moves
might inadvertently drive some users elsewhere if its results favor only big players or
too many ads.

Google’s crusade against search-engine-first content - embodied by the Helpful
Content Update and related changes - has profoundly impacted the web
ecosystem.

It’s cleaned up a lot of spam and pushed creators to be better, but it has also left
many small businesses feeling like collateral damage.

I want the web to be filled with helpful, authentic content (and I support Google’s goal
there). Yet I’ve also witnessed business owners in distress because an update slashed
their traffic by 80% and they’re not sure how to pay their bills now.

The balance between improving search quality and not stifling the little guys is a
delicate one.



                                                          164

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Google, for its part, appears firm: people-first content is the future, and there’s no
reversal in sight. “I expect if we see incremental improvements, that might be
reflected in incremental ranking changes,” Danny Sullivan said - meaning recovery, if it
comes, will be slow and earned hobo-web.co.uk.

In other words, adapt and improve steadily, or risk fading away.

As we move forward, small businesses and content creators must take these lessons
to heart.

The ones who survive and thrive will be those who genuinely align with what Google’s
trying to reward: unique expertise, trustworthiness, and true helpfulness to users.

It’s not an easy path - and it’s certainly not always “fair” in the short term - but it is
the new reality of SEO we have to deal with.

In my view, search engine-first content had its run (and made plenty of money for
some), but people-first content is the only sustainable strategy now for anyone
who publishes on the web. The only other option I can think of in terms outside of
people-first content, is ironically AI targeted content - Content made for AI with AI -
which I go into in my article: Optimise for the Synthetic Content Data Layer
Opportunity Gap.

The challenge for small businesses is to embrace this ethos without losing their
livelihoods in the transition.

And the challenge for Google is to continue refining its systems so that truly great,
user-focused content - even from the smallest sites - gets the visibility it deserves,
while the cheats and the spam fade into history - while at the same time, deal with the
accusation from many quarters is that it is killing the open web.

The story is still unfolding, and as an SEO, I’ll be watching (and adjusting) every step of
the way.




                                                          165

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Prepare for the era of Zero-Click Marketing
Let’s establish a critical distinction that frames this entire chapter.

The industry uses two related but distinct terms to describe this new world:
"Zero-Click Search" and "Zero-Click Marketing."

Understanding the difference is not just semantics; it's the key to understanding the
strategic and philosophical war being waged for the future of the web.

Zero-Click Search (ZCS): This is the user behavior, the phenomenon itself. It occurs
when a user types a query into a search engine like Google and finds the answer to
their query directly on the search engine results page (SERP), without clicking through
to any external website. This is a reality engineered by the search engines, designed to
provide immediate answers and a more efficient user experience.

Zero-Click Marketing (ZCM): This is the strategic response. The term, largely
popularised by Rand Fishkin and his team at SparkToro, refers to the practice of
"creating standalone value in the platforms where people hang out, instead of
dropping teasers and links in hopes that people will be compelled to click over to your
site". It is a proactive strategy focused on building brand awareness, influence, and
community directly on platforms like Google, YouTube, Instagram, and others,
independent of referral clicks.

The very language used to describe this trend reveals a deep schism in the SEO
community.

Proponents, who see opportunity, have coalesced around the term "Zero-Click
Marketing."

They frame it as a new channel to be mastered, a new frontier for building influence
and brand affinity.They are actively creating a new discipline around this concept.

Conversely, those who feel victimised by this trend - primarily publishers and
businesses whose models are built on website traffic - almost exclusively use the term
"Zero-Click Searches."


                                                          166

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




For them, it is not an opportunity but a negative outcome being inflicted upon them by
a monopolistic power.

Their language revolves around traffic loss, theft, and anticompetitive behavior.

This linguistic split is the clearest indicator of the two warring camps.

One side is adapting and building a new strategy (ZCM); the other is fighting what it
sees as an existential threat (ZCS).

To understand fully, you must understand both perspectives.

The Mechanics of the Clickless SERP: A Rogues' Gallery of Features
The rise of zero-click searches is not an accident; it's the result of a deliberate,
long-term strategy by Google to transform its SERP from a list of links into a
comprehensive answer platform.
This has been achieved through the introduction and refinement of numerous SERP
features, each designed to satisfy user intent on-page.
 ●​ Featured Snippets (Position Zero): Often considered the original zero-click
    driver, these are the answer boxes that appear at the very top of the SERP, above
    the traditional organic results. They aim to provide a concise, direct answer to a
    user's question, often formatted as a paragraph, a bulleted or numbered list, or a
    table. Google programmatically extracts this information from a single,
    high-ranking webpage that it deems to be a clear and authoritative source for the
    query.
 ●​ Knowledge Panels & Graphs: When you search for a well-known entity - a
    person, a company, a place, or a concept - a large information box often appears
    on the right-hand side of the desktop SERP. This is the Knowledge Panel,
    powered by Google's Knowledge Graph. It aggregates factual information from a
    multitude of trusted sources, like Wikipedia, to provide a comprehensive overview,
    including dates, descriptions, images, and related information, satisfying many
    queries without requiring a click.




                                                          167

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




●​ Direct Answer Boxes: For simple, factual queries like "what is the capital of
   Sweden?" or "how old is the president?", Google often provides an "Instant
   Answer" directly on the SERP. Unlike Featured Snippets, these answers frequently
   come from Google's own licensed data sources or knowledge base and may not
   include a link to a third-party website at all, making them a pure form of zero-click
   result.
●​ The Local Pack: This is arguably the most impactful zero-click feature for small,
   local businesses. For searches with local intent, like "plumber near me" or "best
   pizza in Durham," Google displays a map alongside three business listings. This
   "Local 3-Pack" provides essential information such as the business name,
   address, phone number, hours of operation, and customer reviews. It facilitates
   direct, high-value actions like phone calls or requests for directions, completely
   bypassing the business's website.
●​ People Also Ask (PAA): This feature appears as a box of related questions in an
   accordion-style format. When a user clicks on a question, it expands to reveal a
   short answer - a snippet pulled from a webpage - along with a link to the source.
   A user can explore multiple facets of their topic and get answers to several
   related questions without ever leaving the Google results page.
●​ The New Apex Predator: AI Overviews (AIO): The launch and expansion of AI
   Overviews in 2024 represents the most significant and controversial evolution in
   this trend. Powered by Google's Gemini models, AIOs are AI-generated
   summaries that synthesize information from multiple web sources to provide a
   single, comprehensive, conversational answer at the very top of the SERP. These
   overviews push all other results, including ads and traditional organic links,
   further down the page, dramatically increasing the likelihood of a zero-click
   search.




                                                         168

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Great Divide - A Digital Cold War for the Future of the Web
The current state of zero-click search didn't happen overnight. It is the culmination of
a nearly two-decade strategic pivot by Google.

Two Competing Realities
The reaction to the relentless rise of zero-click search has been anything but uniform.

It has cleaved the digital marketing and publishing worlds in two, creating a "digital
cold war" with no apparent middle ground.

On one side, you have a coalition of publishers, creators, and SEO professionals who
view this trend as a catastrophic attack on the open web's economic model.

On the other, you have a camp of marketers and strategists who see it as the next,
unavoidable evolution of search - an opportunity to be seized, not a threat to be
fought.

To understand the stakes, we must listen to both sides in their own words.

The Critics' Corner: "A Slow, Brutal Asphyxiation of Organic Traffic"
This is the voice of those whose livelihoods are built on website traffic. Their argument
is a potent mix of documented financial injury and pointed accusations of
monopolistic abuse. They see Google not as an innovator improving user experience,
but as a gatekeeper building a "walled garden" to trap users and hoard revenue.8

The Core Argument: Traffic and Revenue Annihilation
The central complaint is that by scraping content and presenting it directly on the
SERP, Google is severing the lifeline of referral traffic that millions of businesses
depend on for advertising revenue, lead generation, affiliate sales, and e-commerce
conversions.

The language used is often stark, describing the situation as a "bloodbath" where
Google is "collecting scalps" and your website becomes just another "unpaid
consultant".

                                                          169

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The most vocal critics are often publishers and the organizations that represent them.
The Independent Publishers Alliance, in a formal EU antitrust complaint, stated that
Google's actions cause "significant harm to publishers, including news publishers in
the form of traffic, readership and revenue loss".

Danielle Coffey, CEO of the News/Media Alliance, captured the widespread feeling of
a broken social contract between the search engine and creators: "Links were the last
redeeming quality of search that gave publishers traffic and revenue. Now Google just
takes content by force and uses it with no return, no economic return".

James Rosewell of the Movement for an Open Web frames it even more bluntly as a
two-front assault: "They steal publishers' content to feed their AI model and then they
use this capability to steal traffic by putting the Overview ahead of the links to the
original content".

Lily Ray, a prominent SEO strategist, offers a stark prediction about the next step in
Google's AI integration. She warns, "If Google makes AI Mode the default in its
current form, it's going to have a devastating impact on the internet. It will
severely cut into the main source of revenue for most publishers... Google holds
all the power." SEO consultant Barry Adams puts a finer point on the outcome,
stating, "I think 'extinction' is too strong of a word for what's going to happen to
websites. 'Decimation' is the right word." He quantifies this by estimating that
clicks from AI Mode could be cut in half, which for many publishers "could be the
difference between having a viable publishing business and going bankrupt."

She’s right.

And if Google AI mode frightens you, wait until you hear what China’s top search
engine Baidu has already moved to.

Gisele Navarro of HouseFresh provides the crucial first-hand perspective of a small
publisher.

She laments the loss of the web's serendipity, comparing AI search to "asking a
librarian for a book, but they just tell you about the book instead." She captures
the feeling of a broken ecosystem, stating, "This feeling of the web being a big
library for all of us, I think that is gone." Her conclusion is a grim one that reflects
                                                          170

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




the feelings of many independent creators: "I think it's going to destroy the open
web as we know it, for sure. It probably already has."

When considering future economic models, Matthew Prince of Cloudflare identifies a
core issue with the "machine web": "Robots don't click on ads."

This simple statement undercuts the entire ad-based revenue model if AI becomes
the primary audience.

Tom Critchlow of Raptive reinforces this by questioning the viability of direct licensing
deals, noting, "I don't think that paying for content like this is a model that will work at
the scale necessary to sustain the web. It's difficult to see how that would work as a
replacement for the decline in clicks."

Another Wound: Loss of Strategic Data
Beyond the immediate financial hit, sophisticated critics point to a more insidious
second-order effect: the loss of invaluable strategic data. SEO professional Michael
Bonfils articulates this perfectly.

The problem isn't just the lost click; it's the lost insight into the customer journey. He
states, "AI Overviews remove visibility into the mid-funnel stage (user research and
comparison)... This makes it difficult to optimise content strategy since marketers
can’t access the conversations users are having with AI".

When a user gets their answer from a Google AI Overview, the business not only loses
a potential customer but also loses the data point that would have informed their
content strategy, product development, and market understanding.

Pushing Back on the "Adapt or Die" Narrative
Critics vehemently reject the notion that this is simply a natural evolution that
businesses must adapt to.

They see this argument as a convenient excuse that absolves Google of its
responsibility.

Sultan Mahmud reinforces this, arguing, "Organic traffic isn't just a vanity metric - it
                                                          171

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




directly impacts revenue and growth. The idea that businesses should just 'adapt' to
losing traffic is flawed".

They contend that this isn't evolution; it's the deliberate dismantling of the open
web's economic engine by a monopolist for its own gain.

The Proponents' Platform: "Influence Has Always Been Better Than
Traffic"
On the other side of the divide are those who advocate for a radical rethinking of
marketing goals (and I am largely on this side).

This camp, championed by figures like Rand Fishkin, argues that fighting against the
tide of zero-click search is futile.

Instead, they propose embracing the new landscape and shifting the focus from
chasing clicks to building influence. It's worth noting this camp I am with, were on the
right side about quality score, too as I pointed out in this book (and my 2018 SEO book
for beginners).

The Core Argument: Shift from Clicks to Influence
The central thesis of this school of thought is that the ultimate goal of marketing has
always been influence, and that website traffic was merely a proxy for that influence -
and often a poor one.

Zero-Click Marketing, therefore, is about achieving that primary goal of influence
directly where the audience is, be it on the SERP, a social media feed, or a podcast.

The most prominent voice for this perspective is Rand Fishkin, co-founder of
SparkToro and someone I’ve followed closely for 2 decades.

He argues that the math has fundamentally changed. In his company's testing, a
social media post with no link gets approximately 10 times the reach of a post that
includes a link.

I think his conclusion is powerful:


                                                          172

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




"I'd rather influence 10 times as many people than I would draw traffic from a small
percentage". He also points out that the decades of link-building spam have made
publications, podcasts, and other media sources more receptive to pitches that don't
ask for a link, making a zero-click approach more effective for modern PR and
outreach.

The question is… what is more powerful for your brand. A direct link or a call to action:
“search for [your offering] on Google”

The Silver Lining: Higher Quality Traffic
A key pillar of the proponent's argument is that zero-click features act as a powerful
filter. They satisfy the low-intent, top-of-funnel informational queries directly on the
SERP.

This means that the users who do end up clicking through to a website are, by
definition, more qualified. They are looking for something deeper than a simple
answer and are more likely to be further down the conversion funnel.

Tim Cameron-Kitchen, founder of Exposure Ninja, sees this as a net positive, stating it
"compress[es] the buyer journey... you may be losing early clicks - but you're gaining
buyer readiness".

The Opportunity: On-SERP Brand Building
Finally, this camp sees immense brand-building value in appearing in zero-click
features. Even if a user doesn't click, seeing your brand's name as the source of an
authoritative answer in a Featured Snippet or AI Overview builds familiarity, credibility,
and trust.

This on-SERP branding ensures that when a user is eventually ready to make a
purchase or take a more significant action, your brand is already top of mind.




                                                          173

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Table 2: The Zero-Click Debate: Threat vs. Opportunity
To crystallize this fundamental conflict, the following table juxtaposes the core
arguments of the two opposing camps.

It serves as an at-a-glance summary of the debate that defines our industry today.

 The Critics' View: An Existential Threat                       The Proponents' View: A Strategic
                                                                Opportunity


 Core Argument: Traffic & Revenue Loss                          Core Argument: Influence Over Clicks



 "A slow, brutal asphyxiation of organic traffic." 9            "I'd rather influence 10 times as many people
                                                                than I would draw traffic from a small
                                                                percentage." 6



 Core Argument: Monopolistic Abuse                              Core Argument: Higher Quality Traffic


 "Google is keeping users in its 'walled garden'                "You may be losing early clicks - but you're
                                       8
 to maximise its own ad revenue."                               gaining buyer readiness." 14



 Core Argument: Loss of Strategic Data                          Core Argument: On-SERP Brand Building


 "The middle part of that, where a person is                    "Visibility in AI Overviews... builds familiarity.
                                       30                       And when that person is ready to take action,
 researching... that data is gone."
                                                                you're already the brand they recognise." 14




                                                          174

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Inside the "Walled Garden" and Google's Official Rationale and
Rebuttals

The View from Mountain View
Through a carefully orchestrated series of official blog posts, executive statements,
and legal filings, Google has built a clear, if highly contested, justification for its
actions.

Understanding this official rationale is crucial, as it reveals the company's strategic
priorities and provides context for the intense backlash from publishers and creators.

The Prime Directive: Enhancing the User Experience
Google's primary public-facing argument is that every change, from the earliest
Knowledge Panels to the latest AI Overviews, is made in the service of the user.

The stated goal is to provide the best, most relevant, and most direct answers as
quickly and efficiently as possible.

In a May 2024 blog post announcing the broad rollout of AI Overviews, the company
claimed,

"People have already used AI Overviews billions of times... They like that they can get
both a quick overview of a topic and links to learn more".34

Google frames this shift not as a radical departure but as a natural evolution of
search, comparing it to past updates like the move to mobile-first indexing.

The objective, they maintain, is to create a "more delightful" web for users across all
platforms.

This positions Google as a user-centric innovator, with the implication that any
negative impact on publishers is an unfortunate but necessary side effect of progress.




                                                          175

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The "More and Better Clicks" Rebuttal
This is Google's most direct - and most contentious - rebuttal to the widespread
publisher concerns about traffic annihilation.

Faced with a mountain of data showing declining clicks, Google has advanced a
counter-narrative: AI Overviews don't kill traffic; they transform it for the better.

The official line, repeated in various forms, is that AIOs actually 1. increase clicks to
a 2. more diverse set of websites.

The same May 2024 blog post boldly asserted, "we see that the links included in AI
Overviews get more clicks than if the page had appeared as a traditional web
listing for that query".

They also claim that people are visiting a "greater diversity of websites" and that the
clicks that do occur are of "higher quality," meaning users are more likely to stay on
the page because the AI has done a better job of qualifying their intent.

This message is echoed by Google's executives. Robby Stein, VP of Product at Google
Search, insisted that the team is "really focused on how we make it easy to click to
sites" and that AIOs will "ultimately create new opportunities for sites to rank".

However, there is a massive, unbridgeable gap between these official claims and the
data reported by virtually every independent analyst.

This isn't a simple disagreement over numbers; it represents a fundamental
breakdown of trust between Google and the web creator community.

While Google claims more and better clicks, third-party data paints a starkly different
picture. A July 2025 report from Similarweb, published in Press Gazette,
documented a 27% year-over-year drop in traffic to the world's top publishers
and a jump in the overall zero-click search rate from 56% to a staggering 69% in
the year following the AIO rollout.

Data from Ahrefs suggests that the presence of an AI Overview can cause the
click-through rate for the number one organic result to plummet by as much as 34.5%.


                                                          176

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




These two sets of "facts" cannot coexist without a hidden variable or a difference in
methodology.

It's plausible that Google's claims are technically true within a very narrow, specific
context.

For instance, an AI Overview might surface a hyper-niche blog for a very long-tail
query that would never have ranked or received a click before, thus increasing the
"diversity" of sites getting traffic.

However, this micro-level gain is a drop in the ocean compared to the macro-level
volume of traffic being lost by established sites on high-volume head terms.

This discrepancy creates a significant credibility gap, fueling the perception among
publishers that they are being misled while their businesses are being systematically
dismantled.

The "Fair Use" Defense: Our Position on Content Scraping
When the conversation shifts from traffic to legality, and accusations of "theft" and
"scraping" arise, Google's defense rests on two pillars: the legal doctrine of "fair use"
and the assertion that it is only using publicly available information.

In a legal filing responding to a class-action lawsuit, Google's position was
unequivocal: "Using publicly available information to learn is not stealing. Nor is it an
invasion of privacy, conversion, negligence, unfair competition, or copyright
infringement". The company argues that this practice is not only legal but essential for
the advancement of generative AI as a technology.

The core of their legal argument is that training an AI model on copyrighted material is
a "transformative" use. Under U.S. copyright law, "transformative use" is a key factor
in determining fair use. Google contends that it is not simply re-publishing the original
work but using it to create something entirely new - an AI model capable of
generating novel responses.

In response to complaints that publishers are not given a choice, Google often points
to existing controls, stating that "Publishers have always controlled how their content
is made available to Google".
                                                          177

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




They refer to technical tools like the robots.txt file and snippet control meta tags
(nosnippet, max-snippet, etc.) as the mechanisms for this control.

However, this argument is seen by many as disingenuous.

Internal documents unearthed during the U.S. antitrust trial revealed that Google
executives explicitly considered giving publishers a true opt-out for AI training but
ultimately decided against it, referring to the all-or-nothing approach as a "hard red
line".

This exposes the "control" they offer as a Hobson's choice: allow your content to
be used to train our AI and appear in our SERPs, or use the controls to block us and
become effectively invisible on the world's largest discovery platform - Google
Search.




                                                          178

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Ripple Effect: Sector-by-Sector Impact Analysis

No Business Left Untouched
The impact of the zero-click revolution is not uniform. It lands with different force
depending on a business's monetization model, its industry, and its historical reliance
on organic search traffic.

For some, it is a minor tremor; for others, it is a category-five hurricane.

In this section, we will dissect the specific pain points and, in some cases, the
unexpected opportunities that have emerged for key sectors of the economy.

The Small Business Squeeze: A Double-Edged Sword
For small businesses, the rise of zero-click search is a profoundly mixed bag,
presenting both a significant threat and a unique opportunity.

The outcome often depends entirely on the nature of the business itself.

The Downside: Traffic Evaporation and Lost Opportunities
The primary negative impact is the straightforward reduction in organic website
traffic.

This is especially damaging for small businesses that have invested heavily in content
marketing to attract and convert customers.

Imagine a niche blogger who has spent years building a repository of expert articles,
monetized through on-site display ads and affiliate links.

When Google scrapes their carefully crafted "how-to" guide for a Featured Snippet or
AI Overview, they lose the very pageview that generates their revenue. If they are not
a well-connected entity, they might also be entirely removed from SERPs.

This scenario limits opportunities for lead generation, diminishes the ability to tell a
compelling brand story, and cuts off the flow of potential customers into the sales
funnel.

                                                          179

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The competition for the remaining clicks becomes fiercer, often favoring larger, more
established brands and straining the limited resources of small business owners.

The Upside: The Power of the Local Pack
Conversely, for local service-based businesses - the plumbers, electricians, dentists,
and restaurants of the world - the Local Pack can be a powerful and net-positive
zero-click tool.

By surfacing crucial information like a phone number, address, business hours, and
customer reviews directly on the SERP, the Local Pack facilitates immediate,
high-intent actions.

A user searching for "emergency plumber" can click-to-call directly from the results
page, completely bypassing the plumber's website.

In this context, the zero-click "conversion" (a phone call) is far more valuable
than a website visit ever was.

Indeed, one study found that businesses listed in the top three of the Local Pack
receive 93% more actions (calls, direction requests) than those ranked just below
them.

This dichotomy reveals a fundamental truth about the impact of zero-click search: it is
dictated almost entirely by a business's monetization model.

The closer the business model is to direct, off-site action, the less damaging -
and potentially more beneficial - the zero-click trend becomes.

For a local restaurant, the click was always just an unnecessary intermediate step to
the real goal: a phone call for a reservation or a customer walking through the door.
Zero-click search makes this journey more efficient.

However, for a business where the on-site engagement is the monetizable event -
such as a publisher relying on ad impressions - the removal of the click is
catastrophic.

Zero-click search is fundamentally short-circuiting the traditional customer journey.
                                                         180

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Whether that is a benefit or a disaster depends entirely on where your business
makes its money.

The E-commerce Conundrum: Collapsing the Funnel
The world of e-commerce is also experiencing a significant transformation, though
here too, the impact is nuanced and depends heavily on the user's intent.

Informational vs. Transactional Queries
The most significant traffic erosion for e-commerce sites is happening at the top of
the funnel, on broad, informational queries (e.g., "what are the best running shoes for
flat feet?").

These are the types of questions that AI Overviews are increasingly adept at
answering.

However, as the user moves down the funnel to specific, transactional queries (e.g.,
"buy Nike Pegasus size 11 red"), the need to click through to a product page to view
pricing, check inventory, see detailed images, and complete a purchase remains
strong.

These bottom-of-funnel searches are far less affected by the zero-click trend.

This phenomenon is illustrated perfectly by a real-world case study from the B2B
commerce SaaS firm, Commercetools.

Their CMO, Jen Jones, reported to Digiday that the company's click-through rate
(CTR) had declined by 20% following the rollout of AI Overviews.

However, their conversion rates held steady. This strongly suggests that the traffic
they lost consisted of early-stage researchers who were being satisfied by the AIOs
and were unlikely to convert anyway.

The zero-click SERP effectively filtered out low-intent users, making their remaining
website traffic more qualified and valuable.

Despite this, e-commerce sites face a long-term threat from Google's own properties,

                                                         181

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




such as Google Shopping carousels and product listing ads (PLAs), which are
designed to facilitate transactions within Google's ecosystem, further reducing the
need to visit a retailer's website.

Adaptation is key, evidently.

The Publishers' Plight: An Existential Crisis
Nowhere is the impact of zero-click search felt more acutely than in the
publishing industry.

For news outlets, media companies, and independent creators whose business
models are almost entirely dependent on website traffic to generate advertising
revenue and drive subscriptions, the rise of on-SERP answers represents a full-blown
existential crisis.

The data is staggering and paints a grim picture. A July 2025 report from Similarweb,
analyzing the year after the broad rollout of AI Overviews, found that the share of
zero-click searches had surged from 56% to 69%.

In that same period, organic traffic to news publishers plummeted from over 2.3 billion
monthly visits to under 1.7 billion - a loss of more than 600 million visits in less than a
year.

A separate report from Enders Analysis concluded that AI Overviews were directly
"cannibalizing website visits".

This is not an abstract, industry-wide trend; it is having a measurable and devastating
effect on individual, well-known brands.

The following table, based on data from Similarweb and analysis by Press Gazette,
shows the concrete impact on several major publishers, comparing their zero-click
rates before and after the widespread launch of AI Overviews.




                                                          182

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




 News Brand              Zero-Click Rate          Zero-Click Rate           Zero-Click Rate          Key Insight
                         (May 2024)               (May 2025 -               (May 2025 - When
                                                  Overall)                  AIO is Present)



 Mail Online             48.0%                    54.9%                     68.8%                    Massive jump in
                                                                                                     zero-clicks, with
                                                                                                     AIOs driving a
                                                                                                     significantly higher
                                                                                                     rate than average.



 People.com              66.2%                    65.6%                     71.2%                    Already high
                                                                                                     zero-click rate
                                                                                                     pushed even higher
                                                                                                     when an AIO is
                                                                                                     present for a query.



 Buzzfeed                52.8%                    60.7%                     69.2%                    Significant increase
                                                                                                     in overall zero-click
                                                                                                     rate, exacerbated by
                                                                                                     the presence of
                                                                                                     AIOs.



 Ouest France            39.8%                    54.5%                     N/A                      Experienced one of
                                                                                                     the largest overall
                                                                                                     increases in
                                                                                                     zero-click rate over
                                                                                                     the year.



 The Independent         52.4%                    63.6%                     N/A                      Among the worst-hit
 (US)                                                                                                dedicated
                                                                                                     newsrooms, with a
                                                                                                     dramatic rise in its
                                                                                                     overall zero-click
                                                                                                     rate.




Source: Data from Similarweb, analysis by Press Gazette.

This data provides the receipts for the publishers' claims of catastrophic harm.

                                                           183

         © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




It shows a direct correlation between the expansion of AI Overviews and the decline in
user clicks.

For these businesses, the symbiotic relationship they once had with Google -
providing content in exchange for traffic - has been unilaterally impacted.




                                                         184

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Legal Battlefield - Copyright, Antitrust, and the Fight for Fair Use

The Web Goes to Court
As the financial and strategic stakes have escalated, the conflict over zero-click
searches has inevitably spilled out of industry blogs and corporate boardrooms and
into courtrooms and regulatory chambers.

This is no longer just a debate about SEO tactics; it is a high-stakes legal war being
fought on multiple fronts, with the outcomes poised to define the fundamental rules
of the digital economy for decades to come.

Danielle Coffey, President of the News/Media Alliance, frames the issue in starkly
moral and legal terms, calling Google's actions "the definition of theft." She argues
that because "the AI answers are a substitute for the original product," Google is
effectively profiting from stolen goods.

She concludes, "They're making money on our content and we get nothing in
return... I don't see that being a business proposition that we would ever
willingly opt into."

The Antitrust Front: Accusations of Monopoly Abuse
The primary legal weapon being wielded against Google by publishers and their allies
is antitrust law.

The core argument is that Google is not merely competing but is illegally abusing its
monopoly power in search to crush competitors and dominate adjacent markets.

The most significant action on this front is the formal antitrust complaint filed with the
European Commission by a coalition of publishers, including the Independent
Publishers Alliance.

The complaint alleges that Google is abusing its dominant market position by using AI
Overviews to siphon traffic, readership, and revenue away from the very publishers
whose content fuels the AI summaries.

A central pillar of this antitrust argument is the "no choice" dilemma.
                                                          185

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Publishers contend that Google has created a coercive environment where they have
no meaningful way to opt out of having their content scraped for AI Overviews without
accepting the catastrophic penalty of being effectively de-listed from Google's core
search results.

This, they argue, is not a legitimate choice but an anticompetitive tying arrangement
imposed by a monopolist, leaving them in a lose-lose situation.

The Copyright Front - The "Fair Use" Standoff
The second major legal battleground revolves around copyright law.

A wave of lawsuits has been filed against AI developers, including Google and
Microsoft-backed OpenAI, by content creators who allege that the unauthorized
scraping of their work to train large language models constitutes mass copyright
infringement.

A prominent example is the lawsuit filed by education technology company Chegg
against Google.

Chegg claims that Google's AI Overviews engage in "unjust enrichment" by using
Chegg's proprietary educational content to build a competing answer engine.

This, they allege, has directly led to a devastating 49% drop in their non-subscriber
traffic, as Google now provides answers that students once had to visit Chegg's site
to find.

Google's defense against these copyright claims is anchored in the legal doctrine of
"fair use".

The company argues that training an AI model on publicly available data is a
"transformative" use of that data.

They contend that they are not simply re-publishing copyrighted works but are using
them as raw material to create something fundamentally new and different: a trained
AI model.

In a court filing, Google's lawyers asserted,

                                                          186

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




"Using publicly available information to learn is not stealing".39

The outcome of these "fair use" cases will have monumental and far-reaching
implications.

This is not a fight over a single search feature; it is a battle to establish the
fundamental property rights for the digital age.

If the courts side with Google and other AI developers, it will legally codify a
business model for the entire AI industry based on the uncompensated ingestion
of the world's public data. This would accelerate the development of AI but could
devastate the creative industries that produce the data in the first place.

Conversely, if the courts side with the publishers and creators, it could force a radical
shift in the economics of AI development.

It might compel AI companies to enter into widespread licensing agreements to use
training data, creating a new revenue stream for creators but potentially slowing down
AI innovation and concentrating power in the hands of a few large media companies
that can strike such deals.

This legal fight will determine who profits from the vast repository of human
knowledge on the internet and will set the precedent for the relationship between
creators and AI for years to come.




                                                          187

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Zero-Click Playbook: A New SEO Framework for 2025 and
Beyond

From Optimisation to Influence
We have dissected the problem, examined the battle lines, and understood the stakes.
Now, we turn to the most critical question: What do we do about it?

The old SEO playbook, which centered on ranking a list of blue links to win a click, is
dangerously obsolete.

Surviving and thriving in the zero-click era requires a new framework, a fundamental
shift in mindset from Search Engine Optimisation to what might also termed Search
Experience Optimisation.

The goal is no longer just to rank; it is to be the answer, wherever that answer is
delivered, and to build influence that transcends the click.

Search Everywhere Optimisation (coined by Ashley Liddell from Deviation) by
another name.




                                                          188

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Pillar 1: On-SERP SEO - Winning on Google's Turf
The first priority is to adapt to the new terrain and maximize your brand's visibility
within the very SERP features that drive zero-click searches. If you can't always win
the click, you must win the impression and own the answer.
 ●​ Mastering Featured Snippets: Earning "Position Zero" is now table stakes for
    informational queries. This requires a laser focus on content structure. Your
    content must provide clear, concise answers to specific questions, ideally within
    the first 40-50 words of a relevant section. Use question-based headings (H2s,
    H3s) that mirror user queries, and leverage formatting like bulleted lists,
    numbered steps, and data tables, which Google's algorithms can easily parse and
    display as a snippet.
 ●​ Dominating the Local Pack: For any business with a physical location or local
    service area, your Google Business Profile (GBP) is your most valuable on-SERP
    asset. It is non-negotiable to fully optimise it. This means completing every single
    field, uploading high-quality photos and videos, actively encouraging and
    responding to customer reviews, utilizing Google Posts to share updates, and
    ensuring absolute consistency of your Name, Address, and Phone number (NAP)
    across the web.
 ●​ Content for AI (Answer Engine Optimization - AEO): To increase the likelihood
    of being cited in an AI Overview in future, your content must be structured for
    machine readability and demonstrate immense authority. This involves two key
    tactics. First, implement structured data (Schema markup) to explicitly tell
    search engines what your content is about (e.g., using FAQ schema, How-to
    schema, or Product schema). Second, build deep topical authority by creating
    comprehensive content hubs that cover a subject from every angle, solidifying
    your site as a go-to resource that AI models can trust. This is about satisfying
    users who find your content on Google. Note that as I write this, indexation is key.
    If you have a site already indexed in Google, it is relatively easy to get into AI
    Overviews quickly by simply blogging.




                                                          189

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Pillar 2: Building a Moat - Becoming Click-Independent
The long-term survival strategy is to build a brand so strong and a community so loyal
that your reliance on Google for traffic diminishes over time.
The goal is to make your business a destination, not just an answer to a search query.
●​ The Primacy of Brand: In a world of commoditized, AI-synthesized answers, a
   trusted brand becomes the ultimate differentiator. The strategic objective should
   be to shift user behavior from generic, unbranded searches (e.g., "how to fix a
   leaky faucet") to branded searches (e.g., "Bob's Plumbing how to fix a leaky
   faucet"). Branded queries are far less likely to be intercepted by zero-click
   features and signal a direct relationship with the customer.
●​ Cultivating Owned Audiences: The most valuable digital assets are the ones
   you control directly. Focus relentlessly on building channels that are immune to
   Google's algorithmic whims. This means growing a robust email list, fostering an
   engaged social media community, or even developing a mobile app. These are
   direct lines of communication to your audience that no search engine can take
   away.
●​ The E-E-A-T Imperative: Experience, Expertise, Authoritativeness, and
   Trustworthiness are no longer just SEO buzzwords; they are the four pillars of a
   defensible brand in the AI age. Both Google's traditional ranking systems and its
   new AI models are explicitly designed to identify and surface content from
   sources that demonstrate these qualities. Publishing original research, featuring
   expert authors, showcasing real-world experience, and earning links and
   mentions from other authoritative sites are crucial signals that build the trust
   necessary to thrive.




                                                        190

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




Pillar 3: Rethinking Measurement - A New Scorecard for Success
If clicks are no longer the primary measure of success, then metrics like click-through
rate (CTR) and organic sessions can no longer be our Guiding Star.
We need a new scorecard, one that reflects the reality of on-SERP influence and
brand building.
 ●​ From Clicks to Visibility: Your analytics focus must shift from tracking traffic to
    tracking visibility. The new key performance indicators (KPIs) include:
     ○​ Impression Share: How often is your brand appearing on the SERP for your
         most important queries? Are your impressions growing even if clicks are flat?
     ○​ SERP Feature Wins: How often are you capturing the on-SERP real estate
         that matters? This means tracking your appearances in Featured Snippets,
         People Also Ask boxes, Local Packs, and, crucially, your citation rate within AI
         Overviews.
     ○​ Brand Search Volume: Is the number of people searching directly for your
         brand name increasing over time? This is one of the strongest indicators that
         your zero-click brand-building efforts are working.
 ●​ Tracking Off-SERP Conversions: It's essential to connect on-SERP visibility to
    real-world business outcomes. For local businesses, this means implementing call
    tracking to measure phone calls generated from your Google Business Profile. For
    all businesses, it means looking for correlations between periods of high SERP
    impressions and spikes in direct website traffic, as users who see your brand on
    Google may later navigate directly to your site.

Key Takeaway – The Future of marketing is Influence
The shift to a zero-click reality is undeniably the most disruptive force to hit the world
of SEO in a generation.

It is challenging long-held assumptions, dismantling established business models, and
forcing a painful but necessary evolution in our industry.

For many, especially in the publishing world, this change feels like an existential threat,
and their fight in the legal and regulatory arenas will have profound consequences for
the entire digital ecosystem.


                                                          191

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




For others, it represents a clarification of purpose - a move away from the vanity
metric of the click and toward the true goal of marketing: building influence and
earning customer trust.

The path forward is not to fight a losing battle against the tide of technological
change, nor is it to passively accept the erosion of our traffic. The future belongs to
those who can master a hybrid strategy.

Dame Wendy Hall, a web pioneer, offers a more philosophical long-term view, stating,
"I'm not worried in the sense that this is all an evolution... If Google goes this
way, some bright spark will come up with a new way of making money." However,
she adds a crucial, sobering caveat: "Something will happen. But I guess for many
people along the way, it will be too late."

Technology advocate Cory Doctorow sees this moment of disruption as an
opportunity, suggesting that user anger could be harnessed to "build a coalition" for
change. He calls the current situation "a crisis we shouldn't let go to waste."

It requires us to become experts at winning on Google's turf, optimizing our content to
be the answer wherever and however it is displayed. Simultaneously, it demands that
we build resilient, independent brands and cultivate direct relationships with our
audiences, creating a moat that no algorithm can cross.

The era of chasing the click is over.

The era of earning influence has begun.




                                                          192

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Section 3 - A Practical Framework for SEO in the
Post-Trial Era
The Practitioner's Dilemma in a Post-Secrecy World
The landmark legal case against Google has fundamentally changed SEO.

As the preceding chapters have detailed, the legal proceedings forced a level of
technical disclosure that clarified over a decade of carefully managed
corporate secrecy.

For the first time, a canonical, evidence-based blueprint of Google's core ranking
architecture - a sophisticated, multi-stage pipeline built on distinct systems like
Topicality (T*), Quality Score (Q*), and the powerful user-behaviour engine,
Navboost - is now public knowledge.

This newfound clarity, however, presents a profound challenge for the modern search
engine optimisation (SEO) practitioner.

Possessing the blueprint is a necessary but insufficient condition for success.

Knowing that Navboost leverages 13 months of aggregated user click data to refine
rankings is one thing; having the methodology to systematically diagnose, influence,
and monitor these long-term user satisfaction signals is another entirely.

Likewise, understanding that Q* functions as a domain-level quality score is a critical
revelation, but it immediately raises the more difficult question of how to build,
measure, and prove the very trustworthiness it is designed to assess..

This gap between abstract knowledge and practical application constitutes the
practitioner's dilemma in this new, post-secrecy world.

The era of chasing vague "best practices" or reacting to the tremors of unconfirmed
algorithm updates is over.

To compete effectively, practitioners require a strategic framework that is as
deliberate, engineered, and evidence-based as Google's own systems.
                                                         193

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The trial did not just reveal what Google uses; it revealed how Google's engineers
think.

Their stated preference for "hand-crafted," debuggable systems - where, in the words
of engineer HJ Kim, "if anything breaks, Google knows what to fix" - over opaque,
end-to-end machine learning models is a profound indicator of their core engineering
culture.

This philosophy, which prioritises control, transparency, and systematic analysis, must
now be mirrored in the approach of those seeking to achieve sustainable visibility
within that very system.

Mastering Foundational Signals (T* & Q*): The Need for a Foundational
Diagnostic Audit
The trial testimony confirmed that Google's ranking process is built upon a foundation
of "hand-crafted" systems, primarily T* for topical relevance and Q* for site-level
quality.

The first and most critical step for any practitioner is to conduct a thorough diagnosis
of how a website performs against these foundational metrics.

A superficial, automated scan is insufficient for this task.

What is required is a deep, manual review rooted in extensive experience. This
human-centric approach directly mirrors the "human-in-the-loop" philosophy that
Google's own engineers value for problem diagnosis.

A comprehensive manual analysis should be rigorously structured, systematically
checking a site against the guidelines sourced directly from Google's official
documentation.




                                                          194

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




This process must be purpose-built to analyse the very components that feed the T*
and Q* systems:
   ●​ Body (On-Page SEO): A meticulous review of site content, information
      architecture, technical implementation, and content development practices is
      necessary to address the "B" signal in T*.
   ●​ Anchors (Link Profile): A technical review must encompass the site's backlink
      profile and internal linking strategy, which are critical inputs for both the "A"
      signal in T* and the PageRank component of Q*.

Unlike automated tools that often produce overwhelming and unactionable data
dumps, a proper manual audit should deliver a prioritised list of the key changes
required to meet Google's standards for a 'high-quality' website, ensuring that
findings are not only delivered but also understood and successfully implemented.

Engineering for User Satisfaction (Navboost): The Role of Longitudinal
Performance Monitoring
The detailed exposition of the Navboost system was perhaps the most significant
revelation of the trial.

This powerful re-ranking system, fueled by 13 months of aggregated user click data,
acts as a massive filter.

The challenge for practitioners is that Navboost's long-term, historical nature makes it
immune to short-term tactical manipulation.

Success requires a strategic, long-term approach focused on genuinely improving
user satisfaction, which in turn demands a way to monitor these trends over time.

This is where Longitudinal Performance Monitoring becomes essential.

Practitioners need an autonomous reporting system that can provide a long-term view
of site performance, ideally integrating directly with the Google Search Console API
and data from web crawlers.

A crucial function of such a tracking system is its ability to identify "Winners and
Losers."
                                                          195

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




By automatically tracking performance shifts over time, practitioners can see which
pages and keywords are gaining or losing traffic.

Analysing changes in clicks, impressions, and average position - and correlating these
shifts with known Google algorithm updates - allows one to effectively
reverse-engineer which pages are earning positive user satisfaction signals and which
are failing.

This provides direct insight into the very metrics, such as "good clicks" versus "bad
clicks" and the "last longest click," which testimony confirmed are central to
Navboost's operation.

By providing a holistic view of algorithm impacts, crawl errors, indexation status, and
Core Web Vitals over time, a data-driven strategy can be formed to systematically
improve the user experience signals that Navboost is designed to reward.




                                                         196

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Building Verifiable Trust (The Q* & E-E-A-T Nexus): The Entity Trust &
Verification Process
The trial's confirmation of Q*, a domain-level quality score, validated the long-held
suspicion that Google assesses site-level trust. This presents a new and critical
challenge: how to systematically build and demonstrate the Experience, Expertise,
Authoritativeness, and Trust (E-E-A-T) that underpin a high Q* score.

To address this, the "Disconnected Entity Hypothesis" provides a useful mental model.

While just a theory, it posits that websites lacking a clearly defined and verifiable
real-world entity behind them are algorithmically classified as "unhealthy" or
"disconnected," leading to a lower Quality Score. With this framework, the vague goal
of "improving E-E-A-T" transforms into a solvable engineering problem that can be
addressed through a clear Entity Trust & Verification Process.

This process begins with a manual audit of a website against the explicit trust signals
outlined in Google's Search Quality Rater Guidelines, which are the human-generated
data used to train and evaluate systems like Q*.

Following the diagnostic phase, an implementation framework is required. This
involves the systematic generation of necessary policy documents, the creation of
transparent author and company information, and the structuring of the site (using
tools like Schema.org markup) to clearly signal who is responsible for the content. This
systematic process builds the very trust signals that the newly confirmed Q* system is
designed to measure, moving beyond generic advice to offer a clear, actionable path
toward verifiable authority.




                                                         197

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Future-Proofing for the AI-Driven SERP: AI-Readiness and Narrative
Defence
The final layer of Google's ranking pipeline, featuring machine learning and
burgeoning generative AI in search results (AI Overviews), represents the next frontier.

The rise of "Answer Engines" demands a new strategic vision, encapsulated in the
concept of the Synthetic Content Data Layer (SCDL) - the dynamic, invisible
knowledge space that AI systems construct about an entity – and complimentary
strategies designed to improve brand visibility across AI Overviews and other Answer
Engines like Chatgpt, too - Mentions, Mentions Mentions - The Age of AI Overviews.

You will hear many acronyms over the next few years, but most of it is just SEO, IMHO.

The goal of optimisation must evolve from being "search-first" (ranking for keywords)
to "entity-first" (becoming the canonical source of truth for your entity).
To achieve this, practitioners need a strategic framework for AI-Readiness and
Narrative Defence.
This methodology involves three core pillars:
   1.​ AI Footprint Audit & Diagnosis: Use generative AI itself to systematically map
       what the SCDL currently says about an entity. This audit reveals the existing
       "fog" - identifying inaccuracies, information gaps, and narrative vulnerabilities.
   2.​ Ground-Truth Synthesis & Verification: Collaboratively extract an
       organization's complete "ground truth" through interviews and analysis of
       internal documentation. Every piece of information must be subject to a
       non-negotiable human verification mandate to ensure absolute accuracy.
   3.​ Strategic Deployment & Narrative Defence: Publish the verified,
       comprehensive content on the organization's own website. Deploy it with
       meticulous information architecture, internal linking, and structured data
       (Schema.org) to make it highly machine-readable. This positions the website as
       the definitive, canonical source that AI retrieval systems will prefer, turning the
       chaotic fog into a clear window controlled by the brand.




                                                         198

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




Conclusion: An Engineered Approach to Modern SEO
The revelations from the U.S. v. Google trial have provided an unprecedented,
evidence-based blueprint of a search engine that is far more systematic and
data-driven than was ever publicly acknowledged.

Success in this new landscape is not a matter of discovering secret loopholes or
employing manipulative tactics, which are destined for failure.

Rather, it demands the adoption of a philosophy that is in direct alignment with
Google's own internal reality.

The path forward requires an engineered approach to SEO.

The insights from this book coalesce into a clear, three-pronged strategy.

First, practitioners must master the foundational signals of relevance and trust (T*
and Q*) through deep, manual analysis. Second, they must engineer for long-term
user satisfaction to succeed within powerful data-driven systems like Navboost.

Finally, they must look beyond the click and build a defensible brand entity that can
thrive in the new era of AI-driven answer engines.

The ultimate takeaway is that SEO is not a static project to be completed but a
dynamic cycle of implementation, measurement, and adaptation.

By understanding the core principles that govern search, the modern practitioner can
move beyond chasing algorithm updates and begin building a durable, authoritative,
and trustworthy digital presence that is designed to be rewarded.



THE END




                                                         199

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




About the Author & The Path Forward

For over two decades, my professional focus has been on
reverse-engineering the principles of search to build sustainable strategies.

This book was written because the ground has clearly shifted. Check out its sister
book Strategic AI SEO 2025.

The evidence from the U.S. v. Google trial and the Content Warehouse Leak provides
us with a new blueprint, and the rise of AI search presents a new set of challenges.

My goal was to provide a durable framework for navigating this new reality - one
based on evidence, not on chasing algorithm updates.

The strategies and hypotheses within these pages, from optimizing for Navboost and
Q* to solving the "Disconnected Entity" problem, form the core philosophy of my
consultancy via Hobo Web.

I believe that future success in search will not come from finding loopholes, but from
building a deep, defensible alignment with Google's preference for quality, trust, and
real-world authority.

If this approach resonates with you, I invite you to learn more. Contact me at
shaun.anderson@hobo-web.co.uk

And remember when it comes to SEO in 2025, it really is as simple as "Mastering
your P*s and Q*s". "Knowing your A, B and Cs", "Dotting your I's" and "Crossing
your T*s".

Thank you for reading.

Shaun

Hobo



                                                         200

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




SEO Evidence Brief

DOJ v. Google – Case No. 20-cv-3010 (Remedial Phase Opinion)

Full source: Court PDF



1. User Data as a Core Input

      “User data is a critical input that directly improves [search] quality. Google
      utilizes user data at every stage of the search process, from crawling and
      indexing to retrieval and ranking.”​
      View in PDF



2. Learning from User Feedback

      “Learning from this user feedback is perhaps the central way that web
      ranking has improved for 15 years.”​
      View in PDF



3. Page Scoring in Crawling

      “Google assigns a score to the pages it crawls, and it endeavors to exclude
      from its web search index pages without value to users, such as
      spam-heavy or pornographic pages. Google also relies on various ‘ranking
      signals’… Among Google’s top-level signals are those measuring a web
      page’s quality and popularity.”​
      View in PDF




                                                          201

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




4. Quality & Popularity Signals in Crawling

      “Quality and popularity signals, for instance, help Google determine how
      frequently to crawl web pages to ensure the index contains the freshest
      web content.”​
      View in PDF



5. PageRank as an Input to Quality Score

      “PageRank… is a single signal relating to distance from a known good
      source, and it is used as an input to the Quality score.”​
      View in PDF



6. Source of Quality Signals

      “Most of Google’s quality signal is derived from the webpage itself.”​
      View in PDF



7. Human Raters as Training Data for RankEmbed

      “The data underlying RankEmbed models is a combination of
      click-and-query data and scoring of web pages by human raters.”​
      View in PDF



8. Direct Role of Rater Scores

      “RankEmbed and its later iteration RankEmbedBERT are ranking models
      that rely on two main sources of data: % of 70 days of search logs plus
      scores generated by human raters…”​
      View in PDF


                                                         202

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




9. Rater Data Improves Long-Tail Search

      “RankEmbedBERT was again one of those very strong impact things, and it
      particularly helped with long-tail queries where language understanding is
      that much more important.”​
      View in PDF



10. Raw vs. Deep-Learning Ranking Signals

      “Google uses signals to score and rank web pages… Signals range in
      complexity. There are ‘raw’ signals, like the number of clicks, the content of
      a web page, and the terms within a query… At the other end… innovative
      deep-learning models… like RankEmbedBERT.”​
      View in PDF​
      Source: Pages 141–142



11. Spam Score as a Quality Signal

      “So, too, does the spam score… Query data is also important to ensuring
      that the search index contains the pages that are responsive to users’
      queries.”​
      View in PDF​
      Source: Page 142




                                                         203

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




12. User Queries as Training Data

      “Every [user] interaction gives us another example, another bit of training
      data: for this query, a human believed that result would be most relevant.”​
      View in PDF​
      Source: Page 95



13. Signals from Device & Context

      “One of the signals that does go into Google Search is: is it a desktop
      query or is it a mobile query.”​
      View in PDF​
      Source: Page 94



14. Chrome Visit Data as a Popularity Signal

      “Popularity is based on Chrome visit data and the number of anchors…
      used to promote well-linked documents.”​
      View in PDF with highlight​
      Source: Page 147



15. Freshness & Long-Tail Queries

      “Comprehensiveness of the index is crucial… Freshness, or the recency of
      information, is an important factor in search quality. GSEs need to know
      how to recrawl sites… especially for long-tail queries.”​
      View in PDF with highlight​
      Source: Page 96




                                                         204

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




16. Scale, Quality, and User Trust

      “The quality of the experience drives retention — users lose trust in a
      search engine when they’re not given accurate and relevant information.”​
      View in PDF with highlight​
      Source: Page 98



17. Glue & Navboost Data (Super Query Logs)

      “Glue is essentially a ‘super query log’… The data underlying Glue consists
      of… (1) the query (text, language, location, device type); (2) ranking info (10
      blue links, triggered search features); (3) SERP interactions (clicks, hovers,
      dwell time); (4) query interpretation (spelling correction, salient terms). An
      important component of the Glue data is Navboost… a ‘memorization
      system’ that aggregates click-and-query data.”​
      View in PDF with highlight​
      Source: Pages 155–157



18. RankEmbed Training Data = Quality Edge

      “The RankEmbed data is a ‘small fraction’ of Google’s overall traffic, but
      the RankEmbed models trained on that data have directly contributed to
      the company’s quality edge over competitors.”​
      View in PDF with highlight​
      Source: Page 162




                                                         205

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




19. User-Side Data Defined (Click & Query Logs)

      “User-side Data is data Google collects from the pairing of a user query
      and the returned response… Examples include: links clicked, hover time,
      pogo-sticking (click back), and dwell time. User-interaction data is the raw
      material Google uses to improve search services.”​
      View in PDF with highlight​
      Source: Page 155



20. Quality Measures & PageRank as Authoritativeness

      “A key quality signal is PageRank, which captures a web page’s quality and
      authoritativeness based on the frequency and importance of the links
      connecting to it… and is used as an input to the Quality score.”​
      View in PDF with highlight​
      Source: Page 146



21. Spam Scores & Device-Type Flags in Indexing

      “The spam score will allow rivals to avoid crawling web pages of low value
      and focus only on those with helpful content. Finally, the device-type flag
      will enable competitors to close the mobile scale gap by identifying and
      focusing on mobile-friendly websites.”​
      View in PDF with highlight​
      Source: Page 148

22. User Interaction Signals (Clicks, Hovers, Pogo-Sticking, Dwell Time)

      “Examples of such data include the web link or vertical information the
      user clicks on, how long a user hovers over a link, and whether the user
      clicks back from a web page and how quickly… User-interaction data is the
      raw material that Google uses to improve search services.”​
      View in PDF with highlight​
      Source: Page 156
                                                         206

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




23. Required Disclosure of Quality, Popularity, Spam & Device-Type Flags

      “For each DocID, a set of signals, attributes, or metadata… including (A)
      popularity as measured by user intent and feedback systems including
      Navboost/Glue, (B) quality measures including authoritativeness, (C) spam
      score, (D) device-type flag…”​
      View in PDF with highlight​
      Source: Page 140



24. Spam & Low-Value Pages Excluded from Index

      “Google assigns a score to the pages it crawls, and it endeavors to exclude
      from its web search index pages without value to users, such as
      spam-heavy or pornographic pages.”​
      View in PDF with highlight​
      Source: Page 145



25. Filtering Training Data for Quality (Duplicates, Spam, Garbage)

      “It is a common business practice to filter your models… remove
      duplicates, remove spam, garbage, inappropriate content. There’s a lot of
      stuff out there that you don’t want to put into your models.”​
      View in PDF with highlight​
      Source: Page 34




                                                         207

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




26. Index Size & the “80–20 Problem”

      “The size of Google’s index gives it a key competitive advantage… Building
      a search index that can answer 80% of queries is attainable, but answering
      the remaining 20% — the long-tail queries — is particularly challenging.”​
      View in PDF with highlight​
      Source: Page 143



27. Popularity Signal from Chrome Visit Data

      “*Popularity signal (P) ‘uses Chrome data’… a measure quantifying the
      number of links between pages and used to promote well-linked
      documents.**”​
      View in PDF with highlight​
      Source: Page 147

28. Popularity as a Top-Level Signal

      “Google has developed various ‘top-level’ signals that are inputs to
      producing the final score for a web page… Among Google’s top-level
      signals are those measuring a web page’s quality and popularity.”​
      View in PDF (Page 141)

29. Popularity & Quality Guide Crawling Frequency

      “Quality and popularity signals, for instance, help Google determine how
      frequently to crawl web pages to ensure the index contains the freshest
      web content.”​
      View in PDF (Page 142)

30. Plaintiffs Sought Disclosure of Popularity (via Navboost/Glue)

      “They are ‘popularity as measured by user intent and feedback systems
      including Navboost/Glue’ and ‘quality measures including
      authoritativeness’ associated with each DocID.”​
      View in PDF (Page 146)
                                                         208

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




31. Popularity = Chrome Data + Anchors

     “Two exhibits suggest that popularity is based on ‘Chrome visit data’ and
     ‘the number of anchors,’ which is a measure that quantifies the number of
     links between pages and is used to promote well-linked documents. See
     PXR0171 at -095 to -098; PXR0356 at -744 (popularity signal (P*) ‘uses
     Chrome data’).”​
     View in PDF (Page 147)

​
31. Popularity & Quality as Fundamental Ranking Signals

“Compare… describing popularity and quality as ‘fundamental ranking signals.’”​
View in PDF (Page 145–146)




                                                        209

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




SEO Community: The People on the Front Lines

The insights in this book build on a foundation of public research, analysis, and debate
by a dedicated community of SEO professionals.

The individuals and resources listed below are on the front lines, consistently tracking,
testing, and deconstructing the complexities of modern search.

Following them is essential for any practitioner who wants to stay at the cutting edge.

The Author

   ●​ Shaun Anderson (Hobo)
        ○​ Affiliation: SEO Consultant at Hobo Web. Over 20 years in SEO.
        ○​ Focus: Author of this book. Creator of the Hobo SEO Dashboard.
        ○​ Find Me: https://www.hobo-web.co.uk/seo-blog/
        ○​ Profile:



A Foundational Contributor: The Work of Bill Slawski (RIP)

More than any other individual, Bill Slawski dedicated his career to demystifying
Google for the public.

He has been titled the “godfather” of search patent analysis, painstakingly reading,
interpreting, and explaining the technical blueprints behind Google's systems.

His immense body of work translated dense engineering concepts into
understandable strategies, forming a foundational bedrock of knowledge for the
entire SEO community.

His legacy is a library of insights that remains essential for anyone seeking to
understand not just what Google does, but how it thinks.

I constantly refer to Bill's work.

   ●​ Find His Work: seobythesea.com




                                                          210

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The SEO Community's Journalist: Barry Schwartz

For well over a decade, Barry Schwartz has served as the indispensable daily
journalist for the SEO industry.

Through his tireless work at Search Engine Roundtable, he chronicles every whisper
and roar from Google - every algorithm update, SERP test, and official statement.

If a Googler makes a critical comment at a conference or online, Barry is there to
report it with speed, context, and accuracy.

He creates the living archive of our industry's history in real-time. Without his
reporting my own research would have been much harder over the years.

   ●​ Find Him: https://www.seroundtable.com
   ●​ Find him on X: https://x.com/rustybrick




                                                          211

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                Strategic SEO 2025




The Pioneers & Veteran Analysts

  ●​ Aaron Wall
        ○​ Affiliation: Founder of SEO Book.
        ○​ Focus: One of the original pioneers of the SEO industry. His site, SEO
           Book, was a foundational training resource for a generation of SEOs. He
           is also a long-standing and sharp critic of Google's monopoly and
           business practices. My favourite SEO, when I was starting out.
        ○​ Find Him: http://www.seobook.com/
  ●​ Jim Boykin
        ○​ Affiliation: CEO of Internet Marketing Ninjas.
        ○​ Focus: A veteran of the SEO industry, known for his long-standing
           expertise in link building and early analysis of Google's ranking
           principles, including TrustRank concepts. My favourite link builder, when I
           was starting out.
        ○​ Find Him: https://www.internetmarketingninjas.com/
  ●​ Rand Fishkin
        ○​ Affiliation: Co-founder of SparkToro.
        ○​ Focus: A leading voice in analyzing the Google API leak and a long-time
           advocate for "Zero-Click Marketing." His work focuses on the strategic
           shift from chasing clicks to building brand influence. Probably my
           favourite marketer over the years.
        ○​ Find Him: https://sparktoro.com/blog/
  ●​ Ammon Johns
        ○​ Affiliation: SEO Veteran & Foundational Thinker.
        ○​ Focus: A pioneering theorist on searcher behavior and marketing and a
           respected peer of Bill Slawski
        ○​ Find Him: https://www.linkedin.com/in/ammonj
  ●​ John Andrews
        ○​ Affiliation: Competitive Webmaster.
        ○​ Focus: A highly respected veteran practitioner known for his deep,
           "in-the-trenches" experience in hyper-competitive niches. His insights
           come from years of direct, high-stakes competition rather than public
           consulting.
        ○​ Find Him: https://x.com/johnandrews
  ●​ A.J. Kohn
        ○​ Affiliation: SEO Veteran.
        ○​ Focus: Credited with identifying and discussing the "pogo-sticking"
           phenomenon as a potential ranking factor as far back as 2008.
        ○​ Find Him: https://www.blindfiveyearold.com/
                                                        212

      © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                Strategic SEO 2025




The Algorithm & Update Trackers

  ●​ Lily Ray
         ○​ Affiliation: VP of SEO Strategy & Research at Amsive Digital.
         ○​ Focus: A prominent researcher and speaker on E-E-A-T, content quality,
            and the impact of AI Overviews on publishers.
         ○​ Find Her: https://x.com/lilyraynyc
  ●​ Marie Haynes
         ○​ Affiliation: Marie Haynes Consulting.
         ○​ Focus: A leading voice on Google's quality updates.
         ○​ Find Her: https://www.mariehaynes.com/marie-haynes-newsletter/
  ●​ Cyrus Shepard
         ○​ Affiliation: Founder of Zyppy SEO.
         ○​ Focus: Known for large-scale SEO experiments and data-driven case
            studies..
         ○​ Find Him: https://zyppy.com/seo/
  ●​ Aleyda Solis
         ○​ Affiliation: International SEO Consultant, Founder of Orainti.
         ○​ Focus: Her insightful interview with Google's Danny Sullivan provided
            crucial context on the HCU and the challenges facing publishers..
         ○​ Find Her: https://www.aleydasolis.com/en/blog/
  ●​ Tom Capper
         ○​ Affiliation: Senior Search Scientist at Moz.
         ○​ Focus: Known for data-driven analysis of Google's updates and search
            trends. His work on the vulnerability of sites reliant on non-branded
            traffic is particularly relevant.
         ○​ Find Him: https://www.tcapper.co.uk/
  ●​ Barry Adams
         ○​ Affiliation: Founder of Polemic Digital.
         ○​ Focus: A veteran specialist in news publishing SEO. His sharp analysis of
            the impact of AI Overviews provides a critical perspective for publishers.
         ○​ Find Him: https://www.polemicdigital.com/blog/
  ●​ Glenn Gabe
         ○​ Affiliation: President of G-Squared Interactive.
         ○​ Focus: Meticulously tracks and documents the impact of Google's core
            algorithm updates with extensive case studies, providing some of the
            best data on site-level recoveries and declines.
         ○​ Find Him: https://www.gsqi.com/marketing-blog/



                                                        213

      © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Technical & Entity Experts

   ●​ Mark Williams Cook
         ○​ Affiliation: Director at Candour.
         ○​ Focus: Known for his technical SEO investigations, including his
            discovery and analysis of the "site_quality" metric via a Google exploit.
         ○​ Find Him: https://withcandour.co.uk/
   ●​ Jason Barnard
         ○​ Affiliation: The Brand SERP Guy at Kalicube.
         ○​ Focus: An expert on brand SERPs, Knowledge Panels, and practical
            Entity SEO. His work is focused on teaching Google who you are.
         ○​ Find Him: https://kalicube.com
   ●​ Dixon Jones
         ○​ Affiliation: CEO of InLinks.
         ○​ Focus: A pioneer in Entity SEO and the practical application of building
            topic authority through structured data and semantic analysis.
         ○​ Find Him: https://inlinks.com/blog/
   ●​ Paul Madden
         ○​ Affiliation: Link Building Expert.
         ○​ Focus: A highly regarded specialist in advanced and scalable link
            building strategies. His insights bridge the gap between theoretical link
            value and practical, large-scale link acquisition.
         ○​ Find Him: https://x.com/pauldavidmadden
   ●​ Koray Tuğberk GÜBÜR
         ○​ Affiliation: Founder of Holistic SEO.
         ○​ Focus: Known for his deep, often theoretical, explorations of advanced
            SEO concepts, including topical authority and semantic search.
         ○​ Find Him: https://holisticseo.digital
   ●​ Darth Autocrat
         ●​ Affiliation: Anonymous SEO Practitioner / X Personality.
         ●​ Focus: A popular X account known for sharp, cynical, and often insightful
            commentary on Google. A wealth of knowledge.
         ●​ Find Them: https://x.com/darth_na/
   ●​ Gagan Ghotra
         ○​ Affiliation: SEO Consultant.
         ○​ Focus: A young advanced technical SEO who often shares insights on X.
            One for the future.
         ○​ Find Him: https://x.com/gaganghotra_



                                                         214

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




The Black Hat Perspective: Understanding the Other Side

“Yet do I fear thy nature; It is too full o' the milk of human kindness To catch the
nearest way” Lady Macbeth

While this book advocates for sustainable, white-hat strategies, a complete
understanding of the search landscape requires acknowledging the black hat
community.

Paying attention to their methods – not to replicate them, but to understand the
vulnerabilities they exploit - offers invaluable insight into what Google is fighting
against and where the algorithm's boundaries lie.

   ●​ Charles Floate
         ○​ Affiliation: SEO Consultant & Public Figure.
         ○​ Focus: A prominent and vocal figure in the grey and black hat SEO world.
            His work provides a transparent (though controversial) look into
            aggressive link-building tactics, PBNs, and exploiting algorithmic
            loopholes.
         ○​ Find Him: https://www.charlesfloate.com/
   ●​ Tehseowner
         ○​ Affiliation: Blackhat
         ○​ Focus: A blackhat who shares insights on algorithm updates. Very
            insightful.
         ○​ Find Him: https://x.com/tehseowner
   ●​ Grindstoneseo
         ○​ Affiliation: Linkbuilder
         ○​ Focus: Another linkbuilder who shares nuggets of wisdom every now and
            again.
         ○​ Find Him: https://x.com/GrindstoneSEO
   ●​ Ralf Christian
         ○​ Affiliation: Blackhat
         ○​ Focus: Another blackhat who shares nuggets of wisdom every now and
            again.
         ○​ Find Him: https://x.com/elralfchristian/




                                                          215

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                 Strategic SEO 2025




The Leakers & Primary Analysts

   ●​ Michael King
         ○​ Affiliation: Founder & CEO of iPullRank.
         ○​ Focus: Collaborated with Rand Fishkin on the initial analysis of the
            Google API leak. A foremost expert in technical SEO, content strategy,
            and "relevance engineering."
         ○​ Find Him: https://ipullrank.com/google-algo-leak
   ●​ Erfan Azimi
         ○​ Affiliation: SEO Professional.
         ○​ Focus: Credited with the initial discovery and responsible sharing of the
            leaked Google Search API documents, which triggered the industry-wide
            analysis.https://x.com/azimi_erfa15609
         ○​ Find Him:

The Official Voices (From Google)

Google spokespeople prefer Linkedin these days, to the town square of X.

   ●​ John Mueller
        ○​ Affiliation: Search Advocate at Google.
        ○​ Focus: For years, he has been the primary point of contact for
            webmasters through office-hours hangouts. His nuanced answers to
            practitioner questions are a massive source of indirect insight.
        ○​ Find Him: https://www.linkedin.com/in/johnmu/
   ●​ Gary Illyes
        ○​ Affiliation: Analyst at Google.
        ○​ Focus: A member of Google's Search team who frequently interacts with
            the SEO community at conferences and online, providing technical
            clarifications on crawling, indexing, and ranking systems.
        ○​ Find Him: https://www.linkedin.com/in/garyillyes/




                                                         216

       © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                     Strategic SEO 2025




References:
 1.​    Hobo Web Service Reviews & SEO Testimonials, accessed July 7, 2025, https://www.hobo-web.co.uk/testimonials/
 2.​    Hobo SEO for Beginners Ebook V5 Review - A Deep Dive - YouTube, accessed July 7, 2025,
        https://www.youtube.com/watch?v=T47s_SpOvoQ
 3.​    What are Google Search Essentials? - Rank Math, accessed July 7, 2025,
        https://rankmath.com/seo-glossary/google-search-essentials/
 4.​    What Is Google Search Essentials? - Ansira, accessed July 7, 2025, https://ansira.com/blog/google-search-essentials/
 5.​    What Are Google Search Essentials? An Introduction to Google's New SEO Guidelines, accessed July 7, 2025,
        https://www.constant-content.com/content-writing-service/2022/10/what-are-google-search-essentials/
 6.​    Google Search Essentials (formerly Webmaster Guidelines ..., accessed July 7, 2025,
        https://developers.google.com/search/docs/essentials
 7.​    Report Spam, Phishing, or Malware | Google Search Central | Support, accessed July 7, 2025,
        https://developers.google.com/search/help/report-quality-issues
 8.​    Spam policies for Google web search - Publisher Policies Help, accessed July 7, 2025,
        https://support.google.com/publisherpolicies/answer/11035931?hl=en
 9.​    Overview of Google's Search Quality Rater Guidelines - Portent, accessed July 7, 2025,
        https://portent.com/blog/seo/googles-search-quality-evaluator-guidelines-for-seo.htm
 10.​   General Guidelines - RaterHub.com, accessed July 7, 2025,
        https://guidelines.raterhub.com/searchqualityevaluatorguidelines.pdf
 11.​   Google Search Quality Rater Guidelines Explained - Vazoola, accessed July 7, 2025,
        https://www.vazoola.com/resources/googles-new-quality-raters-guideline
 12.​   Google E-E-A-T (2024 Ultimate Guide) | Boostability, accessed July 7, 2025,
        https://www.boostability.com/resources/google-e-e-a-t-guide/
 13.​   What is EEAT? Guide to Understanding EEAT SEO - Victorious SEO Agency, accessed July 7, 2025,
        https://victorious.com/blog/what-is-e-a-t/
 14.​   Google E-E-A-T: How to Create People-First Content (+ Free Audit), accessed July 7, 2025,
        https://backlinko.com/google-e-e-a-t
 15.​   What is Google E-E-A-T? Guidelines and SEO Benefits - Moz, accessed July 7, 2025, https://moz.com/learn/seo/google-eat
 16.​   What Is a Sitemap | Google Search Central | Documentation ..., accessed July 7, 2025,
        https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview
 17.​   Hobo SEO Tutorial - A Deep Dive for Beginners - YouTube, accessed July 7, 2025,
        https://www.youtube.com/watch?v=PJxW9zMnAic
 18.​   SEO tutorial for beginners - Hobo Web, accessed July 7, 2025, https://www.hobo-web.co.uk/seo-tutorial/
 19.​   developers.google.com, accessed July 7, 2025,
        https://developers.google.com/search/docs/crawling-indexing/robots/intro#:~:text=A%20robots.txt%20file%20tells,web%2
        0page%20out%20of%20Google.
 20.​   What Is A Robots.txt File? A Guide to Best Practices and Syntax - Moz, accessed July 7, 2025,
        https://moz.com/learn/seo/robotstxt
 21.​   Robots.txt and SEO: Complete Guide - Backlinko, accessed July 7, 2025, https://backlinko.com/hub/seo/robots-txt
 22.​   Robots.txt Introduction and Guide | Google Search Central ..., accessed July 7, 2025,
        https://developers.google.com/search/docs/crawling-indexing/robots/intro
 23.​   Mobile-first Indexing Best Practices | Google Search Central ..., accessed July 7, 2025,
        https://developers.google.com/search/docs/crawling-indexing/mobile/mobile-sites-mobile-first-indexing
 24.​   What Is Structured Data & Schema Markup in SEO? | Mangools, accessed July 7, 2025,
        https://mangools.com/blog/structured-data/
 25.​   The Beginner's Guide to Structured Data for Organizing & Optimizing Your Website, accessed July 7, 2025,
        https://blog.hubspot.com/marketing/structured-data
 26.​   Structured data for SEO: What you need to know | Crystal Carter - Wix.com, accessed July 7, 2025,
        https://www.wix.com/seo/learn/resource/structured-data-for-seo
 27.​   The Complete JavaScript SEO Guide - Impression Digital, accessed July 7, 2025,
        https://www.impressiondigital.com/blog/javascript-seo-guide/
 28.​   The Definitive Javascript SEO Guide [Best Practices and Beyond] - Neil Patel, accessed July 7, 2025,
        https://neilpatel.com/blog/javascript-seo/
 29.​   Understand JavaScript SEO Basics | Google Search Central ..., accessed July 7, 2025,
        https://developers.google.com/search/docs/crawling-indexing/javascript/javascript-seo-basics
 30.​   The Ultimate Guide to JavaScript SEO - Onely, accessed July 7, 2025,
        https://www.onely.com/blog/ultimate-guide-javascript-seo/
 31.​   JavaScript SEO Issues & Best Practices - Ahrefs, accessed July 7, 2025, https://ahrefs.com/blog/javascript-seo/

                                                             217

           © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




32.​ Hobo SEO Dashboard - Is It Worth It?, accessed July 7, 2025,
     https://www.hobo-web.co.uk/hobo-seo-dashboard-is-it-worth-it/
33.​ How to create a good meta description - Yoast, accessed July 7, 2025, https://yoast.com/meta-descriptions/
34.​ Meta Descriptions: Definition, Examples & How to Write Them (2025 ..., accessed July 7, 2025,
     https://www.shopify.com/blog/how-to-write-meta-descriptions
35.​ How To Write Meta Descriptions - HubSpot Blog, accessed July 7, 2025,
     https://blog.hubspot.com/marketing/how-to-write-a-meta-description
36.​ Why Internal and External Links Are Important for SEO - seoClarity, accessed July 7, 2025,
     https://www.seoclarity.net/resources/knowledgebase/why-internal-and-external-links-important-for-seo-16559/
37.​ Internal linking for SEO: Why and how? • Yoast, accessed July 7, 2025,
     https://yoast.com/internal-linking-for-seo-why-and-how/
38.​ Backlinks, External Links or Internal Links: Which of the 3 is More Important?, accessed July 7, 2025,
     https://www.internallinkjuicer.com/hub/basics/external-links-or-internal-links/
39.​ How do internal and external links contribute to the SEO value of content? - Quora, accessed July 7, 2025,
     https://www.quora.com/How-do-internal-and-external-links-contribute-to-the-SEO-value-of-content
40.​ What is External Linking and is it good for SEO? - Engine Scout, accessed July 7, 2025,
     https://enginescout.com.au/on-page-seo/external-links/
41.​ Internal vs External Links: A Comprehensive Guide for SEO Success - SpyFu, accessed July 7, 2025,
     https://www.spyfu.com/blog/internal-vs-external-links/
42.​ Link Building for SEO: The Beginner's Guide - Ahrefs, accessed July 7, 2025, https://ahrefs.com/blog/link-building/
43.​ Free SEO Ebook PDF Download - Hobo Web, accessed July 7, 2025, https://www.hobo-web.co.uk/free-seo-ebook-pdf/
44.​ 17 Link Building Strategies Proven to Succeed [With Outreach ..., accessed July 7, 2025,
     https://www.siegemedia.com/marketing/link-building-strategies
45.​ The Ultimate Link Building Guide for Beginners | Nutshell, accessed July 7, 2025,
     https://www.nutshell.com/blog/link-building-for-beginners
46.​ Link Building for SEO: A Guide to the Basics - Semrush, accessed July 7, 2025, https://www.semrush.com/blog/link-building/
47.​ Link building strategy: a successful approach in 6 steps - Yoast, accessed July 7, 2025,
     https://yoast.com/successful-link-building-strategy/
48.​ Best link building strategy? : r/SEO - Reddit, accessed July 7, 2025,
     https://www.reddit.com/r/SEO/comments/13xc0gj/best_link_building_strategy/
49.​ Three-Minute Legal Talks: The United States v. Google Case Explained | UW School of Law, accessed July 5, 2025,
     https://www.law.uw.edu/news-events/news/2023/us-vs-google
50.​ Google Search VP Defends Practices As Antitrust Trial Progresses, accessed July 5, 2025,
     https://www.searchenginejournal.com/google-search-vp-defends-practices-as-antitrust-trial-progresses/498806/
51.​ Dept. of Justice v. Google - American Economic Liberties Project, accessed July 5, 2025,
     https://www.economicliberties.us/dept-of-justice-v-google/
52.​ DOJ vs. Google: Key Takeaways from the Historic Antitrust Case - The Futurum Group, accessed July 5, 2025,
     https://futurumgroup.com/insights/doj-vs-google-key-takeaways-from-the-historic-antitrust-case/
53.​ Google's Search Monopoly on Trial: A Landmark Antitrust Case - The National CIO Review, accessed July 5, 2025,
     https://nationalcioreview.com/articles-insights/technology/googles-search-monopoly-on-trial-a-landmark-antitrust-case/
54.​ Google Antitrust Lawsuits Explained - The Lanier Law Firm, accessed July 5, 2025,
     https://www.lanierlawfirm.com/google-antitrust-lawsuits-explained/
55.​ Seven Takeaways from the Justice Department's Antitrust Case Against Google's Search Deals | by Adam Kovacevich |
     Chamber of Progress | Medium, accessed July 5, 2025,
     https://medium.com/chamber-of-progress/seven-takeaways-from-the-justice-departments-antitrust-case-against-googl
     e-s-search-deals-eea52e501510
56.​ Understanding the Google Antitrust Trial: Details and Implications - Neil Patel, accessed July 5, 2025,
     https://neilpatel.com/blog/google-antitrust-trial/
57.​ Key takeaways from the Search trial - Google Blog, accessed July 5, 2025,
     https://blog.google/outreach-initiatives/public-policy/key-takeaways-from-the-search-trial/
58.​ Google exec warns DOJ remedies will undermine user trust in ..., accessed July 5, 2025,
     https://www.courthousenews.com/google-exec-warns-doj-remedies-will-undermine-user-trust-in-internet-search/
59.​ Case 1:20-cv-03010-APM Document 835 Filed 02/23/24 Page 1 of 599, accessed July 5, 2025,
     https://regmedia.co.uk/2024/02/26/us_v_google.pdf
60.​ 6 Takeaways for SEOs From Google's Antitrust Trial - Supple Digital, accessed July 5, 2025,
     https://supple.com.au/guides/seo-insights-google-antitrust-trial/
61.​ Google's Grimoire: DOJ Trial Exposes Search Engine's 'Magic' - The HOTH, accessed July 5, 2025,
     https://www.thehoth.com/blog/googles-grimoire-doj-trial/
62.​ CONTAINS HIGHLY CONFIDENTIAL INFORMATION - SUBJECT TO ..., accessed July 5, 2025,
     https://www.justice.gov/atr/media/1398871/dl

                                                           218

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




63.​ What Is Google's Navboost Algorithm? - Semrush, accessed July 5, 2025, https://www.semrush.com/blog/navboost/
64.​ TOP GOOGLE RANKING FACTORS: TRIAL TESTIMONY - Ottaway Digital, accessed July 5, 2025,
     https://detroitseocompany.com/blog/seo-facts/top-google-ranking-factors-trial-testimony/
65.​ Is Q-Star the Next PageRank? What Google's Antitrust Trial Revealed - Vincent Schmalbach, accessed July 5, 2025,
     https://www.vincentschmalbach.com/is-q-star-the-next-pagerank-what-googles-antitrust-trial-revealed/
66.​ Googler's Deposition Offers View Of Google's Ranking Systems - Search Engine Journal, accessed July 5, 2025,
     https://www.searchenginejournal.com/googlers-deposition-offers-view-of-googles-ranking-systems/546901/
67.​ Former Googler: Google 'Using Clicks in Rankings' - BluShark Digital, accessed July 5, 2025,
     https://blusharkdigital.com/blog/former-googler-google-using-clicks-in-rankings/
68.​ Unraveling the mysteries of Google search: Insights from Pandu Nayak's testimony, accessed July 5, 2025,
     https://seositecheckup.com/articles/unraveling-the-mysteries-of-google-search-insights-from-pandu-nayaks-testimony
69.​ The ABCs of Google ranking signals: What top search engineers ..., accessed July 5, 2025,
     https://searchengineland.com/google-abc-ranking-signals-455360
70.​ Leaked Testimony Reveals How Google Really Ranks Sites - Stan Ventures, accessed July 5, 2025,
     https://www.stanventures.com/news/leaked-testimony-reveals-how-google-really-ranks-sites-2707/
71.​ Google Ranking Leaked: Quality, Clicks, Popularity & LLM - Stradiji, accessed July 5, 2025,
     https://www.stradiji.com/en/googles-ranking-secrets-leaked-quality-clicks-popularity-and-llm-support/
72.​ Google uses Click Data for Rankings - SISTRIX, accessed July 5, 2025,
     https://www.sistrix.com/blog/google-uses-click-data-for-rankings/
73.​ Secrets from the Algorithm: Google Search's Internal Engineering Documentation Has Leaked - iPullRank, accessed July 5,
     2025, https://ipullrank.com/google-algo-leak
74.​ The Google Search Algorithm Leak: What It Means and Decoding It For You - AIOSEO, accessed July 5, 2025,
     https://aioseo.com/google-search-algorithm-leak/
75.​ Navboost in SEO: Google's Ranking Algorithm [Updated Guide] - AllAboutAI.com, accessed July 5, 2025,
     https://www.allaboutai.com/ai-seo/navboost-complete-guide/
76.​ Understanding Navboost: Google's Secret to Ranking Based on User Clicks, Hovers, and Scrolls - Marie Haynes Consulting,
     accessed July 5, 2025, https://www.mariehaynes.com/navboost/
77.​ Google Navboost: How it works and how to optimize for it - SE Ranking, accessed July 5, 2025,
     https://seranking.com/blog/navboost/
78.​ Unraveling Google Navboost and building user trust through UX - Search Engine Land, accessed July 5, 2025,
     https://searchengineland.com/navboost-user-trust-ux-445240
79.​ What is Google's NavBoost Algorithm? | It Depends - An SEO Podcast, accessed July 5, 2025,
     https://transistordigital.com/blog/what-is-googles-navboost-algorithm/
80.​ Understanding Google Rank Brain And How It Impacts SEO - Moz, accessed July 5, 2025,
     https://moz.com/learn/seo/google-rankbrain
81.​ What is Google RankBrain? - Target Internet, accessed July 5, 2025,
     https://targetinternet.com/resources/what-is-google-rankbrain
82.​ DOJ Concludes a Compelling Case in the Landmark US v. Google ..., accessed July 5, 2025,
     https://www.economicliberties.us/press-release/doj-concludes-a-compelling-case-in-the-landmark-us-v-google-antitrus
     t-trial-with-final-witness/
83.​ Google Search's guidance about AI-generated content, accessed July 5, 2025,
     https://developers.google.com/search/blog/2023/02/google-search-and-ai-content
84.​ Google Wants You To Stop Singling Out RankBrain, accessed July 5, 2025,
     https://www.seroundtable.com/google-rankbrain-overblown-26634.html
85.​ www.invoca.com, accessed July 8, 2025,
     https://www.invoca.com/blog/zero-click-search-isnt-the-end-of-seo-its-a-new-beginning#:~:text=A%20zero%2Dclick%2
     0search%20occurs,go%20on%20their%20merry%20way.
86.​ What Are Zero-Click Searches & How Do They Affect Lead Generation? - Hushly, accessed July 8, 2025,
     https://www.hushly.com/blog/zero-click-search/
87.​ What Are Zero Click Searches and How to Appear for Them - TheeDigital, accessed July 8, 2025,
     https://www.theedigital.com/blog/what-are-zero-click-searches
88.​ Understanding Zero-Click Searches: Impact & Optimization Strategies - Williams Media, accessed July 8, 2025,
     https://williamsmedia.co/what-are-zero-click-searches
89.​ sparktoro.com, accessed July 8, 2025,
     https://sparktoro.com/blog/why-do-we-need-zero-click-marketing/#:~:text=You've%20heard%20of%20zero,click%20ove
     r%20to%20your%20site.
90.​ Why Do We Need Zero Click Marketing? - SparkToro, accessed July 8, 2025,
     https://sparktoro.com/blog/why-do-we-need-zero-click-marketing/
91.​ Zero click digital marketing, Is it the end of the click? - Elevated Marketing Solutions ™, accessed July 8, 2025,
     https://elevatedmarketing.solutions/zero-click-digital-marketing-is-it-the-end-of-the-click/

                                                          219

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                  Strategic SEO 2025




92.​ Zero-Click Marketing: an excuse for losing traffic or a real SEO trend? - Luca Tagliaferro, accessed July 8, 2025,
      https://www.lucatagliaferro.com/post/zero-click-searches/
93.​ Zero-Click Search Optimization: Why Google Is Your Frenemy (And How to Beat It Anyway), accessed July 8, 2025,
      https://www.zoomsphere.com/blog/zero-click-search-optimization-why-google-is-your-frenemy-and-how-to-beat-it-an
      yway
94.​ Why Campaigners Are Rallying Against Google's AI Overview - Technology Magazine, accessed July 8, 2025,
      https://technologymagazine.com/articles/why-campaigners-are-rallying-against-googles-ai-overview
95.​ Google's AI Overviews Challenged by EU Antitrust Complaint as Publisher War Escalates, accessed July 8, 2025,
      https://winbuzzer.com/2025/07/04/eu-antitrust-complaint-googles-ai-overviews-hit-by-as-publisher-war-escalates-xcxw
      bn/
96.​ Mastering Zero-Click Marketing: Strategies for Success - MESH Interactive Agency, accessed July 8, 2025,
      https://meshagency.com/mastering-zero-click-marketing-strategies-for-success/
97.​ The Rise of Zero-Click Content: How to Optimize for 2025's Search Trends - StoryChief, accessed July 8, 2025,
      https://storychief.io/blog/zero-click-content
98.​ Zero-Click Searches: Should You Be Worried? - Exposure Ninja, accessed July 8, 2025,
      https://exposureninja.com/blog/zero-click-searches/
99.​ Zero Click Searches - find the definition in the SEO Glossary - Conductor, accessed July 8, 2025,
      https://www.conductor.com/academy/glossary/zero-click-searches/
100.​Zero-Click Searches: Why 60% of Google Users Never Click Through in 2025, accessed July 8, 2025,
      https://upandsocial.com/zero-click-searches-2025-trend-analysis/
101.​ Zero Click Searches & How They Impact SEO - Neil Patel, accessed July 8, 2025,
      https://neilpatel.com/blog/zero-click-searches/
102.​ The Shift To Zero-Click Searches: Is Traffic Still King?, accessed July 8, 2025,
      https://www.searchenginejournal.com/the-shift-to-zero-click-searches-is-traffic-king/540847/
103.​ Zero-Click Searches: What They Mean for Local Businesses - The CEO Views, accessed July 8, 2025,
      https://theceoviews.com/zero-click-searches-what-they-mean-for-local-businesses/
104.​The Rise in Zero-Click Searches: A Deep-Dive | Workshop Digital, accessed July 8, 2025,
      https://www.workshopdigital.com/blog/rise-in-zero-click-searches-deep-dive/
105.​www.wearediagram.com, accessed July 8, 2025,
      https://www.wearediagram.com/blog/zero-click-search-what-you-can-do-about-it#:~:text=%E2%80%9CZero%2Dclick%E
      2%80%9D%20as%20it,businesses%20directly%20in%20search%20results.
106.​The Continuing Rise of Zero-Click Search (and What You Can Do About It) | Diagram, accessed July 8, 2025,
      https://www.wearediagram.com/blog/zero-click-search-what-you-can-do-about-it
107.​ How Zero-Click Searches Can Impact Your Website Profitability - Rocket.net, accessed July 8, 2025,
      https://rocket.net/blog/how-zero-click-searches-can-impact-your-website-profitability/
108.​ Nearly 60% of Google searches end without a click in 2024 - Search Engine Land, accessed July 8, 2025,
      https://searchengineland.com/google-search-zero-click-study-2024-443869
109.​ Similarweb: Zero-Click Searches Surge to 69% Since Google AI ..., accessed July 8, 2025,
      https://www.stanventures.com/news/similarweb-zero-click-search-surge-google-ai-overviews-3562/
110.​ What zero click searches mean for your SEO - Hallam, accessed July 8, 2025,
      https://hallam.agency/blog/what-zero-click-searches-mean-for-your-seo/
111.​ Zero-Click Search Impact on Small Businesses: What You Need To Know - - Scale By SEO, accessed July 8, 2025,
      https://scalebyseo.com/post/zero-click-search-impact-on-small-businesses
112.​ Google faces EU antitrust complaint over AI Overviews feature - PPC Land, accessed July 8, 2025,
      https://ppc.land/google-faces-eu-antitrust-complaint-over-ai-overviews-feature/
113.​ Urgent bid lodged with UK regulator to stop Google AI Overviews 'stealing journalism', accessed July 8, 2025,
      https://pressgazette.co.uk/news/urgent-bid-lodged-with-uk-regulator-to-stop-google-ai-overviews-stealing-journalism/
114.​ 30-Year SEO Pro Shows How To Adapt To Google's Zero-Click Search, accessed July 8, 2025,
      https://www.searchenginejournal.com/google-ai-overviews-zero-click-serps/547263/
115.​ Zero-Click Search Isn't the End of SEO. It's a New Beginning. - Invoca, accessed July 8, 2025,
      https://www.invoca.com/blog/zero-click-search-isnt-the-end-of-seo-its-a-new-beginning
116.​ Zero-Click Searches: Are They Impacting Your Site's Traffic in 2025?, accessed July 8, 2025,
      https://www.websitebuilderexpert.com/news/zero-click-searches/
117.​ The Rise of Zero-Click Searches: What Small Businesses Need to Know, accessed July 8, 2025,
      https://www.needmomentum.com/zero-click-searches/
118.​ Google I/O 2024: New generative AI experiences in ... - Google Blog, accessed July 8, 2025,
      https://blog.google/products/search/generative-ai-google-search-may-2024/
119.​ Evaluating page experience for a better web | Google Search ..., accessed July 8, 2025,
      https://developers.google.com/search/blog/2020/05/evaluating-page-experience
120.​ AI Overviews: About last week - Google Blog, accessed July 8, 2025,

                                                         220

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




      https://blog.google/products/search/ai-overviews-update-may-2024/
121.​ Google Responds To Publishers Concerns On AI Mode - Search Engine Roundtable, accessed July 8, 2025,
      https://www.seroundtable.com/google-response-concerns-ai-mode-39028.html
122.​ AI overviews: How are publishers adapting to the rise of clickless search? | The Current, accessed July 8, 2025,
      https://www.thecurrent.com/marketing-strategy-ai-overviews-publishers-rise-clickless-search
123.​ Google Defends Data Scraping Practices, Citing Vital Role in AI Development - Bill Hartzer, accessed July 8, 2025,
      https://www.billhartzer.com/ai/google-defends-data-scraping-practices-citing-vital-role-in-ai-development/
124.​ Google Wants AI Scraping to Be 'Fair Use.' Will That Fly in Court? - Tom's Hardware, accessed July 8, 2025,
      https://www.tomshardware.com/news/google-ai-scraping-as-fair-use
125.​ A New Look at Fair Use: Anthropic, Meta, and Copyright in AI Training - Reed Smith LLP, accessed July 8, 2025,
      https://www.reedsmith.com/en/perspectives/2025/07/a-new-look-fair-use-anthropic-meta-copyright-ai-training
126.​ Google Decided Against Offering Publishers Options In AI Search - Slashdot, accessed July 8, 2025,
      https://tech.slashdot.org/story/25/05/19/2054230/google-decided-against-offering-publishers-options-in-ai-search
127.​ Generative AI, not ad tech, is the new antitrust battleground for Google - Digiday, accessed July 8, 2025,
      https://digiday.com/media/generative-ai-not-ad-tech-is-the-new-antitrust-battleground-for-google/
128.​ The Impact of Zero-Click Searches on SEO Strategies - Tekrevol, accessed July 8, 2025,
      https://www.tekrevol.com/blogs/the-impact-of-zero-click-searches-on-seo-strategies/
129.​ 76 Local SEO Statistics for 2025 | SeoProfy, accessed July 8, 2025, https://seoprofy.com/blog/local-seo-statistics/
130.​ What is Zero-Click Search and How Does it Impact My SEO? - OuterBox, accessed July 8, 2025,
      https://www.outerboxdesign.com/articles/seo-services/zero-click-search-results-for-seo
131.​ Why Industrial Distribution E-commerce Sites Thrive Despite the ..., accessed July 8, 2025,
      https://www.impaqx.com/why-industrial-distribution-e-commerce-sites-thrive-despite-the-zero-click-search-trend/
132.​ B2B and DTC marketers find themselves on the zero-click search frontline - Digiday, accessed July 8, 2025,
      https://digiday.com/marketing/b2b-and-dtc-marketers-find-themselves-on-the-zero-click-search-frontline/
133.​ 5 Ways to Win the No-Click SERP - Oneupweb, accessed July 8, 2025, https://www.oneupweb.com/blog/zero-click-serp/
134.​ Inside Zero-Click Searches (And Their SEO Impact), accessed July 8, 2025, https://www.seo.com/blog/zero-click-searches/
135.​ How Google AI Overviews is fuelling zero-click searches for top publishers - Press Gazette, accessed July 8, 2025,
      https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/how-google-ai-overviews-is-fuelling-zero-
      click-searches-for-top-publishers/
136.​ EU Slaps Google With Antitrust Complaint Over AI Overviews - CNET, accessed July 8, 2025,
      https://www.cnet.com/tech/services-and-software/eu-slaps-google-with-antitrust-complaint-over-ai-overviews/
137.​ ESG News Recap: Google's AI Faces EU Scrutiny - Impakter, accessed July 8, 2025,
      https://impakter.com/esg-news-google-ai-content-scraping/
138.​ Google's 'biggest AI Search feature' hit by complaint in EU: What the tech giant has to say, accessed July 8, 2025,
      https://timesofindia.indiatimes.com/technology/tech-news/googles-biggest-ai-search-feature-hit-by-complaint-in-eu-wh
      at-the-tech-giant-has-to-say/articleshow/122254961.cms
139.​ Why are European publishers taking Google AI Overviews to court over antitrust violations?, accessed July 8, 2025,
      https://techhq.com/2025/07/google-ai-overviews-antitrust-eu-publishers-complaint/
140.​Google faces EU antitrust complaint over AI Overviews - Search Engine Land, accessed July 8, 2025,
      https://searchengineland.com/google-faces-eu-antitrust-complaint-over-ai-overviews-458123
141.​ Optimising for Featured Snippets and Quick Answers: A Concise Guide, accessed July 8, 2025,
      https://profiletree.com/featured-snippets/
142.​ [2025] How to Optimize for Featured Snippets: A Quick Guide - EmbedPress, accessed July 8, 2025,
      https://embedpress.com/blog/how-to-optimize-for-featured-snippets/
143.​ Writing for SEO: How to write snippet-friendly content that wins in Google and LLMs - Search Engine Land, accessed July
      8, 2025, https://searchengineland.com/guide/how-to-write-for-seo
144.​ How to Optimize for Featured Snippets and Rank Higher - BragDeal, accessed July 8, 2025,
      https://bragdeal.com/blog/article/how-to-optimize-for-featured-snippets-and-rank-higher/
145.​ How Zero-Click Searches Impact Small Business - WeLaunch Digital Solutions, accessed July 8, 2025,
      https://welaunch.ca/how-zero-click-searches-impact-small-business/
146.​ Zero-Click Searches: What Are They & How To Optimize for SEO - Zero Gravity Marketing, accessed July 8, 2025,
      https://zerogravitymarketing.com/blog/how-to-optimize-for-zero-click-searches/
147.​ How AEO Can Help With Zero-Click Searches: Reclaiming Visibility in 2025 - Single Grain, accessed July 8, 2025,
      https://www.singlegrain.com/digital-marketing-strategy/how-aeo-can-help-with-zero-click-searches-in-2025/
148.​ Top ways to ensure your content performs well in Google's AI ..., accessed July 8, 2025,
      https://developers.google.com/search/blog/2025/05/succeeding-in-ai-search
149.​ Welcome the SEO Apocalypse: A Zero-Click Search Survival Guide - TopRank® Marketing, accessed July 8, 2025,
      https://www.toprankmarketing.com/blog/zero-click-search-survival-guide/
150.​4 Ways to Optimize for Zero-Click Searches - WordStream, accessed July 8, 2025,
      https://www.wordstream.com/blog/zero-click-searches

                                                           221

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                    Strategic SEO 2025




151.​ How to Optimize for Zero-Click Searches: The Ultimate Guide for 2025 - Niumatrix Digital, accessed July 8, 2025,
      https://niumatrix.com/zero-click-search-optimization/
152.​ Google API Leak: A Look Inside the Algorithm | Optimal – https://www.winwithoptimal.com/insights/google-api-leak/
153.​ The Google Leak | Understanding The Algorithm Fiasco – Hype Insight –
      https://hypeinsight.com/from-leak-to-leverage-exploring-the-google-leak-and-its-unseen-impact-on-search/
154.​ Google API Leak: Comprehensive Review and Guidance – Marketing Aid –
      https://www.marketingaid.io/google-api-leak-comprehensive-review-and-guidance/
155.​ A practical breakdown of the 2024 Google API leak for law firms and … – LaFleur Marketing
156.​ Google's 14,000 Search Ranking Features Leaked – https://www.seroundtable.com/google-search-data-leak-37462.html
157.​ The 2024 Google Search Engine API Leak + What It Means for Your Site – Responsival –
      https://www.responsival.com/post/2024-google-search-engine-api-leak-what-it-means-for-your-site
158.​ The Google API Leak Should Change How Marketers and Publishers Do SEO – SparkToro –
      https://sparktoro.com/blog/11-min-video-the-google-api-leak-should-change-how-marketers-and-publishers-do-seo/
159.​ Massive Google Search Algorithm Leak: What Publishers Should Know – Admiral Blog –
      https://blog.getadmiral.com/massive-google-search-algorithm-leak-what-publishers-should-know
160.​HUGE Google Search document leak reveals inner workings of ranking algorithm – Search Engine Land
161.​ Rand Fishkin At MozCon: Rethinking Strategies Amid Google API “Leak” – Search Engine Journal –
      https://www.searchenginejournal.com/rand-fishkin-at-mozcon-rethinking-strategies-amid-google-api-leak/518504/
162.​ Analyzing Google Leaked API Document – Mic King (iPullRank) : r/TechSEO –
      https://www.reddit.com/r/TechSEO/comments/1d2a72l/analyzing_google_leaked_api_document_mic_king/
163.​ Secrets from the Algorithm: Google Search’s Internal Engineering Documentation Has Leaked – iPullRank –
      https://ipullrank.com/google-algo-leak
164.​ Key Takeaways From Google Algorithm Leak – Utds Optimal Choice –
      https://utds.al/key-takeaways-from-google-algorithm-leak/
165.​ 22 Things We Might Have Learned From The Google Search Leak – Search Logistics –
      https://www.searchlogistics.com/learn/seo/algorithm/google-search-leak/
166.​ An Anonymous Source Shared Thousands of Leaked Google … – SparkToro –
      https://sparktoro.com/blog/an-anonymous-source-shared-thousands-of-leaked-google-search-api-documents-with-m
      e-everyone-in-seo-should-see-them/
167.​ Google Leak: Compilation of Terms Mentioned in the Documents – USEO – https://useo.es/google-leak-terms/
168.​ Understanding Google’s “Ascorer” and “Twiddlers” in Search Ranking – Julian Redlich –
      https://rankmeamadeus.com/understanding-googles-ascorer-and-twiddlers-in-search-ranking/
169.​ Google’s documentation leak: 12 big takeaways for link builders and digital PRs – Search Engine Land
170.​ The Biggest Takeaways From the Google Search Algorithm Leak – VELOX –
      https://www.veloxmedia.com/blog/google-search-algorithm-leak-takeaways
171.​ 7 Things Google Can Measure That Might Affect Your SEO – WordStream –
      https://www.wordstream.com/blog/google-algorithm-documents-leak
172.​ The Top Things to Know About the Google Algorithm Leak – Collective Measures –
      https://www.collectivemeasures.com/insights/the-top-things-to-know-about-the-google-algorithm-leak
173.​ Google Search Leak 2024: Shocking SEO Revelations & Ranking Factors – The Bullzeye –
      https://thebullzeye.com/google-search-leak/
174.​ What to Make of the Google Leaks? – r/SEO (Reddit) –
      https://www.reddit.com/r/SEO/comments/1d71mlo/what_to_make_of_the_google_leaks/
175.​ The Google Navboost Leak That Validated CTR Manipulation Techniques – Top Of The Results –
      https://www.topoftheresults.com/the-google-navboost-leak-that-validated-ctr-manipulation-techniques/
176.​ Google’s Navboost Algorithm: A Highlight From The Leaked Google Search API Documents – OuterBox Design –
      https://www.outerboxdesign.com/articles/googles-navboost-algorithm-highlight-from-leaked-google-search-api
177.​ Navboost: What It Is and How Google Uses It – Bend Marketing –
      https://bendyourmarketing.com/blog/navboost-what-it-is-and-how-google-uses-it/
178.​ Google Navboost: How it works and how to optimize for it – SE Ranking – https://seranking.com/blog/navboost/​

179.​ Google Search Ranking algo doc leaked – r/SEO (Reddit) –
      https://www.reddit.com/r/SEO/comments/1d2lvd7/google_search_ranking_algo_doc_leaked/
180.​ Document leak from Google: a massive event – Adrenalead –
      https://adrenalead.com/en/blog/document-leak-from-google/
181.​ Google Search Algorithm Documentation Leaked – r/PPC (Reddit) –
      https://www.reddit.com/r/PPC/comments/1d3aty9/google_search_algorithm_documentation_leaked/
182.​ How to Write SEO Content as per Google Leaks – Contensify –
      https://contensifyhq.com/blog/how-to-write-seo-content-google-leaks-tips/
183.​ The Google API Documentation Leak: 10 Insights for Digital Marketers – Digital Marketing Institute –

                                                            222

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
                                                   Strategic SEO 2025




      https://digitalmarketinginstitute.com/blog/the-google-api-documentation-leak-10-insights-for-digital-marketers
184.​ Google Search document leak reveals inner workings of ranking algorithm – MarTech –
      https://martech.org/google-search-document-leak-reveals-inner-workings-of-ranking-algorithm/
185.​ Deciphering SEO News: The Infamous 2024 Google Leak – Nexus Marketing –
      https://nexusmarketing.com/2024-google-leak/
186.​ The Google Search Algorithm Leak: What It Means and Decoding It For You – AIOSEO –
      https://aioseo.com/google-search-algorithm-leak/
187.​ Mike King on relevance engineering and the end of SEO as we know it – Search Engine Land –
      https://searchengineland.com/mike-king-smx-advanced-2025-interview-456186
188.​ Adjusting SEO Tactics After The Google API Exposure – Location3 –
      https://location3.com/blog/adjusting-seo-tactics-after-the-google-api-exposure/
189.​ Google's Navboost Queries and Brands in SEO – Moz – https://moz.com/blog/google-navboost-and-brands
190.​How 2024 Google Algorithm Leak Impacts Contractors And Home Service Companies – Footbridge Media –
      https://www.footbridgemedia.com/marketing-tips/google-algo-leak-2024-impacts-contractor-marketing
191.​ Google Algo Leak: Summary of the Most Interesting Aspects – In Marketing We Trust –
      https://inmarketingwetrust.com.au/google-algo-leak-summary-of-the-most-interesting-aspects/
192.​ Google Search Algorithm Leak – What You Need to Know – Digital Marketing Agency –
      https://pureseo.com/blog/search-algorithm-leak
193.​ SEO Top Secrets and Expert Insights from the Google 2019s Leak – Wide Angle Analytics –
      https://wideangle.co/blog/google-leak
194.​ 20 SEOs Share Their Key Takeaways From the Google API Leaks – Moz​
      https://moz.com/blog/google-api-leaks




                                                          223

        © 2025 Shaun Anderson / Hobo / X (Twitter) / Linkedin / Facebook / SEO Audit / SEO Dashboard / EEAT Tool
